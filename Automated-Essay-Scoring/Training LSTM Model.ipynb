{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './data/'\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
    "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viksp\\AppData\\Local\\Temp\\ipykernel_28304\\3902004401.py:33: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 5s 10ms/step - loss: 66.3540 - mae: 4.4280\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 41.4372 - mae: 3.6095\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 33.8708 - mae: 3.4780\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 30.6748 - mae: 3.3695\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 29.0806 - mae: 3.2214\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 27.2340 - mae: 3.0512\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 25.3184 - mae: 2.8883\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 21.8916 - mae: 2.6841\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 18.7948 - mae: 2.4993\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 17.5299 - mae: 2.3725\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 15.9111 - mae: 2.2780\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 15.1979 - mae: 2.2185\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 14.3564 - mae: 2.1619\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 14.0640 - mae: 2.1183\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 13.4148 - mae: 2.0696\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 12.6777 - mae: 2.0117\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 12.6563 - mae: 1.9949\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.1608 - mae: 1.9560\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.2650 - mae: 1.9520\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.4902 - mae: 1.8992\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.1393 - mae: 1.8748\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.2400 - mae: 1.8643\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.3525 - mae: 1.8669\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.6824 - mae: 1.8161\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.4635 - mae: 1.8027\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.0368 - mae: 1.7778\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.0246 - mae: 1.7746\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.2198 - mae: 1.7692\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.0645 - mae: 1.7724\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.5098 - mae: 1.7453\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.7084 - mae: 1.7490\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.7956 - mae: 1.7534\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.7936 - mae: 1.7538\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2632 - mae: 1.7207\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.1192 - mae: 1.7161\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2731 - mae: 1.7171\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.1619 - mae: 1.7086\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2379 - mae: 1.6960\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.7704 - mae: 1.6766\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2450 - mae: 1.7019\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.6742 - mae: 1.6798\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.8305 - mae: 1.6772\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.7262 - mae: 1.6741\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.6481 - mae: 1.6542\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.0037 - mae: 1.6856\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.5879 - mae: 1.6444\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.9094 - mae: 1.6643\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.6206 - mae: 1.6587\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.6127 - mae: 1.6561\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.4955 - mae: 1.6556\n",
      "82/82 [==============================] - 1s 2ms/step\n",
      "Kappa Score: 0.9531841063696395\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viksp\\AppData\\Local\\Temp\\ipykernel_28304\\3902004401.py:33: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 5s 10ms/step - loss: 61.4058 - mae: 4.2732\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 38.7781 - mae: 3.5355\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 33.0022 - mae: 3.4582\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 29.9948 - mae: 3.3271\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 28.5795 - mae: 3.1923\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 26.7978 - mae: 3.0205\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 24.2413 - mae: 2.8305\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 20.2640 - mae: 2.6108\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 17.8278 - mae: 2.4264\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 16.9481 - mae: 2.3294\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 15.8404 - mae: 2.2703\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 14.3231 - mae: 2.1577\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 13.8860 - mae: 2.1251\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 13.1792 - mae: 2.0723\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.3929 - mae: 2.0345\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.2997 - mae: 1.9839\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.1989 - mae: 1.9690\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.6187 - mae: 1.9251\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.8910 - mae: 1.9152\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.3473 - mae: 1.8853\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.4400 - mae: 1.8725\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.7674 - mae: 1.8436\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.2389 - mae: 1.8432\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.6190 - mae: 1.8188\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.9671 - mae: 1.8159\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.0586 - mae: 1.7695\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.7222 - mae: 1.7497\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.7821 - mae: 1.7569\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.7878 - mae: 1.7578\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.3339 - mae: 1.7285\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.5745 - mae: 1.7378\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.7832 - mae: 1.7372\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.3926 - mae: 1.7176\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.2870 - mae: 1.7128\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.5807 - mae: 1.7080\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.6621 - mae: 1.7161\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.3873 - mae: 1.7032\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.1366 - mae: 1.6971\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.8863 - mae: 1.6811\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.0016 - mae: 1.6743\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.9159 - mae: 1.6802\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.5445 - mae: 1.6513\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.5603 - mae: 1.6553\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.7801 - mae: 1.6718\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.5337 - mae: 1.6452\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.9532 - mae: 1.6652\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.1877 - mae: 1.6404\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.3523 - mae: 1.6300\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.5078 - mae: 1.6378\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.1987 - mae: 1.6245\n",
      "82/82 [==============================] - 1s 3ms/step\n",
      "Kappa Score: 0.9590498350707656\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viksp\\AppData\\Local\\Temp\\ipykernel_28304\\3902004401.py:33: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 5s 10ms/step - loss: 62.9911 - mae: 4.3163\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 40.0523 - mae: 3.5756\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 33.0573 - mae: 3.4283\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 29.6331 - mae: 3.3291\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 28.0223 - mae: 3.1739\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 26.2306 - mae: 3.0029\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 24.1091 - mae: 2.8499\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 21.0946 - mae: 2.6311\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 18.3001 - mae: 2.4532\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 17.2112 - mae: 2.3637\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 15.6420 - mae: 2.2481\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 15.4490 - mae: 2.2124\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 13.8324 - mae: 2.1141\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 13.5559 - mae: 2.0905\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 13.3559 - mae: 2.0538\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 12.0992 - mae: 1.9800\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 12.3726 - mae: 1.9660\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.8579 - mae: 1.9316\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.4871 - mae: 1.8944\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.3981 - mae: 1.8883\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.7391 - mae: 1.8417\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.6282 - mae: 1.8171\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.6015 - mae: 1.8096\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.6934 - mae: 1.8146\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.4853 - mae: 1.7988\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.2744 - mae: 1.7827\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.2038 - mae: 1.7662\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.8411 - mae: 1.7528\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.6505 - mae: 1.7280\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.7924 - mae: 1.7603\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.1905 - mae: 1.7109\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.1854 - mae: 1.7111\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.4458 - mae: 1.7275\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.4455 - mae: 1.7122\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.3498 - mae: 1.7105\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.3448 - mae: 1.7107\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.9882 - mae: 1.6844\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.0798 - mae: 1.6931\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.0030 - mae: 1.6872\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.8744 - mae: 1.6655\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.0751 - mae: 1.6786\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.8426 - mae: 1.6610\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.3512 - mae: 1.6362\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.3661 - mae: 1.6349\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.6084 - mae: 1.6410\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.7581 - mae: 1.6405\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.4134 - mae: 1.6137\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.7483 - mae: 1.6499\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.1998 - mae: 1.6078\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.2818 - mae: 1.6098\n",
      "82/82 [==============================] - 1s 3ms/step\n",
      "Kappa Score: 0.956260575751652\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viksp\\AppData\\Local\\Temp\\ipykernel_28304\\3902004401.py:33: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 5s 10ms/step - loss: 64.7861 - mae: 4.3721\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 39.9454 - mae: 3.6037\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 33.4453 - mae: 3.4606\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 30.0733 - mae: 3.3756\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 27.9735 - mae: 3.2193\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 26.3018 - mae: 3.0453\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 24.4901 - mae: 2.8911\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 22.7571 - mae: 2.7245\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 19.0329 - mae: 2.4993\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 17.7264 - mae: 2.3816\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 15.6030 - mae: 2.2450\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 15.1012 - mae: 2.2023\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 13.9316 - mae: 2.1317\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.8344 - mae: 2.0525\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 13.1616 - mae: 2.0449\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.4656 - mae: 1.9815\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 12.4527 - mae: 1.9744\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.8792 - mae: 1.9503\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.3889 - mae: 1.8935\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.5015 - mae: 1.8819\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.2120 - mae: 1.8613\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.6176 - mae: 1.8353\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.7548 - mae: 1.8280\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.7170 - mae: 1.8104\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.5600 - mae: 1.7900\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.1207 - mae: 1.7740\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.7318 - mae: 1.7492\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.0029 - mae: 1.7663\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.9588 - mae: 1.7446\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.7081 - mae: 1.7350\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2930 - mae: 1.7095\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2148 - mae: 1.7068\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.6045 - mae: 1.7280\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.1915 - mae: 1.7146\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.0247 - mae: 1.6952\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.8108 - mae: 1.6997\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.9533 - mae: 1.6901\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.8288 - mae: 1.6734\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.9232 - mae: 1.6824\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2427 - mae: 1.6906\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.4701 - mae: 1.7093\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.8979 - mae: 1.6570\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.8324 - mae: 1.6657\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.4536 - mae: 1.6368\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.3610 - mae: 1.6299\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.4362 - mae: 1.6511\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.4850 - mae: 1.6364\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.5051 - mae: 1.6346\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.6889 - mae: 1.6496\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.2217 - mae: 1.6226\n",
      "82/82 [==============================] - 1s 3ms/step\n",
      "Kappa Score: 0.9580388408590718\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viksp\\AppData\\Local\\Temp\\ipykernel_28304\\3902004401.py:33: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 5s 11ms/step - loss: 63.1291 - mae: 4.3140\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 39.9834 - mae: 3.6282\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 34.6842 - mae: 3.5232\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 32.3033 - mae: 3.4521\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 29.8775 - mae: 3.2628\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 28.0656 - mae: 3.0989\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 26.0231 - mae: 2.9182\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 22.2803 - mae: 2.6957\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 18.6244 - mae: 2.4493\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 17.4834 - mae: 2.3605\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 16.4158 - mae: 2.2690\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 15.4469 - mae: 2.2044\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 14.3639 - mae: 2.1386\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 14.2443 - mae: 2.1221\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 13.3167 - mae: 2.0498\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 12.8595 - mae: 2.0142\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 12.8650 - mae: 1.9819\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 12.2432 - mae: 1.9552\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 12.2952 - mae: 1.9314\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 12.1074 - mae: 1.8989\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.1206 - mae: 1.8587\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.1141 - mae: 1.8400\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.3298 - mae: 1.8388\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.8665 - mae: 1.8196\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.8083 - mae: 1.8066\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.4734 - mae: 1.7962\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.2233 - mae: 1.7641\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.3800 - mae: 1.7651\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.1791 - mae: 1.7757\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.0196 - mae: 1.7387\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.7541 - mae: 1.7296\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.4916 - mae: 1.7191\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.7334 - mae: 1.7247\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.4940 - mae: 1.7198\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.4162 - mae: 1.7011\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.3468 - mae: 1.6972\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.9141 - mae: 1.6707\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.4267 - mae: 1.6894\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.3760 - mae: 1.6985\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.9078 - mae: 1.6591\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.9240 - mae: 1.6498\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.8852 - mae: 1.6514\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.6526 - mae: 1.6468\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.4322 - mae: 1.6314\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.5237 - mae: 1.6395\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.7321 - mae: 1.6336\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.4067 - mae: 1.6197\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.3842 - mae: 1.6166\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.1715 - mae: 1.6025\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.1920 - mae: 1.5990\n",
      "82/82 [==============================] - 1s 3ms/step\n",
      "WARNING:tensorflow:HDF5 format does not save weights of `optimizer_experimental.Optimizer`, your optimizer will be recompiled at loading time.\n",
      "Kappa Score: 0.9572015138610435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "            \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Avg. Kappa Score is 0.961 which is the highest we have ever seen on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.9567\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('callingoutbluff')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "89eb1e8fcd1790228faf8d8d58a5698f1d4b3e86fe5fa8aa1c0fc5ec965f26af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
