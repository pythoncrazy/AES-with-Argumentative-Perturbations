{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e625f2f2",
   "metadata": {},
   "source": [
    "# quick inference poc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709017a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ead18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e522863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.big_bird because of the following error (look up to see its traceback):\ncannot import name '_sentencepiece' from partially initialized module 'sentencepiece' (most likely due to a circular import) (c:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\sentencepiece\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\transformers\\utils\\import_utils.py:1063\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1063\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1064\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\transformers\\models\\__init__.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     albert,\n\u001b[0;32m     21\u001b[0m     auto,\n\u001b[0;32m     22\u001b[0m     bart,\n\u001b[0;32m     23\u001b[0m     barthez,\n\u001b[0;32m     24\u001b[0m     bartpho,\n\u001b[0;32m     25\u001b[0m     beit,\n\u001b[0;32m     26\u001b[0m     bert,\n\u001b[0;32m     27\u001b[0m     bert_generation,\n\u001b[0;32m     28\u001b[0m     bert_japanese,\n\u001b[0;32m     29\u001b[0m     bertweet,\n\u001b[0;32m     30\u001b[0m     big_bird,\n\u001b[0;32m     31\u001b[0m     bigbird_pegasus,\n\u001b[0;32m     32\u001b[0m     blenderbot,\n\u001b[0;32m     33\u001b[0m     blenderbot_small,\n\u001b[0;32m     34\u001b[0m     bloom,\n\u001b[0;32m     35\u001b[0m     bort,\n\u001b[0;32m     36\u001b[0m     byt5,\n\u001b[0;32m     37\u001b[0m     camembert,\n\u001b[0;32m     38\u001b[0m     canine,\n\u001b[0;32m     39\u001b[0m     clip,\n\u001b[0;32m     40\u001b[0m     codegen,\n\u001b[0;32m     41\u001b[0m     conditional_detr,\n\u001b[0;32m     42\u001b[0m     convbert,\n\u001b[0;32m     43\u001b[0m     convnext,\n\u001b[0;32m     44\u001b[0m     cpm,\n\u001b[0;32m     45\u001b[0m     ctrl,\n\u001b[0;32m     46\u001b[0m     cvt,\n\u001b[0;32m     47\u001b[0m     data2vec,\n\u001b[0;32m     48\u001b[0m     deberta,\n\u001b[0;32m     49\u001b[0m     deberta_v2,\n\u001b[0;32m     50\u001b[0m     decision_transformer,\n\u001b[0;32m     51\u001b[0m     deformable_detr,\n\u001b[0;32m     52\u001b[0m     deit,\n\u001b[0;32m     53\u001b[0m     detr,\n\u001b[0;32m     54\u001b[0m     dialogpt,\n\u001b[0;32m     55\u001b[0m     distilbert,\n\u001b[0;32m     56\u001b[0m     dit,\n\u001b[0;32m     57\u001b[0m     donut,\n\u001b[0;32m     58\u001b[0m     dpr,\n\u001b[0;32m     59\u001b[0m     dpt,\n\u001b[0;32m     60\u001b[0m     electra,\n\u001b[0;32m     61\u001b[0m     encoder_decoder,\n\u001b[0;32m     62\u001b[0m     ernie,\n\u001b[0;32m     63\u001b[0m     esm,\n\u001b[0;32m     64\u001b[0m     flaubert,\n\u001b[0;32m     65\u001b[0m     flava,\n\u001b[0;32m     66\u001b[0m     fnet,\n\u001b[0;32m     67\u001b[0m     fsmt,\n\u001b[0;32m     68\u001b[0m     funnel,\n\u001b[0;32m     69\u001b[0m     glpn,\n\u001b[0;32m     70\u001b[0m     gpt2,\n\u001b[0;32m     71\u001b[0m     gpt_neo,\n\u001b[0;32m     72\u001b[0m     gpt_neox,\n\u001b[0;32m     73\u001b[0m     gpt_neox_japanese,\n\u001b[0;32m     74\u001b[0m     gptj,\n\u001b[0;32m     75\u001b[0m     groupvit,\n\u001b[0;32m     76\u001b[0m     herbert,\n\u001b[0;32m     77\u001b[0m     hubert,\n\u001b[0;32m     78\u001b[0m     ibert,\n\u001b[0;32m     79\u001b[0m     imagegpt,\n\u001b[0;32m     80\u001b[0m     layoutlm,\n\u001b[0;32m     81\u001b[0m     layoutlmv2,\n\u001b[0;32m     82\u001b[0m     layoutlmv3,\n\u001b[0;32m     83\u001b[0m     layoutxlm,\n\u001b[0;32m     84\u001b[0m     led,\n\u001b[0;32m     85\u001b[0m     levit,\n\u001b[0;32m     86\u001b[0m     longformer,\n\u001b[0;32m     87\u001b[0m     longt5,\n\u001b[0;32m     88\u001b[0m     luke,\n\u001b[0;32m     89\u001b[0m     lxmert,\n\u001b[0;32m     90\u001b[0m     m2m_100,\n\u001b[0;32m     91\u001b[0m     marian,\n\u001b[0;32m     92\u001b[0m     markuplm,\n\u001b[0;32m     93\u001b[0m     maskformer,\n\u001b[0;32m     94\u001b[0m     mbart,\n\u001b[0;32m     95\u001b[0m     mbart50,\n\u001b[0;32m     96\u001b[0m     mctct,\n\u001b[0;32m     97\u001b[0m     megatron_bert,\n\u001b[0;32m     98\u001b[0m     megatron_gpt2,\n\u001b[0;32m     99\u001b[0m     mluke,\n\u001b[0;32m    100\u001b[0m     mmbt,\n\u001b[0;32m    101\u001b[0m     mobilebert,\n\u001b[0;32m    102\u001b[0m     mobilevit,\n\u001b[0;32m    103\u001b[0m     mpnet,\n\u001b[0;32m    104\u001b[0m     mt5,\n\u001b[0;32m    105\u001b[0m     mvp,\n\u001b[0;32m    106\u001b[0m     nezha,\n\u001b[0;32m    107\u001b[0m     nllb,\n\u001b[0;32m    108\u001b[0m     nystromformer,\n\u001b[0;32m    109\u001b[0m     openai,\n\u001b[0;32m    110\u001b[0m     opt,\n\u001b[0;32m    111\u001b[0m     owlvit,\n\u001b[0;32m    112\u001b[0m     pegasus,\n\u001b[0;32m    113\u001b[0m     pegasus_x,\n\u001b[0;32m    114\u001b[0m     perceiver,\n\u001b[0;32m    115\u001b[0m     phobert,\n\u001b[0;32m    116\u001b[0m     plbart,\n\u001b[0;32m    117\u001b[0m     poolformer,\n\u001b[0;32m    118\u001b[0m     prophetnet,\n\u001b[0;32m    119\u001b[0m     qdqbert,\n\u001b[0;32m    120\u001b[0m     rag,\n\u001b[0;32m    121\u001b[0m     realm,\n\u001b[0;32m    122\u001b[0m     reformer,\n\u001b[0;32m    123\u001b[0m     regnet,\n\u001b[0;32m    124\u001b[0m     rembert,\n\u001b[0;32m    125\u001b[0m     resnet,\n\u001b[0;32m    126\u001b[0m     retribert,\n\u001b[0;32m    127\u001b[0m     roberta,\n\u001b[0;32m    128\u001b[0m     roformer,\n\u001b[0;32m    129\u001b[0m     segformer,\n\u001b[0;32m    130\u001b[0m     sew,\n\u001b[0;32m    131\u001b[0m     sew_d,\n\u001b[0;32m    132\u001b[0m     speech_encoder_decoder,\n\u001b[0;32m    133\u001b[0m     speech_to_text,\n\u001b[0;32m    134\u001b[0m     speech_to_text_2,\n\u001b[0;32m    135\u001b[0m     splinter,\n\u001b[0;32m    136\u001b[0m     squeezebert,\n\u001b[0;32m    137\u001b[0m     swin,\n\u001b[0;32m    138\u001b[0m     swinv2,\n\u001b[0;32m    139\u001b[0m     t5,\n\u001b[0;32m    140\u001b[0m     tapas,\n\u001b[0;32m    141\u001b[0m     tapex,\n\u001b[0;32m    142\u001b[0m     time_series_transformer,\n\u001b[0;32m    143\u001b[0m     trajectory_transformer,\n\u001b[0;32m    144\u001b[0m     transfo_xl,\n\u001b[0;32m    145\u001b[0m     trocr,\n\u001b[0;32m    146\u001b[0m     unispeech,\n\u001b[0;32m    147\u001b[0m     unispeech_sat,\n\u001b[0;32m    148\u001b[0m     van,\n\u001b[0;32m    149\u001b[0m     videomae,\n\u001b[0;32m    150\u001b[0m     vilt,\n\u001b[0;32m    151\u001b[0m     vision_encoder_decoder,\n\u001b[0;32m    152\u001b[0m     vision_text_dual_encoder,\n\u001b[0;32m    153\u001b[0m     visual_bert,\n\u001b[0;32m    154\u001b[0m     vit,\n\u001b[0;32m    155\u001b[0m     vit_mae,\n\u001b[0;32m    156\u001b[0m     vit_msn,\n\u001b[0;32m    157\u001b[0m     wav2vec2,\n\u001b[0;32m    158\u001b[0m     wav2vec2_conformer,\n\u001b[0;32m    159\u001b[0m     wav2vec2_phoneme,\n\u001b[0;32m    160\u001b[0m     wav2vec2_with_lm,\n\u001b[0;32m    161\u001b[0m     wavlm,\n\u001b[0;32m    162\u001b[0m     whisper,\n\u001b[0;32m    163\u001b[0m     x_clip,\n\u001b[0;32m    164\u001b[0m     xglm,\n\u001b[0;32m    165\u001b[0m     xlm,\n\u001b[0;32m    166\u001b[0m     xlm_prophetnet,\n\u001b[0;32m    167\u001b[0m     xlm_roberta,\n\u001b[0;32m    168\u001b[0m     xlm_roberta_xl,\n\u001b[0;32m    169\u001b[0m     xlnet,\n\u001b[0;32m    170\u001b[0m     yolos,\n\u001b[0;32m    171\u001b[0m     yoso,\n\u001b[0;32m    172\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\transformers\\models\\mt5\\__init__.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m is_sentencepiece_available():\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mt5\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenization_t5\u001b[39;00m \u001b[39mimport\u001b[39;00m T5Tokenizer\n\u001b[0;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict, List, Optional, Tuple\n\u001b[1;32m---> 24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msentencepiece\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspm\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenization_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedTokenizer\n",
      "File \u001b[1;32mc:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\sentencepiece\\__init__.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m __package__ \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39m__name__\u001b[39m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _sentencepiece\n\u001b[0;32m     14\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_sentencepiece' from partially initialized module 'sentencepiece' (most likely due to a circular import) (c:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\sentencepiece\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\viksp\\Documents\\Folder_of_Folders\\Polygence_code\\argument-mining\\argument-mining\\experiments\\yousef\\inference.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/viksp/Documents/Folder_of_Folders/Polygence_code/argument-mining/argument-mining/experiments/yousef/inference.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# -- public imports\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/viksp/Documents/Folder_of_Folders/Polygence_code/argument-mining/argument-mining/experiments/yousef/inference.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BigBirdTokenizer, AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/viksp/Documents/Folder_of_Folders/Polygence_code/argument-mining/argument-mining/experiments/yousef/inference.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/viksp/Documents/Folder_of_Folders/Polygence_code/argument-mining/argument-mining/experiments/yousef/inference.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\transformers\\utils\\import_utils.py:1053\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1051\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1052\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1053\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1054\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1055\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\transformers\\utils\\import_utils.py:1065\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m   1064\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1065\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1066\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1067\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1068\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.big_bird because of the following error (look up to see its traceback):\ncannot import name '_sentencepiece' from partially initialized module 'sentencepiece' (most likely due to a circular import) (c:\\Users\\viksp\\Anaconda3\\envs\\argumin\\lib\\site-packages\\sentencepiece\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# -- public imports\n",
    "\n",
    "from transformers import BigBirdTokenizer, AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1b3df",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'backcall'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# -- private import\n",
    "from argminer.data import KaggleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ce1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# -- dev imports\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eff33ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "Some weights of the model checkpoint at google/bigbird-roberta-large were not used when initializing BigBirdForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = 'google/bigbird-roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, add_prefix_space=True)\n",
    "config_model = AutoConfig.from_pretrained(model) \n",
    "config_model.num_labels = 3\n",
    "model = AutoModelForTokenClassification.from_pretrained(model, config=config_model)\n",
    "optimizer = torch.optim.Adam(params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ee5553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "            'Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble!',\n",
    "            'Thing about! '\n",
    "        ]\n",
    "labels = [\n",
    "    ['O', 'O', 'O', 'O', 'B-PERS', 'I-PERS'],\n",
    "    ['O', 'O', 'O', 'B-PERS']\n",
    "]\n",
    "\n",
    "# TODO see if bert can accept text inputs?\n",
    "labels_numeric = [\n",
    "    [0, 0, 0, 0, 1, 2],\n",
    "    [0, 0, 0, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dd5d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'labels': labels_numeric\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f971c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KaggleDataset(\n",
    "    df,\n",
    "    tokenizer,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "65e8b6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   65, 16003,   717,  1539,   419,   676,   617,   992,   500,  6378,\n",
      "            66,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   65, 16003,   415,   817,  1012, 47489,    66,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'word_ids': tensor([[-1,  0,  1,  2,  3,  4,  4,  4,  5,  5, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1],\n",
      "        [-1,  0,  1,  2,  3,  3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1]]), 'word_id_mask': [tensor([False, False]), tensor([True, True]), tensor([True, True]), tensor([True, True]), tensor([True, True]), tensor([True, True]), tensor([ True, False]), tensor([ True, False]), tensor([ True, False]), tensor([ True, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False])]}\n",
      "tensor([[[-0.1642,  0.0992,  0.3482],\n",
      "         [-0.0166, -0.3431,  0.7926],\n",
      "         [ 0.0151,  0.1921,  0.6123],\n",
      "         [ 0.0664, -0.0830,  0.5713],\n",
      "         [ 0.0521, -0.2243,  0.7010],\n",
      "         [ 0.2475, -0.2039,  0.3427],\n",
      "         [ 0.0468, -0.2063,  0.4431],\n",
      "         [ 0.3500, -0.1089,  0.4890],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [ 0.2735, -0.3232,  0.6088],\n",
      "         [ 0.0164, -0.2838,  0.7500],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.3618, -0.0890,  0.4821],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [ 0.0758, -0.3785,  0.5925],\n",
      "         [-0.1642,  0.0992,  0.3482]],\n",
      "\n",
      "        [[-0.1558,  0.0520,  0.3420],\n",
      "         [-0.1565,  0.0522,  0.3423],\n",
      "         [-0.1566,  0.0523,  0.3424],\n",
      "         [-0.1557,  0.0502,  0.3406],\n",
      "         [-0.1565,  0.0522,  0.3423],\n",
      "         [-0.1569,  0.0525,  0.3428],\n",
      "         [-0.1566,  0.0523,  0.3425],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1565,  0.0523,  0.3424],\n",
      "         [-0.1565,  0.0522,  0.3423],\n",
      "         [-0.1565,  0.0523,  0.3424],\n",
      "         [-0.1564,  0.0522,  0.3423],\n",
      "         [-0.1564,  0.0522,  0.3423],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422]]], grad_fn=<AddBackward0>)\n",
      "tensor([-1,  0,  1,  2,  3,  4,  4,  4,  5,  5, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1])\n",
      "tensor([[-0.0166, -0.3431,  0.7926],\n",
      "        [ 0.0151,  0.1921,  0.6123],\n",
      "        [ 0.0664, -0.0830,  0.5713],\n",
      "        [ 0.0521, -0.2243,  0.7010],\n",
      "        [ 0.2475, -0.2039,  0.3427],\n",
      "        [ 0.0468, -0.2063,  0.4431],\n",
      "        [ 0.3500, -0.1089,  0.4890],\n",
      "        [-0.1642,  0.0992,  0.3482],\n",
      "        [ 0.2735, -0.3232,  0.6088]], grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0166, -0.3431,  0.7926],\n",
      "        [ 0.0151,  0.1921,  0.6123],\n",
      "        [ 0.0664, -0.0830,  0.5713],\n",
      "        [ 0.0521, -0.2243,  0.7010],\n",
      "        [ 0.6442, -0.5192,  1.2748],\n",
      "        [ 0.1094, -0.2241,  0.9570]], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([-1,  0,  1,  2,  3,  3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1])\n",
      "tensor([[-0.1565,  0.0522,  0.3423],\n",
      "        [-0.1566,  0.0523,  0.3424],\n",
      "        [-0.1557,  0.0502,  0.3406],\n",
      "        [-0.1565,  0.0522,  0.3423],\n",
      "        [-0.1569,  0.0525,  0.3428]], grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1565,  0.0522,  0.3423],\n",
      "        [-0.1566,  0.0523,  0.3424],\n",
      "        [-0.1557,  0.0502,  0.3406],\n",
      "        [-0.3134,  0.1048,  0.6851]], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, targets) in enumerate(loader):\n",
    "    print(inputs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # for evaluation purposes may need to save the embedding weights??\n",
    "    \n",
    "    word_ids = inputs['word_ids']\n",
    "    word_id_mask = inputs['word_id_mask']\n",
    "    loss, outputs = model(\n",
    "        labels=targets,\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        return_dict=False\n",
    "    )\n",
    "    print(outputs)\n",
    "    inference(outputs, word_ids, 'avg')\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3245b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(batch, word_id_batch, agg_strategy):\n",
    "    # TODO need to have assertions to prevent certain strategies from happening together!\n",
    "\n",
    "\n",
    "    for (predictions, word_ids) in zip(batch, word_id_batch):\n",
    "        # TODO probably no need to store mask?\n",
    "        mask = word_ids != -1\n",
    "        print(word_ids)\n",
    "        word_ids = word_ids[mask]\n",
    "        predictions = predictions[mask]\n",
    "        print(predictions)\n",
    "        unique_word_ids, word_id_counts = torch.unique_consecutive(word_ids, return_counts=True)\n",
    "        agg_predictions = torch.zeros((len(unique_word_ids), predictions.shape[-1]), dtype=float)\n",
    "        start_id = 0\n",
    "        for i, (unique_word_id, word_id_count) in enumerate(zip(unique_word_ids, word_id_counts)):\n",
    "            end_id = start_id + word_id_count\n",
    "            agg_predictions[i] = predictions[start_id: end_id]\n",
    "            start_id = end_id\n",
    "        print(agg_predictions)\n",
    "    return agg_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "803ba66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b749b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('argumin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "993107e58abe0d6492059bae373968835d9e1848a5bcebe514ea1f633a3ddf74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
