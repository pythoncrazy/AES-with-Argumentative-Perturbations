{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to load flags\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import data_utils_adv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from qwk import quadratic_weighted_kappa\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "print( 'start to load flags\\n')\n",
    "\n",
    "# flags\n",
    "# tf.flags.DEFINE_float(\"epsilon\", 0.1, \"Epsilon value for Adam Optimizer.\")\n",
    "# tf.flags.DEFINE_float(\"l2_lambda\", 0.3, \"Lambda for l2 loss.\")\n",
    "# tf.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate\")\n",
    "# tf.flags.DEFINE_float(\"max_grad_norm\", 10.0, \"Clip gradients to this norm.\")\n",
    "# tf.flags.DEFINE_float(\"keep_prob\", 0.9, \"Keep probability for dropout\")\n",
    "# tf.flags.DEFINE_integer(\"evaluation_interval\", 5, \"Evaluate and print results every x epochs\")\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 15, \"Batch size for training.\")\n",
    "# tf.flags.DEFINE_integer(\"feature_size\", 100, \"Feature size\")\n",
    "# tf.flags.DEFINE_integer(\"num_samples\", 1, \"Number of samples selected from training for each score\")\n",
    "# tf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "# tf.flags.DEFINE_integer(\"epochs\", 200, \"Number of epochs to train for.\")\n",
    "# tf.flags.DEFINE_integer(\"embedding_size\", 300, \"Embedding size for embedding matrices.\")\n",
    "# tf.flags.DEFINE_integer(\"essay_set_id\", 1, \"essay set id, 1 <= id <= 8\")\n",
    "# tf.flags.DEFINE_integer(\"token_num\", 42, \"The number of token in glove (6, 42)\")\n",
    "# tf.flags.DEFINE_boolean(\"gated_addressing\", False, \"Simple gated addressing\")\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"is_regression\", False, \"The output is regression or classification\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "# # hyper-parameters\n",
    "# FLAGS = tf.flags.FLAGS\n",
    "\n",
    "early_stop_count = 0\n",
    "max_step_count = 10\n",
    "is_regression = False\n",
    "gated_addressing = False\n",
    "essay_set_id = 1\n",
    "batch_size = 15\n",
    "embedding_size = 300\n",
    "feature_size = 100\n",
    "l2_lambda = 0.3\n",
    "hops = 3\n",
    "reader = 'bow' # gru may not work\n",
    "epochs = 200\n",
    "num_samples = 1\n",
    "num_tokens = 42\n",
    "test_batch_size = batch_size\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /mnt/data/rajivratn/memory_networks/automated-essay-grading/runs/adversary_training/essay_set_1_cv_1_Mar_25_2020_21:21:22\n",
      "\n",
      "                                               names  essay_id\n",
      "0  8, \"Dear local newspaper, I think effects comp...         1\n",
      "1  10, \"Dear Local Newspaper, @CAPS1 I have found...         2\n",
      "2  9, \"Dear reader, @ORGANIZATION1 has had a dram...         3\n",
      "3  10, \"I agree that computers deffinately are an...         4\n",
      "4  8, \"Dear local Newspaper, @CAPS1 in the societ...         5\n",
      "max_score is 12 \t min_score is 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if is_regression:\n",
    "    from memn2n_kv_regression import MemN2N_KV\n",
    "else:\n",
    "    from memn2n_kv import MemN2N_KV\n",
    "# print flags info\n",
    "orig_stdout = sys.stdout\n",
    "timestamp = time.strftime(\"%b_%d_%Y_%H:%M:%S\", time.localtime())\n",
    "folder_name = 'essay_set_{}_cv_{}_{}'.format(essay_set_id, num_samples, timestamp)\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs/adversary_training/\", folder_name))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# save output to a file\n",
    "#f = file(out_dir+'/out.txt', 'w')\n",
    "#sys.stdout = f\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "# print(\"\\nParameters:\")\n",
    "# for key in sorted(FLAGS.__flags.keys()):\n",
    "#     print(\"{}={}\".format(key, getattr(FLAGS, key)))\n",
    "# print(\"\")\n",
    "\n",
    "# with open(out_dir+'/params', 'w') as f:\n",
    "#     for key in sorted(FLAGS.__flags.keys()):\n",
    "#         f.write(\"{}={}\".format(key, getattr(FLAGS, key)))\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# hyper-parameters end here\n",
    "training_path = 'train_adv/prompt1/noDisfluency&grammar_1_train_valid_reduce.csv'\n",
    "essay_list, resolved_scores, essay_id = data_utils_adv.load_training_data(training_path, essay_set_id)\n",
    "\n",
    "# print(essay_id)\n",
    "testing_path = 'aes_data/essay1/fold_0/test.txt'\n",
    "essay_list_test, resolved_scores_test, essay_id_test = data_utils_adv.load_testing_data(testing_path, essay_set_id)\n",
    "\n",
    "max_score = max(resolved_scores)\n",
    "min_score = min(resolved_scores)\n",
    "if essay_set_id == 7:\n",
    "    min_score, max_score = 0, 30\n",
    "elif essay_set_id == 8:\n",
    "    min_score, max_score = 0, 60\n",
    "\n",
    "print( 'max_score is {} \\t min_score is {}\\n'.format(max_score, min_score))\n",
    "with open(out_dir+'/params', 'a') as f:\n",
    "    f.write('max_score is {} \\t min_score is {} \\n'.format(max_score, min_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> glove is loaded\n",
      "max sentence size: 911 \n",
      "mean sentence size: 394\n",
      "\n",
      "The length of score range is 11\n"
     ]
    }
   ],
   "source": [
    "score_range = range(min_score, max_score+1)\n",
    "\n",
    "#word_idx, _ = data_utils.build_vocab(essay_list, vocab_limit)\n",
    "\n",
    "# load glove\n",
    "word_idx, word2vec = data_utils_adv.load_glove(num_tokens, dim=embedding_size)\n",
    "\n",
    "vocab_size = len(word_idx) + 1\n",
    "# stat info on data set\n",
    "\n",
    "sent_size_list = list(map(len, [essay for essay in essay_list]))\n",
    "# print(\"sent size list\", sent_size_list)\n",
    "max_sent_size = max(sent_size_list)\n",
    "mean_sent_size = int(np.mean(sent_size_list))\n",
    "\n",
    "print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
    "with open(out_dir+'/params', 'a') as f:\n",
    "    f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
    "\n",
    "print( 'The length of score range is {}'.format(len(score_range)))\n",
    "E = data_utils_adv.vectorize_data(essay_list, word_idx, max_sent_size)\n",
    "\n",
    "labeled_data = zip(E, resolved_scores, sent_size_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence size: 811 \n",
      "mean sentence size: 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Testing\n",
    "sent_size_list_T = list(map(len, [essayT for essayT in essay_list_test]))\n",
    "max_sent_size_T = max(sent_size_list_T)\n",
    "mean_sent_size_T = int(np.mean(sent_size_list_T))\n",
    "\n",
    "# max_sent_size_T = max_sent_size_T.ljust(122, ' ')\n",
    "print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size_T, mean_sent_size_T))\n",
    "with open(out_dir+'/params', 'a') as f:\n",
    "    f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size_T, mean_sent_size_T))\n",
    "\n",
    "# print( 'The length of score range of Testing Daat is {}'.format(len(score_range)))\n",
    "E_T = data_utils_adv.vectorize_data(essay_list_test, word_idx, max_sent_size_T)\n",
    "\n",
    "labeled_data_T = zip(E_T, resolved_scores_test, sent_size_list_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(m, e, s, ma):\n",
    "    start_time = time.time()\n",
    "    feed_dict = {\n",
    "        model._query: e,\n",
    "        model._memory_key: m,\n",
    "        model._score_encoding: s,\n",
    "        model._mem_attention_encoding: ma,\n",
    "        model.keep_prob: 0.9\n",
    "        #model.w_placeholder: word2vec\n",
    "    }\n",
    "    _, step, predict_op, cost = sess.run([train_op, global_step, model.predict_op, model.cost], feed_dict)\n",
    "    end_time = time.time()\n",
    "    time_spent = end_time - start_time\n",
    "    return predict_op, cost, time_spent\n",
    "\n",
    "def test_step(e, m):\n",
    "    feed_dict = {\n",
    "        model._query: e,\n",
    "        model._memory_key: m,\n",
    "        model.keep_prob: 1\n",
    "        #model.w_placeholder: word2vec\n",
    "    }\n",
    "    preds, mem_attention_probs = sess.run([model.predict_op, model.mem_attention_probs], feed_dict)\n",
    "    if is_regression:\n",
    "        preds = np.clip(np.round(preds), min_score, max_score)\n",
    "        return preds, mem_attention_probs\n",
    "    else:\n",
    "        return preds, mem_attention_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n",
      "The size of training data: 1415\n",
      "The size of testing data: 357\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410)]\n",
      "Finish epoch 1, total training cost is 571464.4639892578, time spent is 3.193020820617676\n",
      "Finish epoch 2, total training cost is 119654.56945800781, time spent is 3.0785653591156006\n",
      "Finish epoch 3, total training cost is 73020.61526489258, time spent is 3.205556869506836\n",
      "Finish epoch 4, total training cost is 59678.625579833984, time spent is 3.217996835708618\n",
      "Finish epoch 5, total training cost is 54971.46662902832, time spent is 3.1834323406219482\n",
      "Training kappa score = 0.4779711883708012\n",
      "Testing kappa score = 0.044458558272467585\n",
      "Finish epoch 6, total training cost is 49106.27813720703, time spent is 3.308295726776123\n",
      "Finish epoch 7, total training cost is 45636.72431945801, time spent is 3.0067338943481445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8695a13c277b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m                         \u001b[0mmem_atten_encoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mbatched_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_atten_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                     \u001b[0mtotal_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_spent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0mtrain_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9bf71e03ed07>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(m, e, s, ma)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#model.w_placeholder: word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fold_count = 0\n",
    "# kf = KFold(n_splits=2, random_state=random_state)\n",
    "early_stop_count = 0\n",
    "\n",
    "trainE = []\n",
    "train_scores = []\n",
    "train_essay_id = []\n",
    "best_kappa_scores = []\n",
    "\n",
    "testE = []\n",
    "test_scores = []\n",
    "test_essay_id = []\n",
    "\n",
    "print(len(E_T))\n",
    "# print(len(resolved_scores_test))\n",
    "# print(len(essay_list_test))\n",
    "for test_index in range(len(essay_id_test)):\n",
    "    testE.append(E[test_index])\n",
    "#     print(len(testE))\n",
    "    test_scores.append(resolved_scores_test[test_index])\n",
    "    test_essay_id.append(essay_id_test[test_index])\n",
    "\n",
    "for train_index in range(len(essay_id)):\n",
    "#     print(\"Train_index\", train_index)\n",
    "#     result = [int(x) for x, in train_index[0]]\n",
    "#     print(result)\n",
    "#     early_stop_count = 0\n",
    "#     fold_count += 1\n",
    "#     trainE = []\n",
    "#     testE = []\n",
    "#     train_scores = []\n",
    "#     test_scores = []\n",
    "#     train_essay_id = []\n",
    "#     test_essay_id = []\n",
    "\n",
    "    trainE.append(E[train_index])\n",
    "    train_scores.append(resolved_scores[train_index])\n",
    "    train_essay_id.append(essay_id[train_index])\n",
    "    \n",
    "#     for ite in train_index[0]:\n",
    "# #         print(ite)\n",
    "#         trainE.append(E[ite])\n",
    "#         train_scores.append(resolved_scores[ite])\n",
    "#         train_essay_id.append(essay_id[ite])\n",
    "#         print(\"Training Samples\", E[ite], resolved_scores[ite], essay_id[ite])\n",
    "        \n",
    "#     for ite in test_index:\n",
    "#         testE.append(E[ite])\n",
    "#         test_scores.append(resolved_scores[ite])\n",
    "#         test_essay_id.append(essay_id[ite])\n",
    "    \n",
    "#trainE, testE, train_scores, test_scores, train_essay_id, test_essay_id = cross_validation.train_test_split(\n",
    "#    E, resolved_scores, essay_id, test_size=.2, random_state=random_state)\n",
    "\n",
    "memory = []\n",
    "memory_score = []\n",
    "memory_sent_size = []\n",
    "memory_essay_ids = []\n",
    "# pick sampled essay for each score\n",
    "for i in score_range:\n",
    "# test point: limit the number of samples in memory for 8\n",
    "    for j in range(num_samples):\n",
    "        if i in train_scores:\n",
    "            score_idx = train_scores.index(i)\n",
    "            score = train_scores.pop(score_idx)\n",
    "            essay = trainE.pop(score_idx)\n",
    "            sent_size = sent_size_list.pop(score_idx)\n",
    "            memory.append(essay)\n",
    "            memory_score.append(score)\n",
    "            memory_essay_ids.append(train_essay_id.pop(score_idx))\n",
    "            memory_sent_size.append(sent_size)\n",
    "memory_size = len(memory)\n",
    "if is_regression:\n",
    "# bad naming\n",
    "    train_scores_encoding = train_scores\n",
    "else:\n",
    "    train_scores_encoding = list(map(lambda x: score_range.index(x), train_scores))\n",
    "\n",
    "    # data size\n",
    "n_train = len(trainE)\n",
    "n_test = len(testE)\n",
    "\n",
    "print( 'The size of training data: {}'.format(n_train))\n",
    "print( 'The size of testing data: {}'.format(n_test))\n",
    "with open(out_dir+'/params_'.format(names), 'a') as f:\n",
    "    f.write('The size of training data: {}\\n'.format(n_train))\n",
    "    f.write('The size of testing data: {}\\n'.format(n_test))\n",
    "    f.write('\\nEssay scores in memory:\\n{}'.format(memory_score))\n",
    "    f.write('\\nEssay ids in memory:\\n{}'.format(memory_essay_ids))\n",
    "    f.write('\\nEssay ids in training:\\n{}'.format(train_essay_id))\n",
    "    f.write('\\nEssay ids in testing:\\n{}'.format(test_essay_id))\n",
    "\n",
    "batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "batches = [(start, end) for start, end in batches]\n",
    "print(batches)\n",
    "x = 1\n",
    "if x == 1:\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=False)\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        # decay learning rate\n",
    "        starter_learning_rate = 0.0001\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
    "\n",
    "        # test point\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.1)\n",
    "        #optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "        best_kappa_so_far = 0.0\n",
    "        with tf.Session(config=session_conf) as sess:\n",
    "            model = MemN2N_KV(batch_size, vocab_size, max_sent_size, max_sent_size, memory_size,\n",
    "                              memory_size, embedding_size, len(score_range), feature_size, hops, reader, l2_lambda)\n",
    "\n",
    "            grads_and_vars = optimizer.compute_gradients(model.loss_op, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "            grads_and_vars = [(tf.clip_by_norm(g, 10.0), v)\n",
    "                              for g, v in grads_and_vars if g is not None]\n",
    "            #grads_and_vars = [(add_gradient_noise(g, 1e-4), v) for g, v in grads_and_vars]\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
    "            sess.run(tf.global_variables_initializer(), feed_dict={model.w_placeholder: word2vec})\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "            for i in range(1, epochs+1):\n",
    "                train_cost = 0\n",
    "                total_time = 0\n",
    "                np.random.shuffle(batches)\n",
    "                for start, end in batches:\n",
    "                    e = trainE[start:end]\n",
    "                    s = train_scores_encoding[start:end]\n",
    "                    s_num = train_scores[start:end]\n",
    "                    #batched_memory = []\n",
    "                    # batch sized memory\n",
    "                    #for _ in range(len(e)):\n",
    "                    #    batched_memory.append(memory)\n",
    "                    mem_atten_encoding = []\n",
    "                    for ite in s_num:\n",
    "                        mem_encoding = np.zeros(memory_size)\n",
    "                        for j_idx, j in enumerate(memory_score):\n",
    "                            if j == ite:\n",
    "                                mem_encoding[j_idx] = 1\n",
    "                        mem_atten_encoding.append(mem_encoding)\n",
    "                    batched_memory = [memory] * (end-start)\n",
    "                    _, cost, time_spent = train_step(batched_memory, e, s, mem_atten_encoding)\n",
    "                    total_time += time_spent\n",
    "                    train_cost += cost\n",
    "                print( 'Finish epoch {}, total training cost is {}, time spent is {}'.format(i, train_cost, total_time))\n",
    "                # evaluation\n",
    "                if i % 5 == 0 or i == 200:\n",
    "                    # test on training data\n",
    "                    train_preds = []\n",
    "                    for start in range(0, n_train, test_batch_size):\n",
    "                        end = min(n_train, start+test_batch_size)\n",
    "\n",
    "                        #batched_memory = []\n",
    "                        #for _ in range(end-start):\n",
    "                        #    batched_memory.append(memory)\n",
    "                        batched_memory = [memory] * (end-start)\n",
    "#                         print(\"BM\", len(batched_memory))\n",
    "                        preds, _ = test_step(trainE[start:end], batched_memory)\n",
    "                        if type(preds) is np.float32:\n",
    "                            train_preds.append(preds)\n",
    "                        else:\n",
    "                            for ite in preds:\n",
    "                                train_preds.append(ite)\n",
    "                    if not is_regression:\n",
    "                        train_preds = np.add(train_preds, min_score)\n",
    "                    #train_kappp_score = kappa(train_scores, train_preds, 'quadratic')\n",
    "                    train_kappp_score = quadratic_weighted_kappa(\n",
    "                        train_scores, train_preds, min_score, max_score)\n",
    "                    # test on test data\n",
    "                    test_preds = []\n",
    "                    test_atten_probs = []\n",
    "                    for start in range(0, n_test, test_batch_size):\n",
    "                        end = min(n_test, start+test_batch_size)\n",
    "\n",
    "                        #batched_memory = []\n",
    "                        #for _ in range(end-start):\n",
    "                        #    batched_memory.append(memory)\n",
    "                        batched_memory = [memory] * (end-start)\n",
    "#                         print(\"Test\", len(testE[start:end]))\n",
    "                        \n",
    "                        preds, mem_attention_probs = test_step(testE[start:end], batched_memory)\n",
    "                        if type(preds) is np.float32:\n",
    "                            test_preds.append(preds)\n",
    "                        else:\n",
    "                            for ite in preds:\n",
    "                                test_preds.append(ite)\n",
    "                        for ite in mem_attention_probs:\n",
    "                            test_atten_probs.append(ite)\n",
    "                    if not is_regression:\n",
    "                        test_preds = np.add(test_preds, min_score)\n",
    "                    #test_kappp_score = kappa(test_scores, test_preds, 'quadratic')\n",
    "                    test_kappp_score = quadratic_weighted_kappa(\n",
    "                        test_scores, test_preds, min_score, max_score)\n",
    "                    stat_dict = {'pred_score': test_preds}\n",
    "                    stat_df = pd.DataFrame(stat_dict)\n",
    "                    # save the model if it gets best kappa\n",
    "                    if(test_kappp_score > best_kappa_so_far):\n",
    "                        early_stop_count = 0\n",
    "                        best_kappa_so_far = test_kappp_score\n",
    "                        # stats on test\n",
    "                        stat_df.to_csv(out_dir+'/predScore_'+names)\n",
    "                        with open(out_dir+'/mem_atten', 'a') as f:\n",
    "                            for idx, ite in enumerate(test_essay_id):\n",
    "                                f.write('{}\\n'.format(ite))\n",
    "                                f.write('{}\\n'.format(test_atten_probs[idx]))\n",
    "#                         saver.save(sess, out_dir+'/checkpoints', global_step)\n",
    "#                         model.save('prompt1.h5')\n",
    "#                         tf.saved_model.save(model, 'runs/')\n",
    "                    else:\n",
    "                        early_stop_count += 1\n",
    "                    print(\"Training kappa score = {}\".format(train_kappp_score))\n",
    "                    print(\"Testing kappa score = {}\".format(test_kappp_score))\n",
    "                    with open(out_dir+'/eval_'.format(names), 'a') as f:\n",
    "                        f.write(\"Training kappa score = {}\\n\".format(train_kappp_score))\n",
    "                        f.write(\"Testing kappa score = {}\\n\".format(test_kappp_score))\n",
    "                        f.write(\"Best Testing kappa score so far = {}\\n\".format(best_kappa_so_far))\n",
    "                        f.write('*'*10)\n",
    "                        f.write('\\n')\n",
    "                if early_stop_count > max_step_count:\n",
    "                    break\n",
    "            best_kappa_scores.append(best_kappa_so_far)\n",
    "\n",
    "# model\n",
    "with open(out_dir+'/eval_'.format(names), 'a') as f:\n",
    "    f.write('5 fold cv {}\\n'.format(best_kappa_scores))\n",
    "    f.write('final result is {}'.format(np.mean(np.array(best_kappa_scores))))\n",
    "\n",
    "#sys.stdout = orig_stdout\n",
    "#f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2734,\n",
       "  458639,\n",
       "  1,\n",
       "  13,\n",
       "  65,\n",
       "  754,\n",
       "  3352,\n",
       "  17,\n",
       "  23,\n",
       "  1439,\n",
       "  19,\n",
       "  2,\n",
       "  4510,\n",
       "  6,\n",
       "  2,\n",
       "  94,\n",
       "  15,\n",
       "  1413,\n",
       "  131,\n",
       "  179,\n",
       "  141,\n",
       "  70,\n",
       "  19,\n",
       "  2597,\n",
       "  79,\n",
       "  94,\n",
       "  271150,\n",
       "  213,\n",
       "  15,\n",
       "  17,\n",
       "  562,\n",
       "  269,\n",
       "  6300,\n",
       "  4,\n",
       "  7083,\n",
       "  21,\n",
       "  56,\n",
       "  94,\n",
       "  2756,\n",
       "  2,\n",
       "  1444,\n",
       "  10,\n",
       "  15,\n",
       "  7,\n",
       "  574,\n",
       "  142,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  174,\n",
       "  17,\n",
       "  8,\n",
       "  502,\n",
       "  18,\n",
       "  8184,\n",
       "  17,\n",
       "  24,\n",
       "  2108,\n",
       "  40,\n",
       "  126,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  17,\n",
       "  41,\n",
       "  80,\n",
       "  1476,\n",
       "  1,\n",
       "  1609,\n",
       "  127,\n",
       "  2537,\n",
       "  19,\n",
       "  2,\n",
       "  885,\n",
       "  4,\n",
       "  1255,\n",
       "  61,\n",
       "  19,\n",
       "  167,\n",
       "  27,\n",
       "  7,\n",
       "  575,\n",
       "  677195,\n",
       "  10,\n",
       "  646,\n",
       "  27,\n",
       "  42,\n",
       "  13730,\n",
       "  391,\n",
       "  23,\n",
       "  10,\n",
       "  689,\n",
       "  5,\n",
       "  2,\n",
       "  502,\n",
       "  15,\n",
       "  115,\n",
       "  94,\n",
       "  1266,\n",
       "  4,\n",
       "  116,\n",
       "  1362,\n",
       "  1767,\n",
       "  115,\n",
       "  6,\n",
       "  106,\n",
       "  13730,\n",
       "  94,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  1767,\n",
       "  133,\n",
       "  45,\n",
       "  25,\n",
       "  16655,\n",
       "  5,\n",
       "  14903,\n",
       "  574,\n",
       "  176,\n",
       "  15734,\n",
       "  21,\n",
       "  468,\n",
       "  45,\n",
       "  69,\n",
       "  287,\n",
       "  19,\n",
       "  1902,\n",
       "  1,\n",
       "  115,\n",
       "  1362,\n",
       "  137,\n",
       "  117,\n",
       "  200,\n",
       "  45,\n",
       "  65,\n",
       "  545,\n",
       "  1902,\n",
       "  50,\n",
       "  36,\n",
       "  156,\n",
       "  5,\n",
       "  13764,\n",
       "  677195,\n",
       "  27,\n",
       "  170,\n",
       "  42,\n",
       "  13730,\n",
       "  391,\n",
       "  1,\n",
       "  116,\n",
       "  36,\n",
       "  264,\n",
       "  80,\n",
       "  94,\n",
       "  323,\n",
       "  29,\n",
       "  63,\n",
       "  574,\n",
       "  6151,\n",
       "  18,\n",
       "  10,\n",
       "  397,\n",
       "  64,\n",
       "  4342,\n",
       "  1,\n",
       "  133,\n",
       "  36,\n",
       "  25,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  349,\n",
       "  7,\n",
       "  1187,\n",
       "  36,\n",
       "  264,\n",
       "  223,\n",
       "  94,\n",
       "  69,\n",
       "  85,\n",
       "  443,\n",
       "  2,\n",
       "  574,\n",
       "  153,\n",
       "  10,\n",
       "  83,\n",
       "  1,\n",
       "  176,\n",
       "  94,\n",
       "  25,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  45,\n",
       "  65,\n",
       "  545,\n",
       "  1957,\n",
       "  61,\n",
       "  19,\n",
       "  2,\n",
       "  152,\n",
       "  45,\n",
       "  871,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  145,\n",
       "  885,\n",
       "  1,\n",
       "  45,\n",
       "  5499,\n",
       "  117,\n",
       "  57,\n",
       "  2,\n",
       "  253,\n",
       "  152,\n",
       "  10,\n",
       "  397,\n",
       "  66,\n",
       "  2031,\n",
       "  0,\n",
       "  170,\n",
       "  42,\n",
       "  1361,\n",
       "  4,\n",
       "  332,\n",
       "  75,\n",
       "  6954,\n",
       "  55,\n",
       "  2,\n",
       "  885,\n",
       "  152,\n",
       "  83,\n",
       "  1,\n",
       "  2,\n",
       "  125,\n",
       "  1255,\n",
       "  61,\n",
       "  19,\n",
       "  167,\n",
       "  2065,\n",
       "  4847,\n",
       "  2,\n",
       "  557,\n",
       "  6,\n",
       "  7216,\n",
       "  15,\n",
       "  94,\n",
       "  1134240,\n",
       "  1,\n",
       "  45,\n",
       "  76,\n",
       "  1255,\n",
       "  61,\n",
       "  19,\n",
       "  167,\n",
       "  27,\n",
       "  7,\n",
       "  575,\n",
       "  36,\n",
       "  98,\n",
       "  30,\n",
       "  51,\n",
       "  167,\n",
       "  1,\n",
       "  54,\n",
       "  17,\n",
       "  226,\n",
       "  676,\n",
       "  18,\n",
       "  18,\n",
       "  76,\n",
       "  28,\n",
       "  54,\n",
       "  2438,\n",
       "  11,\n",
       "  454,\n",
       "  5,\n",
       "  69,\n",
       "  1080,\n",
       "  155,\n",
       "  167,\n",
       "  170,\n",
       "  3407,\n",
       "  19,\n",
       "  33,\n",
       "  574,\n",
       "  997,\n",
       "  13,\n",
       "  1544,\n",
       "  331,\n",
       "  443,\n",
       "  11,\n",
       "  15,\n",
       "  396,\n",
       "  1,\n",
       "  116,\n",
       "  266,\n",
       "  18,\n",
       "  125,\n",
       "  28,\n",
       "  2119,\n",
       "  40,\n",
       "  15,\n",
       "  36,\n",
       "  30,\n",
       "  5,\n",
       "  47,\n",
       "  10,\n",
       "  161,\n",
       "  323,\n",
       "  2,\n",
       "  574,\n",
       "  50,\n",
       "  36,\n",
       "  47,\n",
       "  23,\n",
       "  116,\n",
       "  40,\n",
       "  2,\n",
       "  702,\n",
       "  41,\n",
       "  28,\n",
       "  7471,\n",
       "  904,\n",
       "  1,\n",
       "  50,\n",
       "  36,\n",
       "  80,\n",
       "  3891,\n",
       "  6,\n",
       "  2597,\n",
       "  36,\n",
       "  41,\n",
       "  28,\n",
       "  646,\n",
       "  27,\n",
       "  7,\n",
       "  56,\n",
       "  1572,\n",
       "  128,\n",
       "  29,\n",
       "  12242,\n",
       "  4,\n",
       "  1638,\n",
       "  620,\n",
       "  6602,\n",
       "  36,\n",
       "  41,\n",
       "  17106,\n",
       "  65759,\n",
       "  8,\n",
       "  431,\n",
       "  27,\n",
       "  2,\n",
       "  15,\n",
       "  164,\n",
       "  7,\n",
       "  140381,\n",
       "  4,\n",
       "  214,\n",
       "  3891,\n",
       "  6,\n",
       "  2597,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [45,\n",
       "  324,\n",
       "  5,\n",
       "  1266,\n",
       "  63,\n",
       "  3582,\n",
       "  133,\n",
       "  6,\n",
       "  85,\n",
       "  6612,\n",
       "  45,\n",
       "  25,\n",
       "  94,\n",
       "  25,\n",
       "  225,\n",
       "  2597,\n",
       "  5,\n",
       "  42,\n",
       "  3929,\n",
       "  5,\n",
       "  73,\n",
       "  18,\n",
       "  10,\n",
       "  443,\n",
       "  11,\n",
       "  63,\n",
       "  271,\n",
       "  23,\n",
       "  450,\n",
       "  2271,\n",
       "  367,\n",
       "  2,\n",
       "  949,\n",
       "  396,\n",
       "  44,\n",
       "  4683,\n",
       "  381,\n",
       "  61,\n",
       "  6,\n",
       "  63,\n",
       "  2092,\n",
       "  45,\n",
       "  25,\n",
       "  4004,\n",
       "  7,\n",
       "  1249,\n",
       "  6,\n",
       "  702,\n",
       "  8,\n",
       "  6193,\n",
       "  271,\n",
       "  1,\n",
       "  600,\n",
       "  1,\n",
       "  2507,\n",
       "  13,\n",
       "  44,\n",
       "  762,\n",
       "  200,\n",
       "  17,\n",
       "  76,\n",
       "  25643,\n",
       "  43,\n",
       "  3058,\n",
       "  38,\n",
       "  13,\n",
       "  24825,\n",
       "  455,\n",
       "  15,\n",
       "  17,\n",
       "  41,\n",
       "  28,\n",
       "  372,\n",
       "  5,\n",
       "  762,\n",
       "  200,\n",
       "  23,\n",
       "  10,\n",
       "  54,\n",
       "  411,\n",
       "  23,\n",
       "  44,\n",
       "  28,\n",
       "  7,\n",
       "  1111,\n",
       "  298,\n",
       "  1507,\n",
       "  173,\n",
       "  8,\n",
       "  2,\n",
       "  884,\n",
       "  131,\n",
       "  18,\n",
       "  10,\n",
       "  7,\n",
       "  10627,\n",
       "  4830,\n",
       "  6,\n",
       "  225,\n",
       "  7,\n",
       "  574,\n",
       "  179,\n",
       "  141,\n",
       "  67,\n",
       "  13,\n",
       "  145,\n",
       "  1716,\n",
       "  2,\n",
       "  574,\n",
       "  11,\n",
       "  98,\n",
       "  284697,\n",
       "  546,\n",
       "  43,\n",
       "  992,\n",
       "  324,\n",
       "  5,\n",
       "  286,\n",
       "  13,\n",
       "  30,\n",
       "  954,\n",
       "  6,\n",
       "  7,\n",
       "  314,\n",
       "  143,\n",
       "  7,\n",
       "  906,\n",
       "  31,\n",
       "  9065,\n",
       "  24290,\n",
       "  5,\n",
       "  163,\n",
       "  42,\n",
       "  3929,\n",
       "  15,\n",
       "  95,\n",
       "  310943,\n",
       "  19,\n",
       "  2,\n",
       "  353,\n",
       "  5,\n",
       "  223,\n",
       "  85,\n",
       "  9610,\n",
       "  95,\n",
       "  31,\n",
       "  2,\n",
       "  574,\n",
       "  997,\n",
       "  10,\n",
       "  30591,\n",
       "  63,\n",
       "  992,\n",
       "  17806,\n",
       "  11,\n",
       "  2,\n",
       "  94,\n",
       "  74,\n",
       "  25,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  249698,\n",
       "  186231,\n",
       "  418,\n",
       "  7,\n",
       "  126,\n",
       "  25,\n",
       "  488,\n",
       "  141,\n",
       "  3352,\n",
       "  63,\n",
       "  992,\n",
       "  5,\n",
       "  19495,\n",
       "  1161,\n",
       "  23473,\n",
       "  7,\n",
       "  1009,\n",
       "  5,\n",
       "  110,\n",
       "  63,\n",
       "  10970,\n",
       "  9610,\n",
       "  104,\n",
       "  7,\n",
       "  307,\n",
       "  143,\n",
       "  45,\n",
       "  1172,\n",
       "  5008,\n",
       "  27,\n",
       "  13,\n",
       "  4511,\n",
       "  61,\n",
       "  169,\n",
       "  567,\n",
       "  25,\n",
       "  24290,\n",
       "  7,\n",
       "  24,\n",
       "  8,\n",
       "  201,\n",
       "  38,\n",
       "  67,\n",
       "  45,\n",
       "  214,\n",
       "  111,\n",
       "  45,\n",
       "  92,\n",
       "  372,\n",
       "  5,\n",
       "  697,\n",
       "  15,\n",
       "  26618,\n",
       "  167,\n",
       "  827,\n",
       "  4,\n",
       "  28,\n",
       "  7,\n",
       "  1336,\n",
       "  1917,\n",
       "  94,\n",
       "  74,\n",
       "  121,\n",
       "  63,\n",
       "  2597,\n",
       "  179,\n",
       "  141,\n",
       "  44,\n",
       "  30,\n",
       "  79,\n",
       "  22669,\n",
       "  371,\n",
       "  1123,\n",
       "  1,\n",
       "  296,\n",
       "  7573,\n",
       "  1,\n",
       "  419,\n",
       "  992,\n",
       "  1,\n",
       "  4,\n",
       "  137,\n",
       "  44,\n",
       "  80,\n",
       "  9065,\n",
       "  24290,\n",
       "  94,\n",
       "  25,\n",
       "  2108,\n",
       "  179,\n",
       "  141,\n",
       "  70,\n",
       "  19,\n",
       "  63,\n",
       "  2597,\n",
       "  4,\n",
       "  25,\n",
       "  2108,\n",
       "  398,\n",
       "  70,\n",
       "  11516,\n",
       "  1,\n",
       "  3845,\n",
       "  1096,\n",
       "  1,\n",
       "  4,\n",
       "  15734,\n",
       "  21,\n",
       "  234,\n",
       "  4,\n",
       "  433,\n",
       "  52,\n",
       "  1321,\n",
       "  2,\n",
       "  22457,\n",
       "  24290,\n",
       "  456,\n",
       "  7,\n",
       "  51,\n",
       "  131,\n",
       "  2459,\n",
       "  5,\n",
       "  357,\n",
       "  9065,\n",
       "  24290,\n",
       "  106,\n",
       "  94,\n",
       "  25,\n",
       "  2108,\n",
       "  418,\n",
       "  4,\n",
       "  418,\n",
       "  8,\n",
       "  566,\n",
       "  6,\n",
       "  7,\n",
       "  574,\n",
       "  357,\n",
       "  11567,\n",
       "  207,\n",
       "  126,\n",
       "  11883,\n",
       "  52,\n",
       "  89,\n",
       "  7,\n",
       "  668,\n",
       "  447,\n",
       "  8,\n",
       "  2,\n",
       "  677195,\n",
       "  11,\n",
       "  129,\n",
       "  140,\n",
       "  38,\n",
       "  67,\n",
       "  45,\n",
       "  214,\n",
       "  5,\n",
       "  145,\n",
       "  111,\n",
       "  45,\n",
       "  125,\n",
       "  28,\n",
       "  34712,\n",
       "  24,\n",
       "  63,\n",
       "  234,\n",
       "  4,\n",
       "  433,\n",
       "  2597,\n",
       "  25,\n",
       "  10807,\n",
       "  6193,\n",
       "  271,\n",
       "  1252984,\n",
       "  136,\n",
       "  28,\n",
       "  238,\n",
       "  387,\n",
       "  55,\n",
       "  18,\n",
       "  567,\n",
       "  76,\n",
       "  28,\n",
       "  993654,\n",
       "  40,\n",
       "  126,\n",
       "  8,\n",
       "  201,\n",
       "  4,\n",
       "  257,\n",
       "  87,\n",
       "  7,\n",
       "  4944,\n",
       "  70,\n",
       "  34,\n",
       "  96,\n",
       "  49,\n",
       "  2597,\n",
       "  30,\n",
       "  903,\n",
       "  969,\n",
       "  7,\n",
       "  208,\n",
       "  143,\n",
       "  17,\n",
       "  44,\n",
       "  185,\n",
       "  2114,\n",
       "  21,\n",
       "  433,\n",
       "  32,\n",
       "  0,\n",
       "  13,\n",
       "  153,\n",
       "  455,\n",
       "  17,\n",
       "  41,\n",
       "  1172,\n",
       "  823,\n",
       "  23,\n",
       "  487,\n",
       "  2597,\n",
       "  25,\n",
       "  33757,\n",
       "  640,\n",
       "  186,\n",
       "  8,\n",
       "  2,\n",
       "  447,\n",
       "  185,\n",
       "  2114,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2507,\n",
       "  2,\n",
       "  239,\n",
       "  51,\n",
       "  447,\n",
       "  6,\n",
       "  2,\n",
       "  677195,\n",
       "  10,\n",
       "  7573,\n",
       "  7573,\n",
       "  10,\n",
       "  34,\n",
       "  2,\n",
       "  98,\n",
       "  447,\n",
       "  15,\n",
       "  10,\n",
       "  10807,\n",
       "  63,\n",
       "  271,\n",
       "  1,\n",
       "  63,\n",
       "  992,\n",
       "  25,\n",
       "  170,\n",
       "  5105,\n",
       "  83,\n",
       "  23,\n",
       "  56,\n",
       "  0,\n",
       "  1,\n",
       "  185,\n",
       "  2114,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2507,\n",
       "  7573,\n",
       "  1,\n",
       "  30591,\n",
       "  2,\n",
       "  992,\n",
       "  1,\n",
       "  9065,\n",
       "  11883,\n",
       "  136,\n",
       "  28,\n",
       "  8,\n",
       "  43,\n",
       "  600,\n",
       "  400,\n",
       "  1524,\n",
       "  15,\n",
       "  2597,\n",
       "  25,\n",
       "  443,\n",
       "  5,\n",
       "  80,\n",
       "  17,\n",
       "  5,\n",
       "  853,\n",
       "  42,\n",
       "  487,\n",
       "  55,\n",
       "  106,\n",
       "  26618,\n",
       "  2664,\n",
       "  79,\n",
       "  94,\n",
       "  139,\n",
       "  15,\n",
       "  18,\n",
       "  1245,\n",
       "  991,\n",
       "  133,\n",
       "  2597,\n",
       "  44,\n",
       "  2638,\n",
       "  495,\n",
       "  1208,\n",
       "  8997,\n",
       "  1,\n",
       "  45,\n",
       "  44,\n",
       "  174,\n",
       "  94,\n",
       "  562,\n",
       "  55,\n",
       "  40204,\n",
       "  1221,\n",
       "  1,\n",
       "  4,\n",
       "  1025,\n",
       "  94,\n",
       "  5,\n",
       "  7083,\n",
       "  21,\n",
       "  433,\n",
       "  2597,\n",
       "  25,\n",
       "  349,\n",
       "  23,\n",
       "  1309,\n",
       "  18,\n",
       "  684,\n",
       "  15,\n",
       "  94,\n",
       "  25,\n",
       "  357,\n",
       "  39843,\n",
       "  207,\n",
       "  126,\n",
       "  1761,\n",
       "  6,\n",
       "  23,\n",
       "  684,\n",
       "  5,\n",
       "  3127,\n",
       "  15,\n",
       "  2597,\n",
       "  25,\n",
       "  102,\n",
       "  11,\n",
       "  991,\n",
       "  1,\n",
       "  38,\n",
       "  24825,\n",
       "  18,\n",
       "  10,\n",
       "  34,\n",
       "  724,\n",
       "  2597,\n",
       "  25,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  98,\n",
       "  10807,\n",
       "  94,\n",
       "  271,\n",
       "  45,\n",
       "  25,\n",
       "  83,\n",
       "  19925,\n",
       "  6193,\n",
       "  1109,\n",
       "  67,\n",
       "  7,\n",
       "  949,\n",
       "  396,\n",
       "  10,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  11,\n",
       "  98,\n",
       "  42,\n",
       "  1108,\n",
       "  1,\n",
       "  45,\n",
       "  25,\n",
       "  10807,\n",
       "  63,\n",
       "  992,\n",
       "  16799,\n",
       "  2597,\n",
       "  164,\n",
       "  15,\n",
       "  540,\n",
       "  518,\n",
       "  2734,\n",
       "  1134240,\n",
       "  6,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1252984,\n",
       "  52,\n",
       "  89,\n",
       "  7,\n",
       "  2723,\n",
       "  19,\n",
       "  586,\n",
       "  1030,\n",
       "  574,\n",
       "  1094,\n",
       "  25,\n",
       "  102,\n",
       "  11,\n",
       "  94,\n",
       "  32,\n",
       "  443,\n",
       "  11,\n",
       "  2,\n",
       "  94,\n",
       "  18,\n",
       "  10,\n",
       "  2830,\n",
       "  15,\n",
       "  49,\n",
       "  49,\n",
       "  94,\n",
       "  25,\n",
       "  332,\n",
       "  702,\n",
       "  21,\n",
       "  1252984,\n",
       "  992,\n",
       "  2756,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2108,\n",
       "  115,\n",
       "  6,\n",
       "  33,\n",
       "  70,\n",
       "  19,\n",
       "  33,\n",
       "  574,\n",
       "  1,\n",
       "  17,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  1413,\n",
       "  70,\n",
       "  21,\n",
       "  33,\n",
       "  234,\n",
       "  4,\n",
       "  433,\n",
       "  5,\n",
       "  2299,\n",
       "  23,\n",
       "  271150,\n",
       "  3020,\n",
       "  2597,\n",
       "  25,\n",
       "  7,\n",
       "  127,\n",
       "  558,\n",
       "  6,\n",
       "  516,\n",
       "  1,\n",
       "  38,\n",
       "  129,\n",
       "  94,\n",
       "  1413,\n",
       "  5,\n",
       "  141,\n",
       "  70,\n",
       "  19,\n",
       "  93,\n",
       "  100,\n",
       "  6,\n",
       "  40,\n",
       "  1,\n",
       "  94,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  676,\n",
       "  1096,\n",
       "  27,\n",
       "  141,\n",
       "  27,\n",
       "  45,\n",
       "  676,\n",
       "  63,\n",
       "  2597,\n",
       "  187,\n",
       "  63,\n",
       "  25,\n",
       "  912,\n",
       "  143,\n",
       "  17,\n",
       "  44,\n",
       "  2114,\n",
       "  21,\n",
       "  433,\n",
       "  4,\n",
       "  234,\n",
       "  1,\n",
       "  38,\n",
       "  76,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  17,\n",
       "  156,\n",
       "  5,\n",
       "  108,\n",
       "  93,\n",
       "  73162,\n",
       "  1,\n",
       "  725,\n",
       "  50,\n",
       "  17,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  108,\n",
       "  93,\n",
       "  289818,\n",
       "  17,\n",
       "  83,\n",
       "  271150,\n",
       "  108,\n",
       "  17,\n",
       "  433,\n",
       "  8,\n",
       "  201,\n",
       "  32,\n",
       "  26,\n",
       "  132,\n",
       "  2756,\n",
       "  38,\n",
       "  17,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  153,\n",
       "  80,\n",
       "  5,\n",
       "  2106,\n",
       "  4,\n",
       "  2263,\n",
       "  60,\n",
       "  21,\n",
       "  188,\n",
       "  84,\n",
       "  234,\n",
       "  4,\n",
       "  433,\n",
       "  79,\n",
       "  6,\n",
       "  2,\n",
       "  2190,\n",
       "  221,\n",
       "  8,\n",
       "  33,\n",
       "  167,\n",
       "  1,\n",
       "  1362,\n",
       "  80,\n",
       "  5,\n",
       "  3342,\n",
       "  21,\n",
       "  17,\n",
       "  1,\n",
       "  2,\n",
       "  49,\n",
       "  19,\n",
       "  63,\n",
       "  2597,\n",
       "  1,\n",
       "  2,\n",
       "  398,\n",
       "  1767,\n",
       "  45,\n",
       "  80,\n",
       "  4,\n",
       "  398,\n",
       "  1279,\n",
       "  417,\n",
       "  234,\n",
       "  10,\n",
       "  2,\n",
       "  115,\n",
       "  411,\n",
       "  298,\n",
       "  8,\n",
       "  33,\n",
       "  167,\n",
       "  1,\n",
       "  4,\n",
       "  27,\n",
       "  17,\n",
       "  80,\n",
       "  1592,\n",
       "  1,\n",
       "  2,\n",
       "  70,\n",
       "  17,\n",
       "  30,\n",
       "  21,\n",
       "  33,\n",
       "  234,\n",
       "  80,\n",
       "  398,\n",
       "  4,\n",
       "  398,\n",
       "  2597,\n",
       "  25,\n",
       "  102,\n",
       "  11,\n",
       "  225,\n",
       "  2348,\n",
       "  4360,\n",
       "  4,\n",
       "  8109,\n",
       "  4886,\n",
       "  1,\n",
       "  38,\n",
       "  45,\n",
       "  25,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  102,\n",
       "  11,\n",
       "  17,\n",
       "  5,\n",
       "  1413,\n",
       "  40,\n",
       "  33,\n",
       "  70,\n",
       "  19,\n",
       "  1,\n",
       "  5939,\n",
       "  18,\n",
       "  1,\n",
       "  4,\n",
       "  1957,\n",
       "  61,\n",
       "  19,\n",
       "  2,\n",
       "  4490,\n",
       "  33,\n",
       "  167,\n",
       "  41,\n",
       "  4831,\n",
       "  155,\n",
       "  8,\n",
       "  1096,\n",
       "  59,\n",
       "  10,\n",
       "  257,\n",
       "  79,\n",
       "  298,\n",
       "  56,\n",
       "  79,\n",
       "  271150,\n",
       "  213,\n",
       "  17,\n",
       "  44,\n",
       "  676,\n",
       "  1096,\n",
       "  4,\n",
       "  2683,\n",
       "  56,\n",
       "  1221,\n",
       "  187,\n",
       "  1,\n",
       "  4,\n",
       "  496,\n",
       "  17,\n",
       "  76,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  17,\n",
       "  156,\n",
       "  5,\n",
       "  47,\n",
       "  18,\n",
       "  100,\n",
       "  495,\n",
       "  39,\n",
       "  17,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  80,\n",
       "  2,\n",
       "  253,\n",
       "  350,\n",
       "  24,\n",
       "  69,\n",
       "  272,\n",
       "  26,\n",
       "  719,\n",
       "  17,\n",
       "  30,\n",
       "  5,\n",
       "  28,\n",
       "  63,\n",
       "  4,\n",
       "  2683,\n",
       "  18,\n",
       "  24,\n",
       "  866,\n",
       "  45,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  80,\n",
       "  5,\n",
       "  350,\n",
       "  2,\n",
       "  885,\n",
       "  152,\n",
       "  4,\n",
       "  2,\n",
       "  7565,\n",
       "  6,\n",
       "  1096,\n",
       "  69,\n",
       "  184,\n",
       "  61,\n",
       "  444,\n",
       "  73,\n",
       "  32,\n",
       "  137,\n",
       "  184,\n",
       "  19,\n",
       "  7,\n",
       "  427,\n",
       "  44,\n",
       "  276,\n",
       "  17,\n",
       "  70,\n",
       "  21,\n",
       "  33,\n",
       "  1379,\n",
       "  1075,\n",
       "  17,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  30,\n",
       "  5,\n",
       "  145,\n",
       "  885,\n",
       "  207,\n",
       "  126,\n",
       "  1,\n",
       "  38,\n",
       "  361,\n",
       "  8,\n",
       "  7,\n",
       "  176,\n",
       "  18,\n",
       "  2481,\n",
       "  102,\n",
       "  5,\n",
       "  80,\n",
       "  61,\n",
       "  4,\n",
       "  2683,\n",
       "  2,\n",
       "  152,\n",
       "  78374,\n",
       "  67,\n",
       "  17,\n",
       "  87,\n",
       "  7,\n",
       "  189,\n",
       "  1190,\n",
       "  4,\n",
       "  57,\n",
       "  5,\n",
       "  28,\n",
       "  1389,\n",
       "  1,\n",
       "  8,\n",
       "  1096,\n",
       "  1,\n",
       "  17,\n",
       "  44,\n",
       "  80,\n",
       "  323,\n",
       "  4,\n",
       "  1609,\n",
       "  33,\n",
       "  2322,\n",
       "  2597,\n",
       "  25,\n",
       "  4,\n",
       "  94,\n",
       "  1413,\n",
       "  5,\n",
       "  141,\n",
       "  70,\n",
       "  19,\n",
       "  93,\n",
       "  796,\n",
       "  6,\n",
       "  332,\n",
       "  42,\n",
       "  1868,\n",
       "  350,\n",
       "  1,\n",
       "  11516,\n",
       "  1,\n",
       "  4,\n",
       "  652,\n",
       "  4,\n",
       "  12237,\n",
       "  21,\n",
       "  63,\n",
       "  234,\n",
       "  4,\n",
       "  433,\n",
       "  8,\n",
       "  4486,\n",
       "  1,\n",
       "  94,\n",
       "  1413,\n",
       "  5,\n",
       "  141,\n",
       "  70,\n",
       "  19,\n",
       "  63,\n",
       "  2597,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  7,\n",
       "  226,\n",
       "  350,\n",
       "  4,\n",
       "  17,\n",
       "  164,\n",
       "  129,\n",
       "  49,\n",
       "  3196,\n",
       "  10672,\n",
       "  1,\n",
       "  67,\n",
       "  17,\n",
       "  25,\n",
       "  19,\n",
       "  33,\n",
       "  574,\n",
       "  1,\n",
       "  17,\n",
       "  124,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  415,\n",
       "  479,\n",
       "  33,\n",
       "  383,\n",
       "  1,\n",
       "  17,\n",
       "  147,\n",
       "  5,\n",
       "  145,\n",
       "  1767,\n",
       "  67,\n",
       "  17,\n",
       "  65,\n",
       "  545,\n",
       "  3407,\n",
       "  8,\n",
       "  371,\n",
       "  201,\n",
       "  32,\n",
       "  132,\n",
       "  1,\n",
       "  17,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  80,\n",
       "  5,\n",
       "  145,\n",
       "  885,\n",
       "  4,\n",
       "  108,\n",
       "  2,\n",
       "  664,\n",
       "  1329,\n",
       "  6,\n",
       "  1096,\n",
       "  1,\n",
       "  54,\n",
       "  67,\n",
       "  17,\n",
       "  80,\n",
       "  61,\n",
       "  161,\n",
       "  249698,\n",
       "  546,\n",
       "  32,\n",
       "  54,\n",
       "  5,\n",
       "  145,\n",
       "  4,\n",
       "  676,\n",
       "  2,\n",
       "  7975,\n",
       "  1279,\n",
       "  417,\n",
       "  10,\n",
       "  102,\n",
       "  11,\n",
       "  17,\n",
       "  1,\n",
       "  17,\n",
       "  94,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  80,\n",
       "  5,\n",
       "  108,\n",
       "  2,\n",
       "  61,\n",
       "  371,\n",
       "  152,\n",
       "  133,\n",
       "  45,\n",
       "  25,\n",
       "  5,\n",
       "  2208,\n",
       "  19,\n",
       "  63,\n",
       "  2597,\n",
       "  1,\n",
       "  4,\n",
       "  121,\n",
       "  93,\n",
       "  5,\n",
       "  149,\n",
       "  221,\n",
       "  32,\n",
       "  295,\n",
       "  370,\n",
       "  15,\n",
       "  45,\n",
       "  44,\n",
       "  47,\n",
       "  69,\n",
       "  885,\n",
       "  8,\n",
       "  2,\n",
       "  1279,\n",
       "  417,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [21,\n",
       "  2597,\n",
       "  11252,\n",
       "  75,\n",
       "  49,\n",
       "  147,\n",
       "  11,\n",
       "  199,\n",
       "  16484,\n",
       "  418,\n",
       "  6,\n",
       "  19832,\n",
       "  32,\n",
       "  230552,\n",
       "  155,\n",
       "  603,\n",
       "  26,\n",
       "  2,\n",
       "  1071,\n",
       "  38,\n",
       "  33,\n",
       "  12616,\n",
       "  52,\n",
       "  2793,\n",
       "  37,\n",
       "  2,\n",
       "  574,\n",
       "  8522,\n",
       "  6193,\n",
       "  495,\n",
       "  823,\n",
       "  1,\n",
       "  965,\n",
       "  1,\n",
       "  4,\n",
       "  375,\n",
       "  40,\n",
       "  26,\n",
       "  2,\n",
       "  390,\n",
       "  6,\n",
       "  7,\n",
       "  241,\n",
       "  4517,\n",
       "  57,\n",
       "  76,\n",
       "  1309,\n",
       "  5,\n",
       "  107,\n",
       "  21,\n",
       "  61,\n",
       "  23,\n",
       "  1111,\n",
       "  2871,\n",
       "  39,\n",
       "  2,\n",
       "  70,\n",
       "  52,\n",
       "  230,\n",
       "  11,\n",
       "  17,\n",
       "  5,\n",
       "  2162,\n",
       "  11,\n",
       "  866,\n",
       "  1,\n",
       "  1134240,\n",
       "  17,\n",
       "  66,\n",
       "  23,\n",
       "  70,\n",
       "  3013,\n",
       "  1,\n",
       "  1784,\n",
       "  2871,\n",
       "  15,\n",
       "  17,\n",
       "  44,\n",
       "  27006,\n",
       "  19,\n",
       "  1,\n",
       "  32,\n",
       "  76,\n",
       "  17,\n",
       "  709,\n",
       "  145,\n",
       "  130,\n",
       "  5,\n",
       "  2,\n",
       "  1808,\n",
       "  642,\n",
       "  39,\n",
       "  34,\n",
       "  98,\n",
       "  15,\n",
       "  38,\n",
       "  17,\n",
       "  136,\n",
       "  1464,\n",
       "  2,\n",
       "  127,\n",
       "  1210,\n",
       "  2,\n",
       "  574,\n",
       "  52,\n",
       "  11,\n",
       "  1446,\n",
       "  107,\n",
       "  7843,\n",
       "  1,\n",
       "  40204,\n",
       "  5384,\n",
       "  4,\n",
       "  6300,\n",
       "  50,\n",
       "  17,\n",
       "  25,\n",
       "  7,\n",
       "  501,\n",
       "  6623,\n",
       "  4,\n",
       "  146,\n",
       "  5,\n",
       "  15046,\n",
       "  21,\n",
       "  84,\n",
       "  94,\n",
       "  116,\n",
       "  17,\n",
       "  731,\n",
       "  264,\n",
       "  206,\n",
       "  7,\n",
       "  574,\n",
       "  37,\n",
       "  2597,\n",
       "  1,\n",
       "  1114,\n",
       "  2728,\n",
       "  1,\n",
       "  12509,\n",
       "  65,\n",
       "  233,\n",
       "  45,\n",
       "  25,\n",
       "  40,\n",
       "  2,\n",
       "  194,\n",
       "  1,\n",
       "  4,\n",
       "  15,\n",
       "  65,\n",
       "  233,\n",
       "  133,\n",
       "  45,\n",
       "  25,\n",
       "  40,\n",
       "  194,\n",
       "  478,\n",
       "  6,\n",
       "  7,\n",
       "  574,\n",
       "  59,\n",
       "  10,\n",
       "  75,\n",
       "  131,\n",
       "  17,\n",
       "  41,\n",
       "  80,\n",
       "  18,\n",
       "  387,\n",
       "  8,\n",
       "  70,\n",
       "  4,\n",
       "  30,\n",
       "  18,\n",
       "  28,\n",
       "  42084,\n",
       "  4,\n",
       "  2320,\n",
       "  10,\n",
       "  15,\n",
       "  17,\n",
       "  65,\n",
       "  5355,\n",
       "  28,\n",
       "  4985,\n",
       "  32,\n",
       "  21535,\n",
       "  50,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  34,\n",
       "  387,\n",
       "  2734,\n",
       "  2985,\n",
       "  2061,\n",
       "  1,\n",
       "  1134240,\n",
       "  17,\n",
       "  30,\n",
       "  7,\n",
       "  574,\n",
       "  26,\n",
       "  111,\n",
       "  39,\n",
       "  1134240,\n",
       "  17,\n",
       "  121,\n",
       "  1086,\n",
       "  1,\n",
       "  1252984,\n",
       "  1,\n",
       "  32,\n",
       "  96,\n",
       "  84,\n",
       "  430,\n",
       "  6,\n",
       "  501,\n",
       "  3264,\n",
       "  212,\n",
       "  39,\n",
       "  120,\n",
       "  50,\n",
       "  17,\n",
       "  25,\n",
       "  66,\n",
       "  2,\n",
       "  2180,\n",
       "  6,\n",
       "  2,\n",
       "  15594,\n",
       "  1,\n",
       "  5786,\n",
       "  0,\n",
       "  8,\n",
       "  23,\n",
       "  391,\n",
       "  17,\n",
       "  570,\n",
       "  3784,\n",
       "  496,\n",
       "  5,\n",
       "  215,\n",
       "  6,\n",
       "  106,\n",
       "  630,\n",
       "  4,\n",
       "  17,\n",
       "  115,\n",
       "  990,\n",
       "  83,\n",
       "  515,\n",
       "  15,\n",
       "  2,\n",
       "  574,\n",
       "  10,\n",
       "  7,\n",
       "  127,\n",
       "  1267,\n",
       "  5,\n",
       "  2638,\n",
       "  495,\n",
       "  1208,\n",
       "  8997,\n",
       "  1,\n",
       "  27,\n",
       "  120,\n",
       "  27,\n",
       "  42,\n",
       "  1124,\n",
       "  131,\n",
       "  5,\n",
       "  652,\n",
       "  21,\n",
       "  84,\n",
       "  94,\n",
       "  187,\n",
       "  254,\n",
       "  2,\n",
       "  495,\n",
       "  1208,\n",
       "  0,\n",
       "  15,\n",
       "  2597,\n",
       "  4,\n",
       "  185,\n",
       "  370,\n",
       "  174,\n",
       "  17,\n",
       "  1627,\n",
       "  1,\n",
       "  17,\n",
       "  76,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  137,\n",
       "  28,\n",
       "  372,\n",
       "  5,\n",
       "  145,\n",
       "  885,\n",
       "  4,\n",
       "  676,\n",
       "  7,\n",
       "  5874,\n",
       "  195,\n",
       "  6,\n",
       "  3254,\n",
       "  1,\n",
       "  2215,\n",
       "  1,\n",
       "  32,\n",
       "  137,\n",
       "  2069,\n",
       "  40,\n",
       "  133,\n",
       "  254,\n",
       "  495,\n",
       "  1208,\n",
       "  8997,\n",
       "  17,\n",
       "  25,\n",
       "  592683,\n",
       "  4,\n",
       "  33,\n",
       "  3258,\n",
       "  6785,\n",
       "  10,\n",
       "  172,\n",
       "  44,\n",
       "  17,\n",
       "  2031,\n",
       "  332,\n",
       "  5,\n",
       "  651,\n",
       "  5,\n",
       "  94,\n",
       "  65,\n",
       "  233,\n",
       "  2055,\n",
       "  69,\n",
       "  5,\n",
       "  652,\n",
       "  5,\n",
       "  93,\n",
       "  45,\n",
       "  25,\n",
       "  7,\n",
       "  127,\n",
       "  235464,\n",
       "  8,\n",
       "  0,\n",
       "  50,\n",
       "  18,\n",
       "  92,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  11,\n",
       "  495,\n",
       "  1208,\n",
       "  544883,\n",
       "  17,\n",
       "  76,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  137,\n",
       "  28,\n",
       "  372,\n",
       "  5,\n",
       "  1215,\n",
       "  221,\n",
       "  60,\n",
       "  133,\n",
       "  33,\n",
       "  1143,\n",
       "  76,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  145,\n",
       "  143,\n",
       "  17,\n",
       "  992,\n",
       "  92,\n",
       "  272,\n",
       "  106,\n",
       "  2664,\n",
       "  80,\n",
       "  64,\n",
       "  2357,\n",
       "  4,\n",
       "  35627,\n",
       "  5,\n",
       "  94,\n",
       "  74,\n",
       "  44,\n",
       "  28,\n",
       "  40,\n",
       "  2,\n",
       "  131,\n",
       "  693,\n",
       "  2,\n",
       "  152,\n",
       "  8,\n",
       "  69,\n",
       "  7,\n",
       "  241,\n",
       "  1634,\n",
       "  2031,\n",
       "  332,\n",
       "  7,\n",
       "  756,\n",
       "  11,\n",
       "  537,\n",
       "  32,\n",
       "  42,\n",
       "  6158,\n",
       "  26,\n",
       "  132,\n",
       "  689,\n",
       "  64646,\n",
       "  1,\n",
       "  4,\n",
       "  17,\n",
       "  30,\n",
       "  10575,\n",
       "  19832,\n",
       "  57,\n",
       "  76,\n",
       "  36,\n",
       "  28,\n",
       "  372,\n",
       "  5,\n",
       "  1134240,\n",
       "  254,\n",
       "  495,\n",
       "  1208,\n",
       "  0,\n",
       "  39,\n",
       "  538,\n",
       "  15,\n",
       "  10,\n",
       "  57,\n",
       "  36,\n",
       "  125,\n",
       "  1134240,\n",
       "  106,\n",
       "  2597,\n",
       "  1928,\n",
       "  5,\n",
       "  244421,\n",
       "  4,\n",
       "  1710,\n",
       "  64,\n",
       "  127,\n",
       "  33860,\n",
       "  6,\n",
       "  7,\n",
       "  391,\n",
       "  137,\n",
       "  343,\n",
       "  2,\n",
       "  574,\n",
       "  10,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  2,\n",
       "  98,\n",
       "  131,\n",
       "  5,\n",
       "  562,\n",
       "  495,\n",
       "  1208,\n",
       "  8997,\n",
       "  1,\n",
       "  18,\n",
       "  10,\n",
       "  180,\n",
       "  42,\n",
       "  411,\n",
       "  432262,\n",
       "  15,\n",
       "  843,\n",
       "  75,\n",
       "  1628,\n",
       "  5,\n",
       "  2182,\n",
       "  67,\n",
       "  17,\n",
       "  139,\n",
       "  55,\n",
       "  18,\n",
       "  1,\n",
       "  64,\n",
       "  391,\n",
       "  76,\n",
       "  28,\n",
       "  75,\n",
       "  143,\n",
       "  254,\n",
       "  106,\n",
       "  5159,\n",
       "  1839,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2662,\n",
       "  213,\n",
       "  17,\n",
       "  295,\n",
       "  2215,\n",
       "  4,\n",
       "  17,\n",
       "  4279,\n",
       "  57,\n",
       "  70,\n",
       "  33,\n",
       "  195,\n",
       "  10,\n",
       "  4,\n",
       "  282,\n",
       "  25,\n",
       "  2464,\n",
       "  172,\n",
       "  6,\n",
       "  2597,\n",
       "  83,\n",
       "  9213,\n",
       "  13,\n",
       "  769,\n",
       "  11,\n",
       "  579,\n",
       "  43,\n",
       "  1731,\n",
       "  19,\n",
       "  58,\n",
       "  574,\n",
       "  0,\n",
       "  1,\n",
       "  7079,\n",
       "  4,\n",
       "  209,\n",
       "  717,\n",
       "  200,\n",
       "  2597,\n",
       "  25,\n",
       "  102,\n",
       "  5,\n",
       "  30,\n",
       "  10,\n",
       "  11,\n",
       "  621,\n",
       "  2597,\n",
       "  25,\n",
       "  127,\n",
       "  11,\n",
       "  201,\n",
       "  2662,\n",
       "  33,\n",
       "  2636,\n",
       "  17,\n",
       "  5,\n",
       "  2694,\n",
       "  114,\n",
       "  7,\n",
       "  1395,\n",
       "  44,\n",
       "  47,\n",
       "  18,\n",
       "  2,\n",
       "  574,\n",
       "  365,\n",
       "  7,\n",
       "  1746,\n",
       "  13,\n",
       "  117,\n",
       "  43,\n",
       "  14773,\n",
       "  356,\n",
       "  1949,\n",
       "  6,\n",
       "  48,\n",
       "  2777,\n",
       "  18,\n",
       "  40,\n",
       "  5,\n",
       "  58,\n",
       "  7696,\n",
       "  19,\n",
       "  58,\n",
       "  2831,\n",
       "  19,\n",
       "  58,\n",
       "  1936,\n",
       "  207,\n",
       "  273,\n",
       "  8,\n",
       "  501,\n",
       "  1293,\n",
       "  36,\n",
       "  30,\n",
       "  5,\n",
       "  192,\n",
       "  60,\n",
       "  2,\n",
       "  9618,\n",
       "  4,\n",
       "  13,\n",
       "  121,\n",
       "  43,\n",
       "  1936,\n",
       "  11,\n",
       "  15,\n",
       "  105,\n",
       "  83,\n",
       "  102,\n",
       "  11,\n",
       "  2,\n",
       "  567,\n",
       "  179,\n",
       "  124,\n",
       "  17,\n",
       "  117,\n",
       "  10,\n",
       "  7,\n",
       "  318,\n",
       "  92,\n",
       "  17,\n",
       "  44,\n",
       "  192,\n",
       "  26,\n",
       "  33,\n",
       "  6172,\n",
       "  4,\n",
       "  1957,\n",
       "  8837,\n",
       "  39,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  310,\n",
       "  279,\n",
       "  201,\n",
       "  754,\n",
       "  13,\n",
       "  139,\n",
       "  45,\n",
       "  25,\n",
       "  113,\n",
       "  102,\n",
       "  5,\n",
       "  30,\n",
       "  133,\n",
       "  17,\n",
       "  44,\n",
       "  121,\n",
       "  93,\n",
       "  11,\n",
       "  18204,\n",
       "  1,\n",
       "  621,\n",
       "  1,\n",
       "  4,\n",
       "  53346,\n",
       "  50,\n",
       "  48,\n",
       "  124,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  30,\n",
       "  15,\n",
       "  1,\n",
       "  48,\n",
       "  76,\n",
       "  28,\n",
       "  779,\n",
       "  18204,\n",
       "  10,\n",
       "  127,\n",
       "  1,\n",
       "  105,\n",
       "  54,\n",
       "  1441,\n",
       "  207679,\n",
       "  17,\n",
       "  44,\n",
       "  437,\n",
       "  51,\n",
       "  6,\n",
       "  33,\n",
       "  234,\n",
       "  473,\n",
       "  74,\n",
       "  291,\n",
       "  3616,\n",
       "  703,\n",
       "  323,\n",
       "  29,\n",
       "  17,\n",
       "  47,\n",
       "  17,\n",
       "  25,\n",
       "  51,\n",
       "  39,\n",
       "  11237,\n",
       "  1,\n",
       "  201,\n",
       "  31,\n",
       "  2597,\n",
       "  769,\n",
       "  30,\n",
       "  93,\n",
       "  50,\n",
       "  18,\n",
       "  31,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  11,\n",
       "  2597,\n",
       "  36,\n",
       "  76,\n",
       "  153,\n",
       "  117,\n",
       "  27,\n",
       "  141,\n",
       "  27,\n",
       "  36,\n",
       "  47,\n",
       "  97,\n",
       "  2597,\n",
       "  25,\n",
       "  2201,\n",
       "  174,\n",
       "  17,\n",
       "  155,\n",
       "  167,\n",
       "  4136,\n",
       "  1,\n",
       "  929,\n",
       "  5,\n",
       "  2,\n",
       "  253,\n",
       "  152,\n",
       "  769,\n",
       "  52,\n",
       "  2597,\n",
       "  1,\n",
       "  521,\n",
       "  1416,\n",
       "  93,\n",
       "  7,\n",
       "  23,\n",
       "  10,\n",
       "  200,\n",
       "  13,\n",
       "  139,\n",
       "  332,\n",
       "  7,\n",
       "  574,\n",
       "  10,\n",
       "  7,\n",
       "  102,\n",
       "  221,\n",
       "  57,\n",
       "  47,\n",
       "  17,\n",
       "  139,\n",
       "  39,\n",
       "  27,\n",
       "  13,\n",
       "  31,\n",
       "  213,\n",
       "  2597,\n",
       "  25,\n",
       "  102,\n",
       "  11,\n",
       "  18204,\n",
       "  1,\n",
       "  137,\n",
       "  13,\n",
       "  23,\n",
       "  10,\n",
       "  200,\n",
       "  115,\n",
       "  621,\n",
       "  30,\n",
       "  2597,\n",
       "  13,\n",
       "  361,\n",
       "  124,\n",
       "  15,\n",
       "  43,\n",
       "  7711,\n",
       "  65,\n",
       "  233,\n",
       "  4254,\n",
       "  31,\n",
       "  8,\n",
       "  1164,\n",
       "  4,\n",
       "  13,\n",
       "  771,\n",
       "  91,\n",
       "  5,\n",
       "  692,\n",
       "  72,\n",
       "  719,\n",
       "  6,\n",
       "  2,\n",
       "  286,\n",
       "  4,\n",
       "  2,\n",
       "  5457,\n",
       "  4,\n",
       "  95,\n",
       "  124,\n",
       "  1,\n",
       "  45,\n",
       "  92,\n",
       "  1208,\n",
       "  6974,\n",
       "  17,\n",
       "  44,\n",
       "  69,\n",
       "  2596,\n",
       "  15,\n",
       "  279,\n",
       "  1521,\n",
       "  4,\n",
       "  1374,\n",
       "  5,\n",
       "  33,\n",
       "  1606,\n",
       "  2086,\n",
       "  57,\n",
       "  70,\n",
       "  17,\n",
       "  147,\n",
       "  5,\n",
       "  28,\n",
       "  26,\n",
       "  2,\n",
       "  79855,\n",
       "  18674,\n",
       "  1,\n",
       "  105,\n",
       "  16860,\n",
       "  17,\n",
       "  44,\n",
       "  652,\n",
       "  5,\n",
       "  33,\n",
       "  433,\n",
       "  187,\n",
       "  105,\n",
       "  7,\n",
       "  127,\n",
       "  131,\n",
       "  5,\n",
       "  216625,\n",
       "  4,\n",
       "  758,\n",
       "  56,\n",
       "  94,\n",
       "  49,\n",
       "  4,\n",
       "  49,\n",
       "  94,\n",
       "  121,\n",
       "  2597,\n",
       "  8,\n",
       "  502,\n",
       "  55,\n",
       "  1409227,\n",
       "  6,\n",
       "  1134240,\n",
       "  13,\n",
       "  139,\n",
       "  207,\n",
       "  51,\n",
       "  136,\n",
       "  30,\n",
       "  51,\n",
       "  55,\n",
       "  1409227,\n",
       "  6,\n",
       "  1134240,\n",
       "  18674,\n",
       "  8,\n",
       "  15643,\n",
       "  152,\n",
       "  497,\n",
       "  10,\n",
       "  2464,\n",
       "  24,\n",
       "  2597,\n",
       "  11,\n",
       "  579,\n",
       "  188,\n",
       "  13385,\n",
       "  54,\n",
       "  332,\n",
       "  18,\n",
       "  11,\n",
       "  7,\n",
       "  356,\n",
       "  10,\n",
       "  127,\n",
       "  54,\n",
       "  57,\n",
       "  47,\n",
       "  17,\n",
       "  139,\n",
       "  55,\n",
       "  93,\n",
       "  39,\n",
       "  25,\n",
       "  45,\n",
       "  865,\n",
       "  32,\n",
       "  57,\n",
       "  39,\n",
       "  13,\n",
       "  139,\n",
       "  25,\n",
       "  113,\n",
       "  1653,\n",
       "  133,\n",
       "  17,\n",
       "  44,\n",
       "  121,\n",
       "  93,\n",
       "  11,\n",
       "  18204,\n",
       "  1,\n",
       "  378,\n",
       "  1,\n",
       "  4,\n",
       "  201,\n",
       "  132,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2734,\n",
       "  2985,\n",
       "  1,\n",
       "  13,\n",
       "  117,\n",
       "  2,\n",
       "  2723,\n",
       "  19,\n",
       "  37192,\n",
       "  2597,\n",
       "  80063,\n",
       "  0,\n",
       "  10,\n",
       "  4082,\n",
       "  18085,\n",
       "  38,\n",
       "  5,\n",
       "  72,\n",
       "  2,\n",
       "  943,\n",
       "  10,\n",
       "  1709760,\n",
       "  2,\n",
       "  353,\n",
       "  10,\n",
       "  42,\n",
       "  123468,\n",
       "  1567,\n",
       "  2871,\n",
       "  1,\n",
       "  173,\n",
       "  8,\n",
       "  40,\n",
       "  1087,\n",
       "  4,\n",
       "  10,\n",
       "  98,\n",
       "  598626,\n",
       "  67,\n",
       "  35656,\n",
       "  13,\n",
       "  65,\n",
       "  754,\n",
       "  34,\n",
       "  976,\n",
       "  15,\n",
       "  105,\n",
       "  1409227,\n",
       "  532515,\n",
       "  11,\n",
       "  207,\n",
       "  259,\n",
       "  5,\n",
       "  2870,\n",
       "  51,\n",
       "  1,\n",
       "  38,\n",
       "  15,\n",
       "  105,\n",
       "  7,\n",
       "  0,\n",
       "  115,\n",
       "  94,\n",
       "  30,\n",
       "  2,\n",
       "  574,\n",
       "  4,\n",
       "  353,\n",
       "  2249,\n",
       "  10,\n",
       "  77992,\n",
       "  51,\n",
       "  6,\n",
       "  2,\n",
       "  115,\n",
       "  181218,\n",
       "  1004,\n",
       "  338,\n",
       "  423,\n",
       "  60,\n",
       "  18,\n",
       "  10,\n",
       "  2580,\n",
       "  96,\n",
       "  154,\n",
       "  8,\n",
       "  7,\n",
       "  390,\n",
       "  1096816,\n",
       "  163,\n",
       "  27,\n",
       "  4981,\n",
       "  4,\n",
       "  1086,\n",
       "  1025,\n",
       "  17,\n",
       "  5,\n",
       "  2377,\n",
       "  4,\n",
       "  25538,\n",
       "  21,\n",
       "  433,\n",
       "  255,\n",
       "  4,\n",
       "  56,\n",
       "  83,\n",
       "  2,\n",
       "  1238,\n",
       "  5,\n",
       "  573,\n",
       "  33,\n",
       "  206,\n",
       "  289,\n",
       "  227,\n",
       "  10,\n",
       "  4082,\n",
       "  1567,\n",
       "  67,\n",
       "  609,\n",
       "  5,\n",
       "  2687,\n",
       "  7,\n",
       "  0,\n",
       "  32,\n",
       "  240,\n",
       "  34,\n",
       "  98,\n",
       "  15,\n",
       "  38,\n",
       "  2,\n",
       "  353,\n",
       "  83,\n",
       "  409,\n",
       "  18,\n",
       "  518,\n",
       "  5,\n",
       "  234080,\n",
       "  2434,\n",
       "  187,\n",
       "  254,\n",
       "  332,\n",
       "  5,\n",
       "  145,\n",
       "  5,\n",
       "  2,\n",
       "  628,\n",
       "  13,\n",
       "  117,\n",
       "  2402,\n",
       "  1567,\n",
       "  11,\n",
       "  165,\n",
       "  1021,\n",
       "  1,\n",
       "  66,\n",
       "  129,\n",
       "  15,\n",
       "  1704,\n",
       "  1475,\n",
       "  5,\n",
       "  2,\n",
       "  4352,\n",
       "  311,\n",
       "  1,\n",
       "  13,\n",
       "  44,\n",
       "  0,\n",
       "  17,\n",
       "  15,\n",
       "  631,\n",
       "  207,\n",
       "  201,\n",
       "  8,\n",
       "  2,\n",
       "  391,\n",
       "  52,\n",
       "  7,\n",
       "  378,\n",
       "  574,\n",
       "  21,\n",
       "  78427,\n",
       "  353,\n",
       "  2,\n",
       "  574,\n",
       "  31,\n",
       "  69,\n",
       "  51,\n",
       "  981,\n",
       "  8551,\n",
       "  940,\n",
       "  11,\n",
       "  0,\n",
       "  106,\n",
       "  25,\n",
       "  129,\n",
       "  1096816,\n",
       "  21,\n",
       "  7,\n",
       "  0,\n",
       "  1571,\n",
       "  98,\n",
       "  106,\n",
       "  25,\n",
       "  1096816,\n",
       "  66,\n",
       "  23,\n",
       "  11,\n",
       "  301,\n",
       "  4,\n",
       "  2620,\n",
       "  6,\n",
       "  96,\n",
       "  642,\n",
       "  129,\n",
       "  2111,\n",
       "  97,\n",
       "  202484,\n",
       "  25,\n",
       "  0,\n",
       "  7079,\n",
       "  143,\n",
       "  2,\n",
       "  353,\n",
       "  10,\n",
       "  532515,\n",
       "  83,\n",
       "  2500,\n",
       "  163,\n",
       "  27,\n",
       "  2,\n",
       "  576,\n",
       "  5548,\n",
       "  667,\n",
       "  25,\n",
       "  714,\n",
       "  187,\n",
       "  13,\n",
       "  44,\n",
       "  484,\n",
       "  17,\n",
       "  160,\n",
       "  97,\n",
       "  15,\n",
       "  159345,\n",
       "  2111,\n",
       "  76,\n",
       "  28,\n",
       "  8820,\n",
       "  2635,\n",
       "  254,\n",
       "  2,\n",
       "  0,\n",
       "  11237,\n",
       "  1,\n",
       "  51,\n",
       "  6,\n",
       "  2,\n",
       "  115,\n",
       "  411,\n",
       "  712,\n",
       "  2,\n",
       "  41634,\n",
       "  6,\n",
       "  2,\n",
       "  353,\n",
       "  137,\n",
       "  343,\n",
       "  4981,\n",
       "  4,\n",
       "  1086,\n",
       "  25,\n",
       "  22326,\n",
       "  11,\n",
       "  102,\n",
       "  45,\n",
       "  83,\n",
       "  30,\n",
       "  7,\n",
       "  1062,\n",
       "  371,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [19,\n",
       "  232,\n",
       "  65,\n",
       "  233,\n",
       "  991,\n",
       "  129,\n",
       "  56,\n",
       "  475068,\n",
       "  30,\n",
       "  89,\n",
       "  1209,\n",
       "  163,\n",
       "  27,\n",
       "  2,\n",
       "  249698,\n",
       "  778,\n",
       "  1,\n",
       "  1114,\n",
       "  374,\n",
       "  1,\n",
       "  185,\n",
       "  195,\n",
       "  46297,\n",
       "  163,\n",
       "  27,\n",
       "  3178,\n",
       "  284697,\n",
       "  38,\n",
       "  570,\n",
       "  2,\n",
       "  115,\n",
       "  837,\n",
       "  4,\n",
       "  115,\n",
       "  173,\n",
       "  10,\n",
       "  2,\n",
       "  574,\n",
       "  2,\n",
       "  1238,\n",
       "  4,\n",
       "  256,\n",
       "  154,\n",
       "  8,\n",
       "  69,\n",
       "  7,\n",
       "  802,\n",
       "  6,\n",
       "  8363,\n",
       "  79,\n",
       "  94,\n",
       "  25,\n",
       "  21,\n",
       "  516,\n",
       "  15,\n",
       "  45,\n",
       "  631,\n",
       "  80,\n",
       "  10292,\n",
       "  5,\n",
       "  18,\n",
       "  4,\n",
       "  121,\n",
       "  18,\n",
       "  207,\n",
       "  1009,\n",
       "  45,\n",
       "  80,\n",
       "  10,\n",
       "  23,\n",
       "  102,\n",
       "  11,\n",
       "  94,\n",
       "  10,\n",
       "  57,\n",
       "  59,\n",
       "  415,\n",
       "  7694,\n",
       "  59,\n",
       "  271,\n",
       "  4,\n",
       "  1333,\n",
       "  1281,\n",
       "  496,\n",
       "  1,\n",
       "  94,\n",
       "  106,\n",
       "  246,\n",
       "  25,\n",
       "  2933,\n",
       "  24,\n",
       "  2,\n",
       "  353,\n",
       "  4082,\n",
       "  163,\n",
       "  2081,\n",
       "  8,\n",
       "  361,\n",
       "  2,\n",
       "  4588,\n",
       "  1367,\n",
       "  163,\n",
       "  27,\n",
       "  1086,\n",
       "  1,\n",
       "  4981,\n",
       "  1,\n",
       "  4,\n",
       "  1709,\n",
       "  106,\n",
       "  25,\n",
       "  570,\n",
       "  2,\n",
       "  10,\n",
       "  11,\n",
       "  201,\n",
       "  8,\n",
       "  2,\n",
       "  855496,\n",
       "  140,\n",
       "  43556,\n",
       "  1086,\n",
       "  2,\n",
       "  460,\n",
       "  1728,\n",
       "  7880,\n",
       "  4,\n",
       "  54,\n",
       "  30,\n",
       "  2,\n",
       "  190,\n",
       "  1378,\n",
       "  460,\n",
       "  37,\n",
       "  2222,\n",
       "  217715,\n",
       "  313,\n",
       "  23,\n",
       "  187,\n",
       "  2479,\n",
       "  264,\n",
       "  647,\n",
       "  94,\n",
       "  25,\n",
       "  357,\n",
       "  10292,\n",
       "  4,\n",
       "  7460,\n",
       "  967,\n",
       "  29,\n",
       "  2,\n",
       "  3331,\n",
       "  6,\n",
       "  1333,\n",
       "  1281,\n",
       "  27,\n",
       "  17,\n",
       "  44,\n",
       "  108,\n",
       "  2,\n",
       "  187,\n",
       "  353,\n",
       "  44,\n",
       "  28,\n",
       "  113,\n",
       "  121,\n",
       "  38,\n",
       "  34,\n",
       "  238,\n",
       "  15,\n",
       "  136,\n",
       "  28,\n",
       "  173,\n",
       "  3121,\n",
       "  647,\n",
       "  18,\n",
       "  44,\n",
       "  2933,\n",
       "  17,\n",
       "  233796,\n",
       "  4,\n",
       "  776,\n",
       "  84,\n",
       "  9572,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2734,\n",
       "  1134240,\n",
       "  2985,\n",
       "  1,\n",
       "  1252984,\n",
       "  343,\n",
       "  2597,\n",
       "  30,\n",
       "  129,\n",
       "  1245,\n",
       "  1,\n",
       "  45,\n",
       "  83,\n",
       "  30,\n",
       "  129,\n",
       "  14183,\n",
       "  179,\n",
       "  13,\n",
       "  515,\n",
       "  15,\n",
       "  2597,\n",
       "  4,\n",
       "  2,\n",
       "  167168,\n",
       "  106,\n",
       "  246,\n",
       "  30,\n",
       "  129,\n",
       "  49,\n",
       "  2346,\n",
       "  525,\n",
       "  15,\n",
       "  1502,\n",
       "  79,\n",
       "  6,\n",
       "  165,\n",
       "  14183,\n",
       "  25,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  443,\n",
       "  11,\n",
       "  33,\n",
       "  992,\n",
       "  1,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  1413,\n",
       "  27,\n",
       "  141,\n",
       "  70,\n",
       "  21,\n",
       "  234,\n",
       "  4,\n",
       "  822452,\n",
       "  4,\n",
       "  2,\n",
       "  3880,\n",
       "  6,\n",
       "  184,\n",
       "  885,\n",
       "  10,\n",
       "  398,\n",
       "  489,\n",
       "  173,\n",
       "  1303,\n",
       "  1,\n",
       "  13,\n",
       "  80,\n",
       "  18,\n",
       "  15,\n",
       "  2597,\n",
       "  25,\n",
       "  113,\n",
       "  1567,\n",
       "  38,\n",
       "  45,\n",
       "  83,\n",
       "  1852,\n",
       "  33,\n",
       "  26789,\n",
       "  50,\n",
       "  17,\n",
       "  30,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  0,\n",
       "  49,\n",
       "  4,\n",
       "  49,\n",
       "  567,\n",
       "  30,\n",
       "  89,\n",
       "  357,\n",
       "  4396,\n",
       "  4,\n",
       "  4286,\n",
       "  1,\n",
       "  689,\n",
       "  5,\n",
       "  2,\n",
       "  121,\n",
       "  6,\n",
       "  9364,\n",
       "  26,\n",
       "  7,\n",
       "  997,\n",
       "  1252984,\n",
       "  126,\n",
       "  13,\n",
       "  117,\n",
       "  49,\n",
       "  101,\n",
       "  249698,\n",
       "  567,\n",
       "  15,\n",
       "  30,\n",
       "  4396,\n",
       "  97,\n",
       "  1,\n",
       "  13,\n",
       "  83,\n",
       "  117,\n",
       "  15,\n",
       "  114,\n",
       "  284697,\n",
       "  1021,\n",
       "  13,\n",
       "  117,\n",
       "  97,\n",
       "  30,\n",
       "  4396,\n",
       "  11,\n",
       "  2,\n",
       "  194,\n",
       "  3227,\n",
       "  717,\n",
       "  225,\n",
       "  106,\n",
       "  2597,\n",
       "  10,\n",
       "  127,\n",
       "  11,\n",
       "  272,\n",
       "  60,\n",
       "  33,\n",
       "  1266714,\n",
       "  1,\n",
       "  38,\n",
       "  50,\n",
       "  17,\n",
       "  713,\n",
       "  19,\n",
       "  18,\n",
       "  11,\n",
       "  114,\n",
       "  42,\n",
       "  1108,\n",
       "  32,\n",
       "  855496,\n",
       "  7,\n",
       "  126,\n",
       "  17,\n",
       "  65,\n",
       "  5355,\n",
       "  305167,\n",
       "  508,\n",
       "  10292,\n",
       "  5,\n",
       "  18,\n",
       "  4,\n",
       "  2296,\n",
       "  17,\n",
       "  65,\n",
       "  795,\n",
       "  3609,\n",
       "  33,\n",
       "  26789,\n",
       "  2,\n",
       "  2748,\n",
       "  70,\n",
       "  7,\n",
       "  1917,\n",
       "  10406,\n",
       "  19,\n",
       "  63,\n",
       "  1936,\n",
       "  7,\n",
       "  320,\n",
       "  10,\n",
       "  55,\n",
       "  855496,\n",
       "  32,\n",
       "  0,\n",
       "  418,\n",
       "  4,\n",
       "  2,\n",
       "  70,\n",
       "  45,\n",
       "  1413,\n",
       "  21,\n",
       "  63,\n",
       "  234,\n",
       "  10,\n",
       "  81022,\n",
       "  42,\n",
       "  1108,\n",
       "  50,\n",
       "  2,\n",
       "  2753,\n",
       "  25,\n",
       "  2502,\n",
       "  2579,\n",
       "  1,\n",
       "  13,\n",
       "  331,\n",
       "  0,\n",
       "  10,\n",
       "  2,\n",
       "  717,\n",
       "  1021,\n",
       "  4,\n",
       "  567,\n",
       "  25,\n",
       "  398,\n",
       "  2392,\n",
       "  1252984,\n",
       "  2,\n",
       "  567,\n",
       "  145,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  10,\n",
       "  145,\n",
       "  19,\n",
       "  0,\n",
       "  32,\n",
       "  0,\n",
       "  4,\n",
       "  652,\n",
       "  5,\n",
       "  433,\n",
       "  15,\n",
       "  45,\n",
       "  108,\n",
       "  0,\n",
       "  1,\n",
       "  54,\n",
       "  13,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  149,\n",
       "  63,\n",
       "  691,\n",
       "  1,\n",
       "  1955,\n",
       "  2,\n",
       "  716,\n",
       "  38,\n",
       "  200,\n",
       "  34,\n",
       "  69,\n",
       "  652,\n",
       "  5,\n",
       "  93,\n",
       "  8,\n",
       "  201,\n",
       "  220,\n",
       "  17,\n",
       "  108,\n",
       "  93,\n",
       "  11,\n",
       "  0,\n",
       "  418,\n",
       "  2756,\n",
       "  2,\n",
       "  121,\n",
       "  6,\n",
       "  516,\n",
       "  10,\n",
       "  170,\n",
       "  10043,\n",
       "  37,\n",
       "  2,\n",
       "  2597,\n",
       "  92,\n",
       "  8645,\n",
       "  11,\n",
       "  368467,\n",
       "  34,\n",
       "  0,\n",
       "  447132,\n",
       "  32,\n",
       "  0,\n",
       "  873,\n",
       "  2,\n",
       "  152,\n",
       "  76,\n",
       "  28,\n",
       "  54,\n",
       "  141,\n",
       "  9841,\n",
       "  4,\n",
       "  8605,\n",
       "  1105,\n",
       "  50,\n",
       "  36,\n",
       "  3407,\n",
       "  5,\n",
       "  632,\n",
       "  619,\n",
       "  6,\n",
       "  167168,\n",
       "  97,\n",
       "  567,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  137,\n",
       "  161,\n",
       "  7,\n",
       "  0,\n",
       "  885,\n",
       "  63,\n",
       "  1872,\n",
       "  51649,\n",
       "  1,\n",
       "  105,\n",
       "  12839,\n",
       "  37,\n",
       "  115,\n",
       "  94,\n",
       "  137,\n",
       "  1266,\n",
       "  8,\n",
       "  63,\n",
       "  342,\n",
       "  54,\n",
       "  45,\n",
       "  44,\n",
       "  652,\n",
       "  5,\n",
       "  94,\n",
       "  54,\n",
       "  2402,\n",
       "  640,\n",
       "  2,\n",
       "  1021,\n",
       "  70,\n",
       "  323,\n",
       "  29,\n",
       "  63,\n",
       "  567,\n",
       "  97,\n",
       "  15,\n",
       "  17,\n",
       "  30,\n",
       "  196,\n",
       "  57,\n",
       "  13,\n",
       "  87,\n",
       "  5,\n",
       "  213,\n",
       "  55,\n",
       "  167168,\n",
       "  97,\n",
       "  7,\n",
       "  246,\n",
       "  1,\n",
       "  76,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  17,\n",
       "  1120,\n",
       "  15,\n",
       "  18,\n",
       "  69,\n",
       "  843,\n",
       "  70,\n",
       "  323,\n",
       "  29,\n",
       "  33,\n",
       "  1658,\n",
       "  167,\n",
       "  39,\n",
       "  13,\n",
       "  139,\n",
       "  18,\n",
       "  142,\n",
       "  4,\n",
       "  455,\n",
       "  15,\n",
       "  17,\n",
       "  1120,\n",
       "  21,\n",
       "  72,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2734,\n",
       "  312,\n",
       "  2985,\n",
       "  1,\n",
       "  2597,\n",
       "  25,\n",
       "  113,\n",
       "  411,\n",
       "  5,\n",
       "  79,\n",
       "  94,\n",
       "  521,\n",
       "  1416,\n",
       "  93,\n",
       "  32,\n",
       "  52,\n",
       "  173,\n",
       "  93,\n",
       "  169,\n",
       "  18,\n",
       "  10,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  443,\n",
       "  5,\n",
       "  206,\n",
       "  7,\n",
       "  574,\n",
       "  133,\n",
       "  17,\n",
       "  44,\n",
       "  180,\n",
       "  168784,\n",
       "  4,\n",
       "  28,\n",
       "  372,\n",
       "  5,\n",
       "  47,\n",
       "  2,\n",
       "  221,\n",
       "  17,\n",
       "  47,\n",
       "  2756,\n",
       "  2662,\n",
       "  213,\n",
       "  15,\n",
       "  17,\n",
       "  147,\n",
       "  5,\n",
       "  192,\n",
       "  238,\n",
       "  60,\n",
       "  1,\n",
       "  2,\n",
       "  298,\n",
       "  17,\n",
       "  257,\n",
       "  145,\n",
       "  5,\n",
       "  11,\n",
       "  154,\n",
       "  10,\n",
       "  7,\n",
       "  574,\n",
       "  94,\n",
       "  8,\n",
       "  1087,\n",
       "  1,\n",
       "  3478,\n",
       "  1,\n",
       "  4,\n",
       "  8,\n",
       "  933,\n",
       "  121,\n",
       "  18,\n",
       "  40,\n",
       "  2,\n",
       "  70,\n",
       "  8320,\n",
       "  41,\n",
       "  799,\n",
       "  145,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  5,\n",
       "  652,\n",
       "  5,\n",
       "  433,\n",
       "  32,\n",
       "  598,\n",
       "  295,\n",
       "  370,\n",
       "  1,\n",
       "  13,\n",
       "  117,\n",
       "  13,\n",
       "  47,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  54,\n",
       "  45,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  2151,\n",
       "  63,\n",
       "  70,\n",
       "  170,\n",
       "  6364,\n",
       "  40,\n",
       "  126,\n",
       "  822452,\n",
       "  4,\n",
       "  170,\n",
       "  1302,\n",
       "  10,\n",
       "  2056,\n",
       "  113,\n",
       "  102,\n",
       "  11,\n",
       "  8320,\n",
       "  5,\n",
       "  153,\n",
       "  192,\n",
       "  104,\n",
       "  1,\n",
       "  38,\n",
       "  137,\n",
       "  343,\n",
       "  45,\n",
       "  47,\n",
       "  15,\n",
       "  1,\n",
       "  45,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  2596,\n",
       "  323,\n",
       "  2,\n",
       "  574,\n",
       "  18,\n",
       "  41,\n",
       "  257,\n",
       "  28,\n",
       "  59,\n",
       "  67,\n",
       "  45,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  30,\n",
       "  2900,\n",
       "  7079,\n",
       "  32,\n",
       "  67,\n",
       "  45,\n",
       "  1442,\n",
       "  735,\n",
       "  193,\n",
       "  2,\n",
       "  2845,\n",
       "  21,\n",
       "  63,\n",
       "  433,\n",
       "  13,\n",
       "  244,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  2031,\n",
       "  57,\n",
       "  43,\n",
       "  167,\n",
       "  76,\n",
       "  28,\n",
       "  66,\n",
       "  666,\n",
       "  111,\n",
       "  29,\n",
       "  3106,\n",
       "  960,\n",
       "  4,\n",
       "  387,\n",
       "  21,\n",
       "  43,\n",
       "  7079,\n",
       "  4,\n",
       "  2,\n",
       "  574,\n",
       "  32,\n",
       "  1134240,\n",
       "  10,\n",
       "  51,\n",
       "  6,\n",
       "  2,\n",
       "  100,\n",
       "  1221,\n",
       "  13,\n",
       "  427,\n",
       "  5,\n",
       "  54,\n",
       "  13,\n",
       "  44,\n",
       "  69,\n",
       "  2106,\n",
       "  130,\n",
       "  4,\n",
       "  4345,\n",
       "  2597,\n",
       "  136,\n",
       "  28,\n",
       "  646,\n",
       "  27,\n",
       "  64,\n",
       "  433,\n",
       "  45,\n",
       "  871,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  2641,\n",
       "  107,\n",
       "  45,\n",
       "  69,\n",
       "  276,\n",
       "  107,\n",
       "  57,\n",
       "  36,\n",
       "  147,\n",
       "  4,\n",
       "  156,\n",
       "  311860,\n",
       "  10,\n",
       "  7,\n",
       "  102,\n",
       "  579,\n",
       "  133,\n",
       "  521,\n",
       "  1416,\n",
       "  2,\n",
       "  574,\n",
       "  5,\n",
       "  149,\n",
       "  1376,\n",
       "  18,\n",
       "  10,\n",
       "  15,\n",
       "  94,\n",
       "  25,\n",
       "  272,\n",
       "  11,\n",
       "  19,\n",
       "  18,\n",
       "  4,\n",
       "  50,\n",
       "  45,\n",
       "  65,\n",
       "  545,\n",
       "  387,\n",
       "  21,\n",
       "  1498334,\n",
       "  45,\n",
       "  44,\n",
       "  30,\n",
       "  7,\n",
       "  189,\n",
       "  452,\n",
       "  24,\n",
       "  370,\n",
       "  4,\n",
       "  433,\n",
       "  38,\n",
       "  13,\n",
       "  515,\n",
       "  15,\n",
       "  2,\n",
       "  607,\n",
       "  1571,\n",
       "  11,\n",
       "  2597,\n",
       "  10,\n",
       "  154,\n",
       "  3246,\n",
       "  13,\n",
       "  80,\n",
       "  111,\n",
       "  29,\n",
       "  201,\n",
       "  1,\n",
       "  13,\n",
       "  1442,\n",
       "  43,\n",
       "  7079,\n",
       "  4,\n",
       "  3728,\n",
       "  7,\n",
       "  189,\n",
       "  11,\n",
       "  23809,\n",
       "  116,\n",
       "  160988,\n",
       "  1134240,\n",
       "  32,\n",
       "  145,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  54,\n",
       "  13,\n",
       "  65,\n",
       "  754,\n",
       "  34,\n",
       "  6364,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  34,\n",
       "  15,\n",
       "  13,\n",
       "  65,\n",
       "  754,\n",
       "  34,\n",
       "  1302,\n",
       "  133,\n",
       "  13,\n",
       "  65,\n",
       "  754,\n",
       "  34,\n",
       "  378,\n",
       "  61,\n",
       "  32,\n",
       "  822452,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  133,\n",
       "  13,\n",
       "  30,\n",
       "  538,\n",
       "  5,\n",
       "  47,\n",
       "  1,\n",
       "  54,\n",
       "  200,\n",
       "  34,\n",
       "  110,\n",
       "  43,\n",
       "  70,\n",
       "  7665,\n",
       "  39,\n",
       "  521,\n",
       "  142,\n",
       "  18,\n",
       "  54,\n",
       "  13,\n",
       "  65,\n",
       "  754,\n",
       "  34,\n",
       "  2,\n",
       "  98,\n",
       "  51,\n",
       "  496,\n",
       "  1,\n",
       "  2597,\n",
       "  25,\n",
       "  69,\n",
       "  23,\n",
       "  1500,\n",
       "  653,\n",
       "  21,\n",
       "  7,\n",
       "  3628,\n",
       "  1,\n",
       "  38,\n",
       "  50,\n",
       "  17,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  2596,\n",
       "  18,\n",
       "  323,\n",
       "  29,\n",
       "  6193,\n",
       "  830,\n",
       "  1109,\n",
       "  101,\n",
       "  17,\n",
       "  44,\n",
       "  108,\n",
       "  15,\n",
       "  45,\n",
       "  25,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  57,\n",
       "  17,\n",
       "  213,\n",
       "  45,\n",
       "  25,\n",
       "  4,\n",
       "  45,\n",
       "  871,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  16310,\n",
       "  6193,\n",
       "  1822,\n",
       "  32,\n",
       "  479,\n",
       "  2,\n",
       "  717,\n",
       "  45,\n",
       "  25,\n",
       "  8,\n",
       "  42,\n",
       "  399,\n",
       "  32,\n",
       "  645,\n",
       "  342,\n",
       "  32,\n",
       "  1872,\n",
       "  10,\n",
       "  133,\n",
       "  94,\n",
       "  66,\n",
       "  93,\n",
       "  4,\n",
       "  1416,\n",
       "  93,\n",
       "  11,\n",
       "  63,\n",
       "  466,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [18,\n",
       "  76,\n",
       "  28,\n",
       "  17071,\n",
       "  160,\n",
       "  39,\n",
       "  85,\n",
       "  41,\n",
       "  17,\n",
       "  287,\n",
       "  60,\n",
       "  21,\n",
       "  33,\n",
       "  437,\n",
       "  32,\n",
       "  430,\n",
       "  6,\n",
       "  2,\n",
       "  1046,\n",
       "  11,\n",
       "  23,\n",
       "  113,\n",
       "  2985,\n",
       "  39,\n",
       "  137,\n",
       "  343,\n",
       "  79,\n",
       "  94,\n",
       "  271150,\n",
       "  515,\n",
       "  15,\n",
       "  2597,\n",
       "  25,\n",
       "  640,\n",
       "  323,\n",
       "  29,\n",
       "  1767,\n",
       "  1,\n",
       "  3845,\n",
       "  1096,\n",
       "  1,\n",
       "  4,\n",
       "  2108,\n",
       "  70,\n",
       "  21,\n",
       "  234,\n",
       "  4,\n",
       "  433,\n",
       "  1,\n",
       "  2597,\n",
       "  47,\n",
       "  30,\n",
       "  7,\n",
       "  1502,\n",
       "  1106,\n",
       "  19,\n",
       "  107,\n",
       "  129,\n",
       "  94,\n",
       "  30,\n",
       "  1903,\n",
       "  4,\n",
       "  79,\n",
       "  2703,\n",
       "  223,\n",
       "  15,\n",
       "  24,\n",
       "  26,\n",
       "  432,\n",
       "  186231,\n",
       "  1,\n",
       "  1252984,\n",
       "  41,\n",
       "  28,\n",
       "  19074,\n",
       "  54,\n",
       "  13,\n",
       "  156,\n",
       "  17,\n",
       "  5,\n",
       "  161,\n",
       "  7,\n",
       "  2184,\n",
       "  6,\n",
       "  85,\n",
       "  129,\n",
       "  94,\n",
       "  149,\n",
       "  2597,\n",
       "  1220,\n",
       "  4,\n",
       "  17,\n",
       "  65,\n",
       "  5355,\n",
       "  28,\n",
       "  2859,\n",
       "  2,\n",
       "  909,\n",
       "  10,\n",
       "  3135,\n",
       "  1,\n",
       "  73,\n",
       "  371,\n",
       "  25,\n",
       "  17,\n",
       "  19,\n",
       "  39,\n",
       "  601,\n",
       "  17,\n",
       "  45,\n",
       "  137,\n",
       "  30,\n",
       "  0,\n",
       "  946,\n",
       "  19,\n",
       "  574,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  141,\n",
       "  49,\n",
       "  918,\n",
       "  5,\n",
       "  121,\n",
       "  2,\n",
       "  353,\n",
       "  101,\n",
       "  569,\n",
       "  29,\n",
       "  7,\n",
       "  247,\n",
       "  5,\n",
       "  26749,\n",
       "  23,\n",
       "  271150,\n",
       "  3020,\n",
       "  1,\n",
       "  1134240,\n",
       "  33,\n",
       "  167,\n",
       "  254,\n",
       "  7,\n",
       "  1936,\n",
       "  32,\n",
       "  7,\n",
       "  2831,\n",
       "  574,\n",
       "  50,\n",
       "  7,\n",
       "  906,\n",
       "  11732,\n",
       "  29,\n",
       "  2,\n",
       "  353,\n",
       "  1,\n",
       "  45,\n",
       "  65,\n",
       "  545,\n",
       "  49,\n",
       "  990,\n",
       "  5,\n",
       "  696,\n",
       "  18,\n",
       "  133,\n",
       "  45,\n",
       "  871,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  28,\n",
       "  300,\n",
       "  5,\n",
       "  1587,\n",
       "  24,\n",
       "  2,\n",
       "  19134,\n",
       "  6,\n",
       "  7,\n",
       "  11074,\n",
       "  47,\n",
       "  17,\n",
       "  2109,\n",
       "  156,\n",
       "  5,\n",
       "  28,\n",
       "  2,\n",
       "  148,\n",
       "  51,\n",
       "  328,\n",
       "  827,\n",
       "  39,\n",
       "  13,\n",
       "  76,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  13,\n",
       "  762,\n",
       "  15,\n",
       "  17,\n",
       "  25,\n",
       "  7,\n",
       "  396,\n",
       "  74,\n",
       "  1043,\n",
       "  5,\n",
       "  287,\n",
       "  60,\n",
       "  21,\n",
       "  2,\n",
       "  529,\n",
       "  654,\n",
       "  54,\n",
       "  200,\n",
       "  34,\n",
       "  121,\n",
       "  33,\n",
       "  574,\n",
       "  49,\n",
       "  489,\n",
       "  39,\n",
       "  67,\n",
       "  45,\n",
       "  213,\n",
       "  1252984,\n",
       "  1,\n",
       "  45,\n",
       "  672,\n",
       "  1252984,\n",
       "  129,\n",
       "  94,\n",
       "  41,\n",
       "  213,\n",
       "  2,\n",
       "  194,\n",
       "  15,\n",
       "  13,\n",
       "  65,\n",
       "  6227,\n",
       "  118,\n",
       "  13,\n",
       "  117,\n",
       "  129,\n",
       "  1021,\n",
       "  4,\n",
       "  84,\n",
       "  2620,\n",
       "  271150,\n",
       "  139,\n",
       "  15,\n",
       "  876,\n",
       "  23913,\n",
       "  25,\n",
       "  7,\n",
       "  2151,\n",
       "  6,\n",
       "  592,\n",
       "  38,\n",
       "  397,\n",
       "  45,\n",
       "  65,\n",
       "  545,\n",
       "  2552,\n",
       "  101,\n",
       "  2527,\n",
       "  41,\n",
       "  338,\n",
       "  28,\n",
       "  50,\n",
       "  17,\n",
       "  87,\n",
       "  173,\n",
       "  2597,\n",
       "  1,\n",
       "  17,\n",
       "  76,\n",
       "  65,\n",
       "  6227,\n",
       "  3642,\n",
       "  7,\n",
       "  1103,\n",
       "  130,\n",
       "  365,\n",
       "  246,\n",
       "  52,\n",
       "  33,\n",
       "  1555,\n",
       "  32,\n",
       "  996,\n",
       "  338,\n",
       "  681,\n",
       "  17,\n",
       "  15,\n",
       "  569,\n",
       "  29,\n",
       "  7,\n",
       "  11074,\n",
       "  10,\n",
       "  4725,\n",
       "  39,\n",
       "  496,\n",
       "  39,\n",
       "  75,\n",
       "  39,\n",
       "  15,\n",
       "  65,\n",
       "  233,\n",
       "  143,\n",
       "  2597,\n",
       "  230,\n",
       "  8,\n",
       "  2,\n",
       "  299,\n",
       "  6,\n",
       "  1598,\n",
       "  200,\n",
       "  10,\n",
       "  5830,\n",
       "  38,\n",
       "  261,\n",
       "  15,\n",
       "  1187,\n",
       "  61,\n",
       "  25,\n",
       "  17,\n",
       "  44,\n",
       "  4789,\n",
       "  21,\n",
       "  94,\n",
       "  8,\n",
       "  84,\n",
       "  407,\n",
       "  32,\n",
       "  1292,\n",
       "  1,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  49,\n",
       "  918,\n",
       "  5,\n",
       "  562,\n",
       "  29,\n",
       "  7,\n",
       "  334,\n",
       "  101,\n",
       "  7,\n",
       "  11074,\n",
       "  1,\n",
       "  4,\n",
       "  2296,\n",
       "  1252984,\n",
       "  41,\n",
       "  28,\n",
       "  19074,\n",
       "  1556,\n",
       "  88,\n",
       "  13,\n",
       "  160,\n",
       "  39,\n",
       "  30,\n",
       "  33,\n",
       "  255,\n",
       "  2111,\n",
       "  338,\n",
       "  4610,\n",
       "  17,\n",
       "  7,\n",
       "  4161,\n",
       "  8291,\n",
       "  8,\n",
       "  219,\n",
       "  202,\n",
       "  32,\n",
       "  391,\n",
       "  39,\n",
       "  50,\n",
       "  496,\n",
       "  1,\n",
       "  124,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  17,\n",
       "  30,\n",
       "  5,\n",
       "  1,\n",
       "  853,\n",
       "  2527,\n",
       "  39,\n",
       "  116,\n",
       "  1,\n",
       "  17,\n",
       "  87,\n",
       "  5,\n",
       "  967,\n",
       "  26,\n",
       "  432,\n",
       "  249698,\n",
       "  738,\n",
       "  169,\n",
       "  33,\n",
       "  4161,\n",
       "  8291,\n",
       "  999,\n",
       "  130,\n",
       "  13,\n",
       "  762,\n",
       "  15,\n",
       "  79,\n",
       "  456,\n",
       "  44,\n",
       "  196,\n",
       "  29,\n",
       "  7,\n",
       "  247,\n",
       "  4,\n",
       "  6908,\n",
       "  40,\n",
       "  6,\n",
       "  2,\n",
       "  2142,\n",
       "  15,\n",
       "  45,\n",
       "  65,\n",
       "  6227,\n",
       "  1892,\n",
       "  29,\n",
       "  15,\n",
       "  51,\n",
       "  247,\n",
       "  38,\n",
       "  826,\n",
       "  456,\n",
       "  562,\n",
       "  6489,\n",
       "  101,\n",
       "  15,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2734,\n",
       "  2985,\n",
       "  1,\n",
       "  8,\n",
       "  139,\n",
       "  15,\n",
       "  2597,\n",
       "  25,\n",
       "  34,\n",
       "  102,\n",
       "  11,\n",
       "  2,\n",
       "  991,\n",
       "  59,\n",
       "  25,\n",
       "  261,\n",
       "  1598,\n",
       "  200,\n",
       "  13,\n",
       "  139,\n",
       "  23,\n",
       "  4,\n",
       "  106,\n",
       "  25,\n",
       "  2,\n",
       "  404,\n",
       "  1134240,\n",
       "  1,\n",
       "  94,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  80,\n",
       "  5,\n",
       "  1413,\n",
       "  70,\n",
       "  21,\n",
       "  63,\n",
       "  234,\n",
       "  311,\n",
       "  1,\n",
       "  17,\n",
       "  44,\n",
       "  80,\n",
       "  96260,\n",
       "  622,\n",
       "  225,\n",
       "  3035,\n",
       "  6450,\n",
       "  67,\n",
       "  33,\n",
       "  415,\n",
       "  33,\n",
       "  7079,\n",
       "  17,\n",
       "  44,\n",
       "  80,\n",
       "  0,\n",
       "  24,\n",
       "  2,\n",
       "  574,\n",
       "  2,\n",
       "  1134240,\n",
       "  717,\n",
       "  200,\n",
       "  2597,\n",
       "  25,\n",
       "  443,\n",
       "  11,\n",
       "  2,\n",
       "  991,\n",
       "  10,\n",
       "  133,\n",
       "  94,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  1413,\n",
       "  400,\n",
       "  70,\n",
       "  21,\n",
       "  63,\n",
       "  234,\n",
       "  1,\n",
       "  13,\n",
       "  117,\n",
       "  13,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  13,\n",
       "  1413,\n",
       "  26,\n",
       "  432,\n",
       "  729,\n",
       "  43,\n",
       "  126,\n",
       "  9364,\n",
       "  26,\n",
       "  7,\n",
       "  997,\n",
       "  67,\n",
       "  13,\n",
       "  125,\n",
       "  28,\n",
       "  885,\n",
       "  1770,\n",
       "  43,\n",
       "  829,\n",
       "  21,\n",
       "  433,\n",
       "  5539,\n",
       "  1,\n",
       "  3035,\n",
       "  6450,\n",
       "  10,\n",
       "  2,\n",
       "  115,\n",
       "  29817,\n",
       "  67,\n",
       "  13,\n",
       "  430,\n",
       "  238,\n",
       "  1,\n",
       "  13,\n",
       "  121,\n",
       "  0,\n",
       "  11,\n",
       "  96,\n",
       "  694,\n",
       "  2,\n",
       "  2260,\n",
       "  26,\n",
       "  677195,\n",
       "  313,\n",
       "  26,\n",
       "  432,\n",
       "  1409227,\n",
       "  456,\n",
       "  5411,\n",
       "  0,\n",
       "  622,\n",
       "  8,\n",
       "  2500,\n",
       "  11,\n",
       "  579,\n",
       "  1,\n",
       "  50,\n",
       "  17,\n",
       "  701,\n",
       "  5,\n",
       "  5411,\n",
       "  57,\n",
       "  796,\n",
       "  2,\n",
       "  1409227,\n",
       "  76,\n",
       "  5411,\n",
       "  1252984,\n",
       "  904,\n",
       "  1,\n",
       "  43,\n",
       "  148,\n",
       "  717,\n",
       "  200,\n",
       "  13,\n",
       "  139,\n",
       "  2597,\n",
       "  25,\n",
       "  34,\n",
       "  102,\n",
       "  11,\n",
       "  991,\n",
       "  10,\n",
       "  133,\n",
       "  17,\n",
       "  44,\n",
       "  80,\n",
       "  13368,\n",
       "  24,\n",
       "  18,\n",
       "  7,\n",
       "  270,\n",
       "  13,\n",
       "  117,\n",
       "  15,\n",
       "  133,\n",
       "  13,\n",
       "  88,\n",
       "  4018,\n",
       "  5,\n",
       "  2,\n",
       "  152,\n",
       "  6,\n",
       "  2597,\n",
       "  67,\n",
       "  338,\n",
       "  13,\n",
       "  47,\n",
       "  43,\n",
       "  7079,\n",
       "  13,\n",
       "  244,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  287,\n",
       "  43,\n",
       "  992,\n",
       "  172,\n",
       "  2,\n",
       "  997,\n",
       "  1405,\n",
       "  4160,\n",
       "  15,\n",
       "  2597,\n",
       "  25,\n",
       "  34,\n",
       "  102,\n",
       "  11,\n",
       "  991,\n",
       "  8,\n",
       "  4486,\n",
       "  1,\n",
       "  2597,\n",
       "  287,\n",
       "  17,\n",
       "  29,\n",
       "  2108,\n",
       "  70,\n",
       "  21,\n",
       "  33,\n",
       "  234,\n",
       "  1,\n",
       "  52,\n",
       "  456,\n",
       "  59859,\n",
       "  622,\n",
       "  1,\n",
       "  4,\n",
       "  20654,\n",
       "  17,\n",
       "  29,\n",
       "  415,\n",
       "  33,\n",
       "  7079,\n",
       "  13,\n",
       "  455,\n",
       "  17,\n",
       "  1120,\n",
       "  21,\n",
       "  72,\n",
       "  19,\n",
       "  23,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [59,\n",
       "  44,\n",
       "  28,\n",
       "  75,\n",
       "  49,\n",
       "  1,\n",
       "  43,\n",
       "  829,\n",
       "  5792,\n",
       "  43,\n",
       "  7079,\n",
       "  5171,\n",
       "  97,\n",
       "  57,\n",
       "  371,\n",
       "  25,\n",
       "  17,\n",
       "  26,\n",
       "  39,\n",
       "  13,\n",
       "  44,\n",
       "  30,\n",
       "  43,\n",
       "  235,\n",
       "  1,\n",
       "  7079,\n",
       "  1,\n",
       "  1189,\n",
       "  1,\n",
       "  142,\n",
       "  132,\n",
       "  1,\n",
       "  798,\n",
       "  1,\n",
       "  719,\n",
       "  1,\n",
       "  4,\n",
       "  141,\n",
       "  49,\n",
       "  2867,\n",
       "  4,\n",
       "  641966,\n",
       "  54,\n",
       "  13,\n",
       "  117,\n",
       "  143,\n",
       "  18,\n",
       "  10,\n",
       "  10659,\n",
       "  13,\n",
       "  145,\n",
       "  5,\n",
       "  149,\n",
       "  18,\n",
       "  17,\n",
       "  44,\n",
       "  137,\n",
       "  80,\n",
       "  1252984,\n",
       "  5,\n",
       "  295,\n",
       "  7,\n",
       "  247,\n",
       "  17,\n",
       "  25,\n",
       "  569,\n",
       "  1,\n",
       "  7696,\n",
       "  1492,\n",
       "  17,\n",
       "  562,\n",
       "  56,\n",
       "  622,\n",
       "  15,\n",
       "  17,\n",
       "  271150,\n",
       "  34,\n",
       "  30,\n",
       "  1219,\n",
       "  85,\n",
       "  5,\n",
       "  690,\n",
       "  61,\n",
       "  254,\n",
       "  7,\n",
       "  904676,\n",
       "  1,\n",
       "  75,\n",
       "  49,\n",
       "  1,\n",
       "  13,\n",
       "  65,\n",
       "  754,\n",
       "  6364,\n",
       "  1134240,\n",
       "  17,\n",
       "  121,\n",
       "  33,\n",
       "  574,\n",
       "  5,\n",
       "  1134240,\n",
       "  479,\n",
       "  66,\n",
       "  23,\n",
       "  39,\n",
       "  50,\n",
       "  54,\n",
       "  17,\n",
       "  264,\n",
       "  676,\n",
       "  18,\n",
       "  23,\n",
       "  153,\n",
       "  1492,\n",
       "  133,\n",
       "  7,\n",
       "  270,\n",
       "  6,\n",
       "  94,\n",
       "  30,\n",
       "  364,\n",
       "  280,\n",
       "  21,\n",
       "  7535,\n",
       "  2111,\n",
       "  44,\n",
       "  137,\n",
       "  204,\n",
       "  1007,\n",
       "  19,\n",
       "  201,\n",
       "  2081,\n",
       "  6,\n",
       "  7079,\n",
       "  1,\n",
       "  67,\n",
       "  1189,\n",
       "  25,\n",
       "  1134240,\n",
       "  1,\n",
       "  13918,\n",
       "  4981,\n",
       "  1,\n",
       "  1086,\n",
       "  1,\n",
       "  1318,\n",
       "  1,\n",
       "  4,\n",
       "  54,\n",
       "  141,\n",
       "  49,\n",
       "  25,\n",
       "  173,\n",
       "  24,\n",
       "  94,\n",
       "  5,\n",
       "  652,\n",
       "  1,\n",
       "  1924,\n",
       "  85,\n",
       "  45,\n",
       "  331,\n",
       "  5,\n",
       "  521,\n",
       "  5,\n",
       "  69,\n",
       "  281,\n",
       "  18,\n",
       "  61,\n",
       "  0,\n",
       "  223,\n",
       "  146,\n",
       "  5,\n",
       "  454,\n",
       "  1,\n",
       "  692,\n",
       "  1547,\n",
       "  4892,\n",
       "  1,\n",
       "  5,\n",
       "  1134240,\n",
       "  57,\n",
       "  338,\n",
       "  94,\n",
       "  145,\n",
       "  19,\n",
       "  207,\n",
       "  126,\n",
       "  11661,\n",
       "  798,\n",
       "  1,\n",
       "  719,\n",
       "  1,\n",
       "  2200,\n",
       "  1,\n",
       "  4,\n",
       "  69,\n",
       "  55,\n",
       "  57,\n",
       "  338,\n",
       "  45,\n",
       "  156,\n",
       "  17,\n",
       "  44,\n",
       "  83,\n",
       "  192,\n",
       "  60,\n",
       "  154,\n",
       "  19,\n",
       "  96,\n",
       "  1320,\n",
       "  17,\n",
       "  25,\n",
       "  378,\n",
       "  19,\n",
       "  8,\n",
       "  539,\n",
       "  7696,\n",
       "  262033,\n",
       "  17,\n",
       "  5,\n",
       "  80,\n",
       "  165,\n",
       "  25,\n",
       "  5830,\n",
       "  5743,\n",
       "  26,\n",
       "  1371,\n",
       "  19,\n",
       "  7,\n",
       "  574,\n",
       "  163,\n",
       "  27,\n",
       "  3894,\n",
       "  3566,\n",
       "  1,\n",
       "  1293,\n",
       "  1,\n",
       "  4,\n",
       "  137,\n",
       "  1371,\n",
       "  18,\n",
       "  137,\n",
       "  1161,\n",
       "  17,\n",
       "  3294,\n",
       "  19,\n",
       "  85,\n",
       "  5,\n",
       "  5411,\n",
       "  2,\n",
       "  694,\n",
       "  24,\n",
       "  48611,\n",
       "  18,\n",
       "  21,\n",
       "  7,\n",
       "  419,\n",
       "  1243622,\n",
       "  335,\n",
       "  13,\n",
       "  44,\n",
       "  110,\n",
       "  129,\n",
       "  8356,\n",
       "  5,\n",
       "  628,\n",
       "  43,\n",
       "  132,\n",
       "  8,\n",
       "  1,\n",
       "  709,\n",
       "  116,\n",
       "  2861,\n",
       "  32,\n",
       "  27326,\n",
       "  238,\n",
       "  19,\n",
       "  2,\n",
       "  756,\n",
       "  50,\n",
       "  13,\n",
       "  999,\n",
       "  18,\n",
       "  61,\n",
       "  94,\n",
       "  121,\n",
       "  574,\n",
       "  11,\n",
       "  207,\n",
       "  298,\n",
       "  29,\n",
       "  132,\n",
       "  5,\n",
       "  10467,\n",
       "  3582,\n",
       "  27,\n",
       "  13,\n",
       "  1903,\n",
       "  17,\n",
       "  44,\n",
       "  618,\n",
       "  132,\n",
       "  1716,\n",
       "  33,\n",
       "  574,\n",
       "  1,\n",
       "  17,\n",
       "  44,\n",
       "  351,\n",
       "  254,\n",
       "  0,\n",
       "  27,\n",
       "  120,\n",
       "  1,\n",
       "  2,\n",
       "  132,\n",
       "  2105,\n",
       "  6901,\n",
       "  4,\n",
       "  625,\n",
       "  10659,\n",
       "  54,\n",
       "  67,\n",
       "  17,\n",
       "  1731,\n",
       "  10,\n",
       "  15054,\n",
       "  19,\n",
       "  89043,\n",
       "  75,\n",
       "  2525,\n",
       "  106,\n",
       "  25,\n",
       "  2,\n",
       "  261,\n",
       "  1912,\n",
       "  13,\n",
       "  41,\n",
       "  1130,\n",
       "  19,\n",
       "  1,\n",
       "  133,\n",
       "  13,\n",
       "  1134240,\n",
       "  515,\n",
       "  15,\n",
       "  2597,\n",
       "  25,\n",
       "  1567,\n",
       "  4,\n",
       "  94,\n",
       "  121,\n",
       "  93,\n",
       "  7,\n",
       "  174,\n",
       "  5,\n",
       "  110,\n",
       "  167,\n",
       "  7,\n",
       "  270,\n",
       "  49,\n",
       "  384,\n",
       "  49,\n",
       "  154,\n",
       "  1,\n",
       "  2,\n",
       "  226,\n",
       "  2,\n",
       "  2137,\n",
       "  2734,\n",
       "  312,\n",
       "  2985,\n",
       "  1,\n",
       "  1134240,\n",
       "  17,\n",
       "  30,\n",
       "  7,\n",
       "  574,\n",
       "  26,\n",
       "  111,\n",
       "  17,\n",
       "  676,\n",
       "  39,\n",
       "  2597,\n",
       "  25,\n",
       "  7,\n",
       "  231,\n",
       "  298,\n",
       "  97,\n",
       "  8,\n",
       "  167,\n",
       "  1,\n",
       "  45,\n",
       "  174,\n",
       "  21,\n",
       "  129,\n",
       "  221,\n",
       "  370,\n",
       "  25,\n",
       "  1029,\n",
       "  19,\n",
       "  2597,\n",
       "  24,\n",
       "  3336,\n",
       "  567,\n",
       "  1,\n",
       "  69,\n",
       "  975,\n",
       "  1,\n",
       "  176,\n",
       "  1592,\n",
       "  567,\n",
       "  25,\n",
       "  3845,\n",
       "  803,\n",
       "  13641,\n",
       "  1,\n",
       "  1475,\n",
       "  1,\n",
       "  2614,\n",
       "  1,\n",
       "  2530,\n",
       "  1,\n",
       "  4,\n",
       "  141,\n",
       "  49,\n",
       "  1198,\n",
       "  6,\n",
       "  370,\n",
       "  7535,\n",
       "  10,\n",
       "  34,\n",
       "  7,\n",
       "  2443,\n",
       "  97,\n",
       "  1,\n",
       "  133,\n",
       "  33,\n",
       "  574,\n",
       "  2636,\n",
       "  17,\n",
       "  67,\n",
       "  7,\n",
       "  694,\n",
       "  10,\n",
       "  32728,\n",
       "  884,\n",
       "  1117,\n",
       "  19,\n",
       "  5,\n",
       "  2,\n",
       "  209,\n",
       "  1320,\n",
       "  281,\n",
       "  72,\n",
       "  324,\n",
       "  24,\n",
       "  976,\n",
       "  13,\n",
       "  88,\n",
       "  7,\n",
       "  263,\n",
       "  74,\n",
       "  6550,\n",
       "  2597,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  131,\n",
       "  5,\n",
       "  652,\n",
       "  5,\n",
       "  433,\n",
       "  1,\n",
       "  1559,\n",
       "  4,\n",
       "  256,\n",
       "  235,\n",
       "  1,\n",
       "  457,\n",
       "  798,\n",
       "  19,\n",
       "  1709,\n",
       "  1,\n",
       "  80,\n",
       "  201,\n",
       "  132,\n",
       "  387,\n",
       "  1,\n",
       "  4,\n",
       "  141,\n",
       "  49,\n",
       "  37,\n",
       "  724,\n",
       "  1,\n",
       "  19,\n",
       "  607,\n",
       "  307,\n",
       "  10,\n",
       "  18,\n",
       "  1492,\n",
       "  72,\n",
       "  713,\n",
       "  641966,\n",
       "  75,\n",
       "  49,\n",
       "  1,\n",
       "  13,\n",
       "  124,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  117,\n",
       "  696,\n",
       "  57,\n",
       "  31,\n",
       "  7079,\n",
       "  422,\n",
       "  689,\n",
       "  69,\n",
       "  55,\n",
       "  521,\n",
       "  225,\n",
       "  7,\n",
       "  574,\n",
       "  1518,\n",
       "  6,\n",
       "  79,\n",
       "  2114,\n",
       "  334,\n",
       "  8,\n",
       "  7696,\n",
       "  45,\n",
       "  121,\n",
       "  5,\n",
       "  4789,\n",
       "  21,\n",
       "  96,\n",
       "  51,\n",
       "  11237,\n",
       "  1,\n",
       "  1371,\n",
       "  10,\n",
       "  2,\n",
       "  1887,\n",
       "  298,\n",
       "  21,\n",
       "  2597,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [11,\n",
       "  579,\n",
       "  1,\n",
       "  5550,\n",
       "  2434,\n",
       "  4,\n",
       "  2607,\n",
       "  1,\n",
       "  38,\n",
       "  115,\n",
       "  837,\n",
       "  10,\n",
       "  415,\n",
       "  33,\n",
       "  2626,\n",
       "  187,\n",
       "  66,\n",
       "  72,\n",
       "  1,\n",
       "  129,\n",
       "  567,\n",
       "  30,\n",
       "  234,\n",
       "  693,\n",
       "  2,\n",
       "  2436,\n",
       "  32,\n",
       "  69,\n",
       "  693,\n",
       "  2,\n",
       "  391,\n",
       "  1,\n",
       "  577,\n",
       "  131,\n",
       "  1,\n",
       "  9445,\n",
       "  93,\n",
       "  10,\n",
       "  2411,\n",
       "  4,\n",
       "  254,\n",
       "  7,\n",
       "  574,\n",
       "  40,\n",
       "  15,\n",
       "  76,\n",
       "  28,\n",
       "  113,\n",
       "  4386,\n",
       "  129,\n",
       "  2620,\n",
       "  8,\n",
       "  677195,\n",
       "  4,\n",
       "  60618,\n",
       "  1292,\n",
       "  25,\n",
       "  166,\n",
       "  401,\n",
       "  4,\n",
       "  737,\n",
       "  2402,\n",
       "  200,\n",
       "  2597,\n",
       "  30,\n",
       "  7,\n",
       "  102,\n",
       "  1106,\n",
       "  19,\n",
       "  107,\n",
       "  45,\n",
       "  174,\n",
       "  107,\n",
       "  713,\n",
       "  2392,\n",
       "  21,\n",
       "  7843,\n",
       "  6938,\n",
       "  1,\n",
       "  174,\n",
       "  64,\n",
       "  301,\n",
       "  713,\n",
       "  19,\n",
       "  217,\n",
       "  8,\n",
       "  201,\n",
       "  4,\n",
       "  174,\n",
       "  64,\n",
       "  1021,\n",
       "  4,\n",
       "  84,\n",
       "  2620,\n",
       "  8,\n",
       "  63,\n",
       "  621,\n",
       "  8,\n",
       "  2756,\n",
       "  1109,\n",
       "  567,\n",
       "  66,\n",
       "  72,\n",
       "  257,\n",
       "  30,\n",
       "  7079,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  2620,\n",
       "  147,\n",
       "  93,\n",
       "  11,\n",
       "  63,\n",
       "  621,\n",
       "  1,\n",
       "  66,\n",
       "  3352,\n",
       "  4784,\n",
       "  32,\n",
       "  8109,\n",
       "  7,\n",
       "  3661,\n",
       "  2597,\n",
       "  937604,\n",
       "  98,\n",
       "  174,\n",
       "  567,\n",
       "  1,\n",
       "  45,\n",
       "  174,\n",
       "  2620,\n",
       "  179,\n",
       "  18,\n",
       "  10,\n",
       "  9517,\n",
       "  1708,\n",
       "  4,\n",
       "  15,\n",
       "  10,\n",
       "  200,\n",
       "  1409227,\n",
       "  6,\n",
       "  1252984,\n",
       "  1442,\n",
       "  63,\n",
       "  2626,\n",
       "  187,\n",
       "  45,\n",
       "  174,\n",
       "  107,\n",
       "  366,\n",
       "  7843,\n",
       "  234,\n",
       "  473,\n",
       "  274,\n",
       "  1,\n",
       "  59,\n",
       "  25,\n",
       "  603,\n",
       "  5,\n",
       "  161,\n",
       "  1007,\n",
       "  19,\n",
       "  94,\n",
       "  1,\n",
       "  38,\n",
       "  47,\n",
       "  603,\n",
       "  430,\n",
       "  60,\n",
       "  2419,\n",
       "  39,\n",
       "  567,\n",
       "  4,\n",
       "  8320,\n",
       "  44,\n",
       "  83,\n",
       "  121,\n",
       "  93,\n",
       "  11,\n",
       "  4884,\n",
       "  2597,\n",
       "  174,\n",
       "  567,\n",
       "  179,\n",
       "  23,\n",
       "  131,\n",
       "  17,\n",
       "  44,\n",
       "  108,\n",
       "  33,\n",
       "  234,\n",
       "  29,\n",
       "  703,\n",
       "  323,\n",
       "  8,\n",
       "  2,\n",
       "  2135,\n",
       "  6,\n",
       "  33,\n",
       "  206,\n",
       "  111,\n",
       "  166,\n",
       "  401,\n",
       "  25,\n",
       "  34,\n",
       "  2,\n",
       "  98,\n",
       "  1075,\n",
       "  415,\n",
       "  15,\n",
       "  343,\n",
       "  43,\n",
       "  1059,\n",
       "  1,\n",
       "  42,\n",
       "  793943,\n",
       "  1,\n",
       "  52,\n",
       "  5,\n",
       "  438851,\n",
       "  61,\n",
       "  4784,\n",
       "  4,\n",
       "  1973,\n",
       "  55,\n",
       "  58,\n",
       "  740,\n",
       "  422,\n",
       "  480,\n",
       "  280,\n",
       "  7,\n",
       "  273,\n",
       "  34,\n",
       "  98,\n",
       "  13,\n",
       "  462305,\n",
       "  43,\n",
       "  1581,\n",
       "  1,\n",
       "  18,\n",
       "  10,\n",
       "  83,\n",
       "  49,\n",
       "  6,\n",
       "  7,\n",
       "  502,\n",
       "  17,\n",
       "  44,\n",
       "  83,\n",
       "  30,\n",
       "  6393,\n",
       "  155,\n",
       "  437,\n",
       "  21,\n",
       "  75,\n",
       "  447,\n",
       "  13,\n",
       "  3468,\n",
       "  121,\n",
       "  2,\n",
       "  574,\n",
       "  5,\n",
       "  960,\n",
       "  2298,\n",
       "  284,\n",
       "  225,\n",
       "  7,\n",
       "  334,\n",
       "  21,\n",
       "  19227,\n",
       "  7,\n",
       "  1511,\n",
       "  541,\n",
       "  5,\n",
       "  174,\n",
       "  72,\n",
       "  567,\n",
       "  5317,\n",
       "  19,\n",
       "  93,\n",
       "  11,\n",
       "  4884,\n",
       "  32,\n",
       "  7079,\n",
       "  2597,\n",
       "  30,\n",
       "  7,\n",
       "  102,\n",
       "  1106,\n",
       "  11,\n",
       "  94,\n",
       "  6,\n",
       "  23,\n",
       "  642,\n",
       "  18,\n",
       "  125,\n",
       "  28,\n",
       "  8109,\n",
       "  60,\n",
       "  7,\n",
       "  825,\n",
       "  2785,\n",
       "  6,\n",
       "  7,\n",
       "  756,\n",
       "  32,\n",
       "  640,\n",
       "  1007,\n",
       "  19,\n",
       "  7,\n",
       "  2250,\n",
       "  1264,\n",
       "  1,\n",
       "  577,\n",
       "  131,\n",
       "  45,\n",
       "  215,\n",
       "  1615,\n",
       "  7,\n",
       "  574,\n",
       "  3267,\n",
       "  29,\n",
       "  132,\n",
       "  1,\n",
       "  36,\n",
       "  121,\n",
       "  93,\n",
       "  8,\n",
       "  64,\n",
       "  2756,\n",
       "  1109,\n",
       "  21,\n",
       "  437,\n",
       "  1,\n",
       "  2,\n",
       "  715,\n",
       "  41,\n",
       "  28,\n",
       "  1174,\n",
       "  1184,\n",
       "  4,\n",
       "  128,\n",
       "  34,\n",
       "  98,\n",
       "  25,\n",
       "  45,\n",
       "  1653,\n",
       "  1,\n",
       "  38,\n",
       "  45,\n",
       "  25,\n",
       "  1288,\n",
       "  4,\n",
       "  13,\n",
       "  455,\n",
       "  33,\n",
       "  1120,\n",
       "  21,\n",
       "  72,\n",
       "  45,\n",
       "  25,\n",
       "  3468,\n",
       "  3352,\n",
       "  4784,\n",
       "  1,\n",
       "  8109,\n",
       "  6190,\n",
       "  4,\n",
       "  40,\n",
       "  165,\n",
       "  221,\n",
       "  1918,\n",
       "  93,\n",
       "  271150,\n",
       "  28,\n",
       "  2146,\n",
       "  8,\n",
       "  115,\n",
       "  1175,\n",
       "  4,\n",
       "  3352,\n",
       "  2527,\n",
       "  1662095,\n",
       "  161,\n",
       "  5,\n",
       "  141,\n",
       "  70,\n",
       "  5,\n",
       "  1539,\n",
       "  93,\n",
       "  219,\n",
       "  113,\n",
       "  1220,\n",
       "  131,\n",
       "  5,\n",
       "  366,\n",
       "  234,\n",
       "  10,\n",
       "  155,\n",
       "  8968,\n",
       "  32,\n",
       "  7,\n",
       "  289,\n",
       "  4399,\n",
       "  15,\n",
       "  10,\n",
       "  200,\n",
       "  2597,\n",
       "  30,\n",
       "  7,\n",
       "  102,\n",
       "  1106,\n",
       "  19,\n",
       "  94,\n",
       "  232,\n",
       "  2734,\n",
       "  1134240,\n",
       "  1,\n",
       "  94,\n",
       "  121,\n",
       "  574,\n",
       "  40,\n",
       "  114,\n",
       "  2,\n",
       "  152,\n",
       "  1,\n",
       "  11,\n",
       "  51,\n",
       "  717,\n",
       "  32,\n",
       "  219,\n",
       "  1,\n",
       "  38,\n",
       "  45,\n",
       "  25,\n",
       "  113,\n",
       "  1288,\n",
       "  8,\n",
       "  64,\n",
       "  152,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2734,\n",
       "  312,\n",
       "  2985,\n",
       "  1,\n",
       "  13,\n",
       "  24942,\n",
       "  15,\n",
       "  2597,\n",
       "  30,\n",
       "  7,\n",
       "  2346,\n",
       "  1106,\n",
       "  19,\n",
       "  6193,\n",
       "  1109,\n",
       "  13,\n",
       "  24942,\n",
       "  23,\n",
       "  133,\n",
       "  74,\n",
       "  1413,\n",
       "  5,\n",
       "  141,\n",
       "  70,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  80,\n",
       "  61,\n",
       "  27,\n",
       "  141,\n",
       "  27,\n",
       "  45,\n",
       "  136,\n",
       "  1,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  1413,\n",
       "  400,\n",
       "  70,\n",
       "  21,\n",
       "  63,\n",
       "  234,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  574,\n",
       "  244,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  47,\n",
       "  497,\n",
       "  43,\n",
       "  100,\n",
       "  717,\n",
       "  10,\n",
       "  13,\n",
       "  24942,\n",
       "  15,\n",
       "  94,\n",
       "  147,\n",
       "  5,\n",
       "  80,\n",
       "  61,\n",
       "  49,\n",
       "  67,\n",
       "  45,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  80,\n",
       "  61,\n",
       "  1,\n",
       "  45,\n",
       "  47,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  422818,\n",
       "  4,\n",
       "  15,\n",
       "  10,\n",
       "  113,\n",
       "  13577,\n",
       "  796,\n",
       "  6,\n",
       "  1459,\n",
       "  2,\n",
       "  370,\n",
       "  32,\n",
       "  2,\n",
       "  3970,\n",
       "  45,\n",
       "  136,\n",
       "  80,\n",
       "  61,\n",
       "  4,\n",
       "  295,\n",
       "  2,\n",
       "  195,\n",
       "  13,\n",
       "  83,\n",
       "  24942,\n",
       "  15,\n",
       "  45,\n",
       "  136,\n",
       "  676,\n",
       "  1096,\n",
       "  133,\n",
       "  13,\n",
       "  331,\n",
       "  66,\n",
       "  45,\n",
       "  25,\n",
       "  9889,\n",
       "  2,\n",
       "  1329,\n",
       "  6,\n",
       "  1096,\n",
       "  40,\n",
       "  193,\n",
       "  93,\n",
       "  36,\n",
       "  76,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  156,\n",
       "  5,\n",
       "  2151,\n",
       "  64,\n",
       "  5827,\n",
       "  4,\n",
       "  0,\n",
       "  76,\n",
       "  36,\n",
       "  39,\n",
       "  219,\n",
       "  717,\n",
       "  10,\n",
       "  15,\n",
       "  45,\n",
       "  47,\n",
       "  34,\n",
       "  1413,\n",
       "  400,\n",
       "  70,\n",
       "  21,\n",
       "  234,\n",
       "  50,\n",
       "  17,\n",
       "  30,\n",
       "  234,\n",
       "  582,\n",
       "  17,\n",
       "  1,\n",
       "  116,\n",
       "  17,\n",
       "  136,\n",
       "  161,\n",
       "  1867,\n",
       "  6,\n",
       "  15,\n",
       "  4,\n",
       "  7083,\n",
       "  21,\n",
       "  51,\n",
       "  219,\n",
       "  17,\n",
       "  44,\n",
       "  30,\n",
       "  452,\n",
       "  21,\n",
       "  33,\n",
       "  234,\n",
       "  24,\n",
       "  803,\n",
       "  370,\n",
       "  17,\n",
       "  44,\n",
       "  83,\n",
       "  30,\n",
       "  452,\n",
       "  24,\n",
       "  69,\n",
       "  3773,\n",
       "  61,\n",
       "  1,\n",
       "  73,\n",
       "  3830,\n",
       "  33,\n",
       "  501,\n",
       "  1195,\n",
       "  4,\n",
       "  2,\n",
       "  574,\n",
       "  244,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  257,\n",
       "  47,\n",
       "  15,\n",
       "  1,\n",
       "  73,\n",
       "  2388,\n",
       "  72,\n",
       "  5,\n",
       "  43,\n",
       "  209,\n",
       "  307,\n",
       "  2,\n",
       "  574,\n",
       "  244,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  47,\n",
       "  497,\n",
       "  1,\n",
       "  54,\n",
       "  161,\n",
       "  1867,\n",
       "  6,\n",
       "  57,\n",
       "  17,\n",
       "  30,\n",
       "  97,\n",
       "  79,\n",
       "  574,\n",
       "  70,\n",
       "  10,\n",
       "  877,\n",
       "  1,\n",
       "  38,\n",
       "  18,\n",
       "  44,\n",
       "  28,\n",
       "  113,\n",
       "  2955,\n",
       "  24,\n",
       "  357,\n",
       "  0,\n",
       "  5,\n",
       "  18,\n",
       "  2,\n",
       "  574,\n",
       "  271150,\n",
       "  30,\n",
       "  1207,\n",
       "  6,\n",
       "  154,\n",
       "  1,\n",
       "  38,\n",
       "  59,\n",
       "  10,\n",
       "  538,\n",
       "  15,\n",
       "  17,\n",
       "  244,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  149,\n",
       "  8,\n",
       "  7,\n",
       "  247,\n",
       "  33,\n",
       "  574,\n",
       "  244,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  146,\n",
       "  33,\n",
       "  829,\n",
       "  1,\n",
       "  54,\n",
       "  59,\n",
       "  10,\n",
       "  7,\n",
       "  717,\n",
       "  200,\n",
       "  36,\n",
       "  136,\n",
       "  80,\n",
       "  172,\n",
       "  2,\n",
       "  574,\n",
       "  59,\n",
       "  25,\n",
       "  216732,\n",
       "  6,\n",
       "  84,\n",
       "  221,\n",
       "  15,\n",
       "  17,\n",
       "  44,\n",
       "  47,\n",
       "  15,\n",
       "  17,\n",
       "  244,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  47,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  1,\n",
       "  296,\n",
       "  777,\n",
       "  1,\n",
       "  640,\n",
       "  315,\n",
       "  6,\n",
       "  234,\n",
       "  1,\n",
       "  4,\n",
       "  96,\n",
       "  6,\n",
       "  33,\n",
       "  282025,\n",
       "  696,\n",
       "  15,\n",
       "  2,\n",
       "  574,\n",
       "  271150,\n",
       "  28,\n",
       "  127,\n",
       "  29,\n",
       "  70,\n",
       "  5,\n",
       "  70,\n",
       "  1,\n",
       "  38,\n",
       "  18,\n",
       "  44,\n",
       "  34,\n",
       "  291,\n",
       "  33,\n",
       "  167,\n",
       "  11,\n",
       "  17,\n",
       "  15,\n",
       "  10,\n",
       "  200,\n",
       "  13,\n",
       "  24942,\n",
       "  15,\n",
       "  36,\n",
       "  136,\n",
       "  1413,\n",
       "  398,\n",
       "  70,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  1,\n",
       "  4,\n",
       "  49,\n",
       "  70,\n",
       "  357,\n",
       "  61,\n",
       "  1,\n",
       "  4,\n",
       "  332,\n",
       "  452,\n",
       "  21,\n",
       "  234,\n",
       "  1,\n",
       "  4,\n",
       "  15,\n",
       "  2,\n",
       "  2,\n",
       "  574,\n",
       "  244,\n",
       "  856,\n",
       "  65,\n",
       "  769,\n",
       "  47,\n",
       "  497,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2734,\n",
       "  677195,\n",
       "  9603,\n",
       "  1,\n",
       "  49,\n",
       "  4,\n",
       "  49,\n",
       "  94,\n",
       "  121,\n",
       "  2597,\n",
       "  1,\n",
       "  38,\n",
       "  34,\n",
       "  521,\n",
       "  8741,\n",
       "  15,\n",
       "  23,\n",
       "  1245,\n",
       "  353971,\n",
       "  165,\n",
       "  74,\n",
       "  288,\n",
       "  0,\n",
       "  8,\n",
       "  516,\n",
       "  515,\n",
       "  15,\n",
       "  2597,\n",
       "  30,\n",
       "  7,\n",
       "  1502,\n",
       "  1106,\n",
       "  19,\n",
       "  94,\n",
       "  38,\n",
       "  2597,\n",
       "  44,\n",
       "  28,\n",
       "  7,\n",
       "  443,\n",
       "  298,\n",
       "  45,\n",
       "  44,\n",
       "  30907,\n",
       "  301,\n",
       "  29,\n",
       "  622722,\n",
       "  1,\n",
       "  3845,\n",
       "  1096,\n",
       "  4,\n",
       "  2108,\n",
       "  70,\n",
       "  21,\n",
       "  234,\n",
       "  4,\n",
       "  433,\n",
       "  100,\n",
       "  6,\n",
       "  40,\n",
       "  1,\n",
       "  2597,\n",
       "  44,\n",
       "  647,\n",
       "  301,\n",
       "  29,\n",
       "  1292817,\n",
       "  1134240,\n",
       "  13,\n",
       "  44,\n",
       "  228,\n",
       "  80,\n",
       "  885,\n",
       "  119,\n",
       "  201,\n",
       "  1,\n",
       "  133,\n",
       "  13,\n",
       "  65,\n",
       "  754,\n",
       "  3472,\n",
       "  43,\n",
       "  437,\n",
       "  1252984,\n",
       "  190,\n",
       "  201,\n",
       "  906,\n",
       "  7,\n",
       "  806,\n",
       "  548449,\n",
       "  745,\n",
       "  1409227,\n",
       "  6,\n",
       "  456,\n",
       "  26,\n",
       "  7,\n",
       "  1063,\n",
       "  201,\n",
       "  0,\n",
       "  67,\n",
       "  45,\n",
       "  230,\n",
       "  111,\n",
       "  45,\n",
       "  145,\n",
       "  160,\n",
       "  19,\n",
       "  5,\n",
       "  2,\n",
       "  574,\n",
       "  23,\n",
       "  745,\n",
       "  15,\n",
       "  631,\n",
       "  729,\n",
       "  6,\n",
       "  2,\n",
       "  456,\n",
       "  26,\n",
       "  69,\n",
       "  2,\n",
       "  1063,\n",
       "  201,\n",
       "  25,\n",
       "  19,\n",
       "  2,\n",
       "  574,\n",
       "  23,\n",
       "  10,\n",
       "  131,\n",
       "  2597,\n",
       "  44,\n",
       "  28,\n",
       "  7,\n",
       "  443,\n",
       "  298,\n",
       "  10672,\n",
       "  1,\n",
       "  79,\n",
       "  301,\n",
       "  1413,\n",
       "  49,\n",
       "  70,\n",
       "  8,\n",
       "  2,\n",
       "  25837,\n",
       "  12030,\n",
       "  101,\n",
       "  2,\n",
       "  5685,\n",
       "  13,\n",
       "  1413,\n",
       "  115,\n",
       "  6,\n",
       "  43,\n",
       "  70,\n",
       "  5883,\n",
       "  8,\n",
       "  60995,\n",
       "  16,\n",
       "  32,\n",
       "  1315,\n",
       "  14,\n",
       "  101,\n",
       "  1279,\n",
       "  417,\n",
       "  1252984,\n",
       "  1063,\n",
       "  201,\n",
       "  906,\n",
       "  2031,\n",
       "  1,\n",
       "  7,\n",
       "  596,\n",
       "  10,\n",
       "  9364,\n",
       "  26,\n",
       "  7,\n",
       "  574,\n",
       "  997,\n",
       "  803,\n",
       "  7,\n",
       "  195,\n",
       "  32,\n",
       "  19,\n",
       "  1086,\n",
       "  672,\n",
       "  53891,\n",
       "  18,\n",
       "  65,\n",
       "  233,\n",
       "  7,\n",
       "  550,\n",
       "  126,\n",
       "  8,\n",
       "  2,\n",
       "  1063,\n",
       "  6,\n",
       "  186231,\n",
       "  249698,\n",
       "  61,\n",
       "  6,\n",
       "  284697,\n",
       "  456,\n",
       "  0,\n",
       "  15,\n",
       "  45,\n",
       "  1413,\n",
       "  49,\n",
       "  70,\n",
       "  12030,\n",
       "  101,\n",
       "  61,\n",
       "  2597,\n",
       "  44,\n",
       "  28,\n",
       "  7,\n",
       "  443,\n",
       "  298,\n",
       "  8,\n",
       "  7,\n",
       "  20636,\n",
       "  904,\n",
       "  1,\n",
       "  2597,\n",
       "  44,\n",
       "  12161,\n",
       "  21,\n",
       "  7,\n",
       "  20636,\n",
       "  70,\n",
       "  2108,\n",
       "  21,\n",
       "  234,\n",
       "  32,\n",
       "  433,\n",
       "  13,\n",
       "  1134240,\n",
       "  1413,\n",
       "  42,\n",
       "  1073,\n",
       "  1625,\n",
       "  26,\n",
       "  7,\n",
       "  234,\n",
       "  438,\n",
       "  19,\n",
       "  106,\n",
       "  574,\n",
       "  0,\n",
       "  4002,\n",
       "  906,\n",
       "  855496,\n",
       "  61,\n",
       "  6,\n",
       "  1721002,\n",
       "  933,\n",
       "  30,\n",
       "  7,\n",
       "  574,\n",
       "  4,\n",
       "  18,\n",
       "  10,\n",
       "  0,\n",
       "  173,\n",
       "  0,\n",
       "  24,\n",
       "  301,\n",
       "  23,\n",
       "  745,\n",
       "  15,\n",
       "  189,\n",
       "  458639,\n",
       "  6,\n",
       "  2,\n",
       "  432624,\n",
       "  65,\n",
       "  259,\n",
       "  1006,\n",
       "  10,\n",
       "  228,\n",
       "  61,\n",
       "  371,\n",
       "  803,\n",
       "  21,\n",
       "  433,\n",
       "  2597,\n",
       "  44,\n",
       "  47,\n",
       "  49,\n",
       "  5063,\n",
       "  116,\n",
       "  102,\n",
       "  8,\n",
       "  7,\n",
       "  259,\n",
       "  1006,\n",
       "  18,\n",
       "  44,\n",
       "  647,\n",
       "  301,\n",
       "  29,\n",
       "  11516,\n",
       "  3845,\n",
       "  2,\n",
       "  61,\n",
       "  2485,\n",
       "  4,\n",
       "  2108,\n",
       "  70,\n",
       "  21,\n",
       "  234,\n",
       "  13,\n",
       "  686,\n",
       "  17,\n",
       "  5,\n",
       "  484,\n",
       "  33,\n",
       "  301,\n",
       "  32,\n",
       "  1396,\n",
       "  301,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  854,\n",
       "  182,\n",
       "  19,\n",
       "  574,\n",
       "  70,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testE[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to load flags\n",
      "\n",
      "Writing to /mnt/data/rajivratn/memory_networks/automated-essay-grading/runs/adversary_training/essay_set_5_cv_1_Mar_25_2020_15:36:12\n",
      "\n",
      "                                               names  essay_id\n",
      "0  2, \"In this memoir of Narciso Rodriguez, @PERS...         1\n",
      "1  1, The mood created by the author is showing h...         2\n",
      "2  3, \"The mood created by the author in the memo...         3\n",
      "3  4, \"In the memoir \"\"@PERSON1 Rodriguez\"\" from ...         4\n",
      "4  2, \"The author's interpretation of their child...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 371 \n",
      "mean sentence size: 114\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 452 \n",
      "mean sentence size: 129\n",
      "\n",
      "361\n",
      "The size of training data: 1439\n",
      "The size of testing data: 361\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n",
      "Finish epoch 1, total training cost is 94961.56799316406, time spent is 1.2556993961334229\n",
      "Finish epoch 2, total training cost is 38812.06191253662, time spent is 0.8987135887145996\n",
      "Finish epoch 3, total training cost is 21734.44490814209, time spent is 1.032458782196045\n",
      "Finish epoch 4, total training cost is 17606.736068725586, time spent is 1.1784284114837646\n",
      "Finish epoch 5, total training cost is 13847.842525482178, time spent is 1.185840129852295\n",
      "Training kappa score = 0.3262199614062471\n",
      "Testing kappa score = 0.08966169100993338\n",
      "Finish epoch 6, total training cost is 13238.493877410889, time spent is 1.1562795639038086\n",
      "Finish epoch 7, total training cost is 12120.09765625, time spent is 1.1087279319763184\n",
      "Finish epoch 8, total training cost is 10755.195013046265, time spent is 1.1792211532592773\n",
      "Finish epoch 9, total training cost is 10209.990505218506, time spent is 1.1631062030792236\n",
      "Finish epoch 10, total training cost is 9342.653831481934, time spent is 1.114173412322998\n",
      "Training kappa score = 0.32460176585924905\n",
      "Testing kappa score = 0.02242911958050653\n",
      "Finish epoch 11, total training cost is 9361.394771575928, time spent is 1.0982272624969482\n",
      "Finish epoch 12, total training cost is 8889.734004974365, time spent is 1.1194703578948975\n",
      "Finish epoch 13, total training cost is 9213.695978164673, time spent is 1.1924216747283936\n",
      "Finish epoch 14, total training cost is 8223.91153717041, time spent is 1.224754810333252\n",
      "Finish epoch 15, total training cost is 7865.1039962768555, time spent is 1.1281309127807617\n",
      "Training kappa score = 0.3581969306478737\n",
      "Testing kappa score = 0.04920697115794137\n",
      "Finish epoch 16, total training cost is 7014.855075836182, time spent is 1.1657323837280273\n",
      "Finish epoch 17, total training cost is 7293.944393157959, time spent is 1.1411006450653076\n",
      "Finish epoch 18, total training cost is 6614.2279415130615, time spent is 1.168820858001709\n",
      "Finish epoch 19, total training cost is 6672.184825897217, time spent is 1.1259098052978516\n",
      "Finish epoch 20, total training cost is 6590.881315231323, time spent is 1.1040823459625244\n",
      "Training kappa score = 0.5922144597431596\n",
      "Testing kappa score = 0.1191637054788861\n",
      "Finish epoch 21, total training cost is 6423.630056381226, time spent is 1.109424352645874\n",
      "Finish epoch 22, total training cost is 6220.207763671875, time spent is 1.1203181743621826\n",
      "Finish epoch 23, total training cost is 5862.651514053345, time spent is 1.1247966289520264\n",
      "Finish epoch 24, total training cost is 5781.927261352539, time spent is 1.1133098602294922\n",
      "Finish epoch 25, total training cost is 5470.752601623535, time spent is 1.1509497165679932\n",
      "Training kappa score = 0.6395902804421931\n",
      "Testing kappa score = 0.05225721398109007\n",
      "Finish epoch 26, total training cost is 5492.42346572876, time spent is 1.1433873176574707\n",
      "Finish epoch 27, total training cost is 5251.498199462891, time spent is 1.1145343780517578\n",
      "Finish epoch 28, total training cost is 5493.826166152954, time spent is 1.137620210647583\n",
      "Finish epoch 29, total training cost is 5065.491165161133, time spent is 1.1274831295013428\n",
      "Finish epoch 30, total training cost is 4870.868111610413, time spent is 1.1093034744262695\n",
      "Training kappa score = 0.6832781903639435\n",
      "Testing kappa score = 0.07876405269328901\n",
      "Finish epoch 31, total training cost is 4677.302260398865, time spent is 1.1062211990356445\n",
      "Finish epoch 32, total training cost is 4766.879199981689, time spent is 1.2071278095245361\n",
      "Finish epoch 33, total training cost is 4652.594795227051, time spent is 1.185490608215332\n",
      "Finish epoch 34, total training cost is 4432.713677406311, time spent is 1.206099510192871\n",
      "Finish epoch 35, total training cost is 4379.868669509888, time spent is 1.1907939910888672\n",
      "Training kappa score = 0.6736620409901852\n",
      "Testing kappa score = 0.09488325185942603\n",
      "Finish epoch 36, total training cost is 4158.059415817261, time spent is 1.1996963024139404\n",
      "Finish epoch 37, total training cost is 4286.398623466492, time spent is 1.1954262256622314\n",
      "Finish epoch 38, total training cost is 4193.332963943481, time spent is 1.2617247104644775\n",
      "Finish epoch 39, total training cost is 4164.739689826965, time spent is 1.319530963897705\n",
      "Finish epoch 40, total training cost is 4073.2880239486694, time spent is 1.3026514053344727\n",
      "Training kappa score = 0.6021729247983902\n",
      "Testing kappa score = 0.0671292173830873\n",
      "Finish epoch 41, total training cost is 4057.1132946014404, time spent is 1.3041181564331055\n",
      "Finish epoch 42, total training cost is 3713.8539838790894, time spent is 1.243288516998291\n",
      "Finish epoch 43, total training cost is 3793.968695640564, time spent is 1.196871042251587\n",
      "Finish epoch 44, total training cost is 3822.361095428467, time spent is 1.2115602493286133\n",
      "Finish epoch 45, total training cost is 4008.4405517578125, time spent is 1.2118451595306396\n",
      "Training kappa score = 0.7095063883024137\n",
      "Testing kappa score = 0.07491332494082448\n",
      "Finish epoch 46, total training cost is 3749.700996875763, time spent is 1.2720212936401367\n",
      "Finish epoch 47, total training cost is 3529.8901081085205, time spent is 1.2103519439697266\n",
      "Finish epoch 48, total training cost is 3424.2273240089417, time spent is 1.1870334148406982\n",
      "Finish epoch 49, total training cost is 3318.103512763977, time spent is 1.147852897644043\n",
      "Finish epoch 50, total training cost is 3490.253921508789, time spent is 1.1309640407562256\n",
      "Training kappa score = 0.7088296191722695\n",
      "Testing kappa score = 0.09079522692024211\n",
      "Finish epoch 51, total training cost is 3445.262830734253, time spent is 1.2176434993743896\n",
      "Finish epoch 52, total training cost is 3418.118535041809, time spent is 1.10007905960083\n",
      "Finish epoch 53, total training cost is 3080.9486236572266, time spent is 1.1386282444000244\n",
      "Finish epoch 54, total training cost is 3198.5146560668945, time spent is 1.1469719409942627\n",
      "Finish epoch 55, total training cost is 3278.0019764900208, time spent is 1.0603578090667725\n",
      "Training kappa score = 0.7224014636263936\n",
      "Testing kappa score = 0.06890430460464259\n",
      "Finish epoch 56, total training cost is 3466.4377298355103, time spent is 1.1572725772857666\n",
      "Finish epoch 57, total training cost is 3235.624568939209, time spent is 1.1514759063720703\n",
      "Finish epoch 58, total training cost is 3023.2931451797485, time spent is 1.1701703071594238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 59, total training cost is 2750.8058581352234, time spent is 1.1649034023284912\n",
      "Finish epoch 60, total training cost is 3039.6970529556274, time spent is 1.1537415981292725\n",
      "Training kappa score = 0.7989829984238559\n",
      "Testing kappa score = 0.10997675207452573\n",
      "Finish epoch 61, total training cost is 2968.6233139038086, time spent is 1.1748902797698975\n",
      "Finish epoch 62, total training cost is 3008.0530219078064, time spent is 1.1389391422271729\n",
      "Finish epoch 63, total training cost is 2689.3619718551636, time spent is 1.1304388046264648\n",
      "Finish epoch 64, total training cost is 2669.113923072815, time spent is 1.1438019275665283\n",
      "Finish epoch 65, total training cost is 2616.634388923645, time spent is 1.119722843170166\n",
      "Training kappa score = 0.7983552764526223\n",
      "Testing kappa score = 0.08824644333310483\n",
      "Finish epoch 66, total training cost is 2411.1045427322388, time spent is 1.125469446182251\n",
      "Finish epoch 67, total training cost is 2387.3488302230835, time spent is 1.1300559043884277\n",
      "Finish epoch 68, total training cost is 2348.787908554077, time spent is 1.158383846282959\n",
      "Finish epoch 69, total training cost is 2614.753731250763, time spent is 1.1323928833007812\n",
      "Finish epoch 70, total training cost is 2661.6169142723083, time spent is 1.143723726272583\n",
      "Training kappa score = 0.7320496062525941\n",
      "Testing kappa score = 0.11051025873914155\n",
      "Finish epoch 71, total training cost is 2436.3026447296143, time spent is 1.166358470916748\n",
      "Finish epoch 72, total training cost is 2492.8886132240295, time spent is 1.1740672588348389\n",
      "Finish epoch 73, total training cost is 2159.6474838256836, time spent is 1.163999319076538\n",
      "Finish epoch 74, total training cost is 2095.630973815918, time spent is 1.181368350982666\n",
      "Finish epoch 75, total training cost is 2077.893039703369, time spent is 1.1051278114318848\n",
      "Training kappa score = 0.7666521510094364\n",
      "Testing kappa score = 0.10181007640155049\n",
      "                                               names  essay_id\n",
      "0  2, \"In this memoir of Narciso Rodriguez, @PERS...         1\n",
      "1  1, The mood created by the author is showing h...         2\n",
      "2  3, \"The mood created by the author in the memo...         3\n",
      "3  4, \"In the memoir \"\"@PERSON1 Rodriguez\"\" from ...         4\n",
      "4  2, \"The author's interpretation of their child...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 369 \n",
      "mean sentence size: 129\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 452 \n",
      "mean sentence size: 129\n",
      "\n",
      "361\n",
      "The size of training data: 1439\n",
      "The size of testing data: 361\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n",
      "Finish epoch 1, total training cost is 71407.85160064697, time spent is 1.8364856243133545\n",
      "Finish epoch 2, total training cost is 21344.2596282959, time spent is 1.2521822452545166\n",
      "Finish epoch 3, total training cost is 15672.692459106445, time spent is 1.3115324974060059\n",
      "Finish epoch 4, total training cost is 12946.69137954712, time spent is 1.294365644454956\n",
      "Finish epoch 5, total training cost is 11213.363418579102, time spent is 1.331336498260498\n",
      "Training kappa score = 0.6907760794034984\n",
      "Testing kappa score = 0.018224226704347535\n",
      "Finish epoch 6, total training cost is 10178.88353919983, time spent is 1.281184434890747\n",
      "Finish epoch 7, total training cost is 9004.549770355225, time spent is 1.3055593967437744\n",
      "Finish epoch 8, total training cost is 8828.045483589172, time spent is 1.2981631755828857\n",
      "Finish epoch 9, total training cost is 7650.236479759216, time spent is 1.2545814514160156\n",
      "Finish epoch 10, total training cost is 7512.896184921265, time spent is 1.3176417350769043\n",
      "Training kappa score = 0.6246213544526982\n",
      "Testing kappa score = 0.0032527705283863506\n",
      "Finish epoch 11, total training cost is 7776.41934967041, time spent is 1.2939727306365967\n",
      "Finish epoch 12, total training cost is 6894.923164367676, time spent is 1.272472858428955\n",
      "Finish epoch 13, total training cost is 6990.134978294373, time spent is 1.3216016292572021\n",
      "Finish epoch 14, total training cost is 6605.499155044556, time spent is 1.3033263683319092\n",
      "Finish epoch 15, total training cost is 6871.500590324402, time spent is 1.302739143371582\n",
      "Training kappa score = 0.6814662603304362\n",
      "Testing kappa score = 0.02681791128382882\n",
      "Finish epoch 16, total training cost is 6922.072641372681, time spent is 1.2912192344665527\n",
      "Finish epoch 17, total training cost is 6226.2792682647705, time spent is 1.3078124523162842\n",
      "Finish epoch 18, total training cost is 6105.864486694336, time spent is 1.2955687046051025\n",
      "Finish epoch 19, total training cost is 5847.541864395142, time spent is 1.312551498413086\n",
      "Finish epoch 20, total training cost is 5658.695529937744, time spent is 1.3062076568603516\n",
      "Training kappa score = 0.6544071713932218\n",
      "Testing kappa score = 0.04253799857186591\n",
      "Finish epoch 21, total training cost is 5532.531517028809, time spent is 1.3235478401184082\n",
      "Finish epoch 22, total training cost is 5541.004828453064, time spent is 1.3124728202819824\n",
      "Finish epoch 23, total training cost is 5650.076847076416, time spent is 1.2973368167877197\n",
      "Finish epoch 24, total training cost is 5565.2588720321655, time spent is 1.231581687927246\n",
      "Finish epoch 25, total training cost is 5307.703050613403, time spent is 1.2387328147888184\n",
      "Training kappa score = 0.7437872491702491\n",
      "Testing kappa score = 0.05938569307974162\n",
      "Finish epoch 26, total training cost is 5284.222559928894, time spent is 1.3027753829956055\n",
      "Finish epoch 27, total training cost is 4915.404797077179, time spent is 1.2840650081634521\n",
      "Finish epoch 28, total training cost is 5014.889112472534, time spent is 1.3091213703155518\n",
      "Finish epoch 29, total training cost is 4750.6071434021, time spent is 1.2913799285888672\n",
      "Finish epoch 30, total training cost is 5010.048575401306, time spent is 1.2656280994415283\n",
      "Training kappa score = 0.7062027782723596\n",
      "Testing kappa score = 0.08568497912078821\n",
      "Finish epoch 31, total training cost is 4592.066668987274, time spent is 1.2434511184692383\n",
      "Finish epoch 32, total training cost is 4595.614077568054, time spent is 1.2314085960388184\n",
      "Finish epoch 33, total training cost is 4697.922060966492, time spent is 1.2909595966339111\n",
      "Finish epoch 34, total training cost is 4587.769629478455, time spent is 1.2383198738098145\n",
      "Finish epoch 35, total training cost is 4275.9324197769165, time spent is 1.1850061416625977\n",
      "Training kappa score = 0.7047750936041443\n",
      "Testing kappa score = 0.05255685213425565\n",
      "Finish epoch 36, total training cost is 4219.465682029724, time spent is 1.277071475982666\n",
      "Finish epoch 37, total training cost is 4524.383321762085, time spent is 1.2123980522155762\n",
      "Finish epoch 38, total training cost is 4137.537035703659, time spent is 1.2410740852355957\n",
      "Finish epoch 39, total training cost is 4333.251415252686, time spent is 1.258390188217163\n",
      "Finish epoch 40, total training cost is 3928.8635568618774, time spent is 1.2648673057556152\n",
      "Training kappa score = 0.7129929897394607\n",
      "Testing kappa score = -0.002854214836835345\n",
      "Finish epoch 41, total training cost is 4116.309654712677, time spent is 1.241450309753418\n",
      "Finish epoch 42, total training cost is 4017.1304035186768, time spent is 1.289788007736206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 43, total training cost is 3884.0906496047974, time spent is 1.2893686294555664\n",
      "Finish epoch 44, total training cost is 4037.2582273483276, time spent is 1.2415132522583008\n",
      "Finish epoch 45, total training cost is 4177.274675369263, time spent is 1.2682571411132812\n",
      "Training kappa score = 0.7577709624045951\n",
      "Testing kappa score = 0.07018775531695509\n",
      "Finish epoch 46, total training cost is 3920.5119214057922, time spent is 1.270003080368042\n",
      "Finish epoch 47, total training cost is 4022.1473412513733, time spent is 1.2505526542663574\n",
      "Finish epoch 48, total training cost is 3949.310520172119, time spent is 1.2937395572662354\n",
      "Finish epoch 49, total training cost is 3824.9885215759277, time spent is 1.323718786239624\n",
      "Finish epoch 50, total training cost is 3890.7696323394775, time spent is 1.234682559967041\n",
      "Training kappa score = 0.7665803018667833\n",
      "Testing kappa score = -0.0013320164534305068\n",
      "Finish epoch 51, total training cost is 3631.0906438827515, time spent is 1.2866854667663574\n",
      "Finish epoch 52, total training cost is 3505.488256454468, time spent is 1.2560443878173828\n",
      "Finish epoch 53, total training cost is 3691.776041984558, time spent is 1.280937910079956\n",
      "Finish epoch 54, total training cost is 3467.5821056365967, time spent is 1.2696270942687988\n",
      "Finish epoch 55, total training cost is 3713.3457441329956, time spent is 1.2180912494659424\n",
      "Training kappa score = 0.7363233945816017\n",
      "Testing kappa score = 0.043963789787431096\n",
      "Finish epoch 56, total training cost is 3475.7720766067505, time spent is 1.2995185852050781\n",
      "Finish epoch 57, total training cost is 3484.3063383102417, time spent is 1.2712056636810303\n",
      "Finish epoch 58, total training cost is 3254.8916964530945, time spent is 1.3122518062591553\n",
      "Finish epoch 59, total training cost is 3310.549596786499, time spent is 1.3000893592834473\n",
      "Finish epoch 60, total training cost is 3248.6328630447388, time spent is 1.274550437927246\n",
      "Training kappa score = 0.7537307352133079\n",
      "Testing kappa score = 0.08445034510344562\n",
      "Finish epoch 61, total training cost is 3299.9506797790527, time spent is 1.2807443141937256\n",
      "Finish epoch 62, total training cost is 3176.610523223877, time spent is 1.240422010421753\n",
      "Finish epoch 63, total training cost is 3241.660202026367, time spent is 1.2738165855407715\n",
      "Finish epoch 64, total training cost is 3114.1514263153076, time spent is 1.2647855281829834\n",
      "Finish epoch 65, total training cost is 3168.9381551742554, time spent is 1.3281874656677246\n",
      "Training kappa score = 0.7656258187906669\n",
      "Testing kappa score = 0.04575443332597884\n",
      "Finish epoch 66, total training cost is 3165.221667289734, time spent is 1.2820327281951904\n",
      "Finish epoch 67, total training cost is 3046.33931350708, time spent is 1.2881929874420166\n",
      "Finish epoch 68, total training cost is 3038.810260772705, time spent is 1.2852861881256104\n",
      "Finish epoch 69, total training cost is 3045.9061222076416, time spent is 1.2366971969604492\n",
      "Finish epoch 70, total training cost is 2996.865625858307, time spent is 1.353729248046875\n",
      "Training kappa score = 0.7708091777001174\n",
      "Testing kappa score = 0.04721627631496905\n",
      "Finish epoch 71, total training cost is 3067.0863103866577, time spent is 1.265362024307251\n",
      "Finish epoch 72, total training cost is 2852.0211415290833, time spent is 1.3276326656341553\n",
      "Finish epoch 73, total training cost is 2849.560128211975, time spent is 1.293517827987671\n",
      "Finish epoch 74, total training cost is 2827.740342617035, time spent is 1.281095266342163\n",
      "Finish epoch 75, total training cost is 2789.8043060302734, time spent is 1.275650978088379\n",
      "Training kappa score = 0.7869636668520991\n",
      "Testing kappa score = -0.0005658102977472712\n",
      "Finish epoch 76, total training cost is 2851.323540210724, time spent is 1.2587013244628906\n",
      "Finish epoch 77, total training cost is 2857.398434638977, time spent is 1.3060088157653809\n",
      "Finish epoch 78, total training cost is 2722.498031139374, time spent is 1.3836262226104736\n",
      "Finish epoch 79, total training cost is 2863.9467458724976, time spent is 1.2012298107147217\n",
      "Finish epoch 80, total training cost is 2834.5398149490356, time spent is 1.2729623317718506\n",
      "Training kappa score = 0.7843298641271271\n",
      "Testing kappa score = 0.040568942351835946\n",
      "Finish epoch 81, total training cost is 2795.5254039764404, time spent is 1.2850377559661865\n",
      "Finish epoch 82, total training cost is 2756.143943786621, time spent is 1.3104937076568604\n",
      "Finish epoch 83, total training cost is 2702.9038009643555, time spent is 1.2941274642944336\n",
      "Finish epoch 84, total training cost is 2613.2445096969604, time spent is 1.3638231754302979\n",
      "Finish epoch 85, total training cost is 2554.494290828705, time spent is 1.3441033363342285\n",
      "Training kappa score = 0.791338830887174\n",
      "Testing kappa score = 0.07907080634353358\n",
      "                                               names  essay_id\n",
      "0  2, \"In this memoir of Narciso Rodriguez, @PERS...         1\n",
      "1  1, The mood created by the author is showing h...         2\n",
      "2  3, \"The mood created by the author in the memo...         3\n",
      "3  4, \"In the memoir \"\"@PERSON1 Rodriguez\"\" from ...         4\n",
      "4  2, \"The author's interpretation of their child...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 271 \n",
      "mean sentence size: 98\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 452 \n",
      "mean sentence size: 129\n",
      "\n",
      "361\n",
      "The size of training data: 1439\n",
      "The size of testing data: 361\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n",
      "Finish epoch 1, total training cost is 27867.111358642578, time spent is 2.003552198410034\n",
      "Finish epoch 2, total training cost is 13798.983699798584, time spent is 1.2087640762329102\n",
      "Finish epoch 3, total training cost is 11182.779041290283, time spent is 1.183793067932129\n",
      "Finish epoch 4, total training cost is 9095.159057617188, time spent is 1.1497159004211426\n",
      "Finish epoch 5, total training cost is 7929.075519561768, time spent is 1.1753745079040527\n",
      "Training kappa score = 0.5404519308535969\n",
      "Testing kappa score = -0.0007169756405751571\n",
      "Finish epoch 6, total training cost is 6715.005075454712, time spent is 1.1171705722808838\n",
      "Finish epoch 7, total training cost is 6057.793648719788, time spent is 1.180379867553711\n",
      "Finish epoch 8, total training cost is 5308.623615264893, time spent is 1.184410572052002\n",
      "Finish epoch 9, total training cost is 4925.090408325195, time spent is 1.1499812602996826\n",
      "Finish epoch 10, total training cost is 4474.462404251099, time spent is 1.1383240222930908\n",
      "Training kappa score = 0.6172602992643155\n",
      "Testing kappa score = 0.06479739727498068\n",
      "Finish epoch 11, total training cost is 4184.167499065399, time spent is 1.1578028202056885\n",
      "Finish epoch 12, total training cost is 4105.866209030151, time spent is 1.2020959854125977\n",
      "Finish epoch 13, total training cost is 3852.60249042511, time spent is 1.1397902965545654\n",
      "Finish epoch 14, total training cost is 3831.050395965576, time spent is 1.1659080982208252\n",
      "Finish epoch 15, total training cost is 3636.7446155548096, time spent is 1.126239538192749\n",
      "Training kappa score = 0.7522384178478176\n",
      "Testing kappa score = 0.05928701390816282\n",
      "Finish epoch 16, total training cost is 3711.696011543274, time spent is 1.1690723896026611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 17, total training cost is 3460.8056983947754, time spent is 1.1590652465820312\n",
      "Finish epoch 18, total training cost is 3649.5960187911987, time spent is 1.1729764938354492\n",
      "Finish epoch 19, total training cost is 3106.1287207603455, time spent is 1.1293153762817383\n",
      "Finish epoch 20, total training cost is 2769.5387427806854, time spent is 1.1634559631347656\n",
      "Training kappa score = 0.8105224025735345\n",
      "Testing kappa score = 0.00509279149943942\n",
      "Finish epoch 21, total training cost is 2780.8136320114136, time spent is 1.1621601581573486\n",
      "Finish epoch 22, total training cost is 2776.3474202156067, time spent is 1.1633827686309814\n",
      "Finish epoch 23, total training cost is 2839.6686124801636, time spent is 1.1463675498962402\n",
      "Finish epoch 24, total training cost is 2563.244955062866, time spent is 1.1388189792633057\n",
      "Finish epoch 25, total training cost is 2565.3184604644775, time spent is 1.2020835876464844\n",
      "Training kappa score = 0.753160362605503\n",
      "Testing kappa score = -0.013089793262834881\n",
      "Finish epoch 26, total training cost is 2569.8956060409546, time spent is 1.2043914794921875\n",
      "Finish epoch 27, total training cost is 2397.3418073654175, time spent is 1.1544313430786133\n",
      "Finish epoch 28, total training cost is 2402.3553223609924, time spent is 1.1444313526153564\n",
      "Finish epoch 29, total training cost is 2425.4434657096863, time spent is 1.1799702644348145\n",
      "Finish epoch 30, total training cost is 2332.2882192134857, time spent is 1.1706528663635254\n",
      "Training kappa score = 0.8393572841815199\n",
      "Testing kappa score = 0.038451842027380945\n",
      "Finish epoch 31, total training cost is 2218.3899250030518, time spent is 1.1825037002563477\n",
      "Finish epoch 32, total training cost is 2189.660575389862, time spent is 1.1651580333709717\n",
      "Finish epoch 33, total training cost is 2286.326936006546, time spent is 1.1671578884124756\n",
      "Finish epoch 34, total training cost is 2291.56236076355, time spent is 1.1617796421051025\n",
      "Finish epoch 35, total training cost is 2361.8390188217163, time spent is 1.1621613502502441\n",
      "Training kappa score = 0.8234682823988355\n",
      "Testing kappa score = -0.013904521810144566\n",
      "Finish epoch 36, total training cost is 2246.2495408058167, time spent is 1.1871051788330078\n",
      "Finish epoch 37, total training cost is 2324.485383272171, time spent is 1.166762113571167\n",
      "Finish epoch 38, total training cost is 2031.026780128479, time spent is 1.1702594757080078\n",
      "Finish epoch 39, total training cost is 2257.37549495697, time spent is 1.1595277786254883\n",
      "Finish epoch 40, total training cost is 2087.8195610046387, time spent is 1.1587927341461182\n",
      "Training kappa score = 0.8265384317338408\n",
      "Testing kappa score = 0.0135650730052157\n",
      "Finish epoch 41, total training cost is 1975.5741753578186, time spent is 1.1875133514404297\n",
      "Finish epoch 42, total training cost is 1968.5557279586792, time spent is 1.164947271347046\n",
      "Finish epoch 43, total training cost is 2056.1396203041077, time spent is 1.1802787780761719\n",
      "Finish epoch 44, total training cost is 1999.0705366134644, time spent is 1.1434788703918457\n",
      "Finish epoch 45, total training cost is 2074.3151121139526, time spent is 1.1665210723876953\n",
      "Training kappa score = 0.8485470182673958\n",
      "Testing kappa score = 0.012332401233240309\n",
      "Finish epoch 46, total training cost is 1869.1566212177277, time spent is 1.1547472476959229\n",
      "Finish epoch 47, total training cost is 1959.958212852478, time spent is 1.1580612659454346\n",
      "Finish epoch 48, total training cost is 1827.2369039058685, time spent is 1.1586520671844482\n",
      "Finish epoch 49, total training cost is 1879.358775138855, time spent is 1.164536952972412\n",
      "Finish epoch 50, total training cost is 1631.0246803760529, time spent is 1.1446843147277832\n",
      "Training kappa score = 0.8101860626549542\n",
      "Testing kappa score = -0.024562522421971655\n",
      "Finish epoch 51, total training cost is 1767.5670647621155, time spent is 1.159132957458496\n",
      "Finish epoch 52, total training cost is 1920.9305400848389, time spent is 1.162132978439331\n",
      "Finish epoch 53, total training cost is 1741.0430681705475, time spent is 1.138700008392334\n",
      "Finish epoch 54, total training cost is 1776.961998462677, time spent is 1.1421771049499512\n",
      "Finish epoch 55, total training cost is 1734.4955842494965, time spent is 1.1320557594299316\n",
      "Training kappa score = 0.8505369866428467\n",
      "Testing kappa score = -0.010454164768803453\n",
      "Finish epoch 56, total training cost is 1632.620313167572, time spent is 1.1511025428771973\n",
      "Finish epoch 57, total training cost is 1699.7036628723145, time spent is 1.1462736129760742\n",
      "Finish epoch 58, total training cost is 1580.3911361694336, time spent is 1.1761760711669922\n",
      "Finish epoch 59, total training cost is 1609.875837802887, time spent is 1.149935245513916\n",
      "Finish epoch 60, total training cost is 1594.4359464645386, time spent is 1.1805708408355713\n",
      "Training kappa score = 0.8430325599634372\n",
      "Testing kappa score = -0.0010204977854362252\n",
      "Finish epoch 61, total training cost is 1620.0863394737244, time spent is 1.1608946323394775\n",
      "Finish epoch 62, total training cost is 1552.9146831035614, time spent is 1.1549580097198486\n",
      "Finish epoch 63, total training cost is 1505.6968700885773, time spent is 1.1310460567474365\n",
      "Finish epoch 64, total training cost is 1518.604423046112, time spent is 1.153799057006836\n",
      "Finish epoch 65, total training cost is 1584.9683437347412, time spent is 1.1897187232971191\n",
      "Training kappa score = 0.8023846357761408\n",
      "Testing kappa score = 0.01842123994590361\n",
      "                                               names  essay_id\n",
      "0  2, \"In this memoir of Narciso Rodriguez, @PERS...         1\n",
      "1  1, The mood created by the author is showing h...         2\n",
      "2  3, \"The mood created by the author in the memo...         3\n",
      "3  4, \"In the memoir \"\"@PERSON1 Rodriguez\"\" from ...         4\n",
      "4  2, \"The author's interpretation of their child...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 369 \n",
      "mean sentence size: 129\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 452 \n",
      "mean sentence size: 129\n",
      "\n",
      "361\n",
      "The size of training data: 1439\n",
      "The size of testing data: 361\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n",
      "Finish epoch 1, total training cost is 46579.64464569092, time spent is 1.540386438369751\n",
      "Finish epoch 2, total training cost is 18151.344215393066, time spent is 1.0024001598358154\n",
      "Finish epoch 3, total training cost is 12964.834049224854, time spent is 1.0243823528289795\n",
      "Finish epoch 4, total training cost is 11474.808002471924, time spent is 1.006988286972046\n",
      "Finish epoch 5, total training cost is 10600.44920539856, time spent is 1.025597333908081\n",
      "Training kappa score = 0.7316906870495143\n",
      "Testing kappa score = -0.06490596723126707\n",
      "Finish epoch 6, total training cost is 9155.44140625, time spent is 1.032546043395996\n",
      "Finish epoch 7, total training cost is 8459.785339355469, time spent is 1.0345268249511719\n",
      "Finish epoch 8, total training cost is 8739.738353729248, time spent is 1.0373563766479492\n",
      "Finish epoch 9, total training cost is 8097.878940582275, time spent is 1.0290868282318115\n",
      "Finish epoch 10, total training cost is 7339.7528076171875, time spent is 1.027909278869629\n",
      "Training kappa score = 0.7165887181695715\n",
      "Testing kappa score = -0.04455122246533261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 11, total training cost is 7103.888006210327, time spent is 1.0278429985046387\n",
      "Finish epoch 12, total training cost is 6744.615863800049, time spent is 1.0239388942718506\n",
      "Finish epoch 13, total training cost is 6394.612607955933, time spent is 1.0308411121368408\n",
      "Finish epoch 14, total training cost is 6337.051078796387, time spent is 1.025437593460083\n",
      "Finish epoch 15, total training cost is 6056.854274749756, time spent is 1.026761770248413\n",
      "Training kappa score = 0.6943610768816066\n",
      "Testing kappa score = -0.03821729815283059\n",
      "Finish epoch 16, total training cost is 5884.905466079712, time spent is 1.034196376800537\n",
      "Finish epoch 17, total training cost is 5347.359408378601, time spent is 1.0250475406646729\n",
      "Finish epoch 18, total training cost is 5815.326434135437, time spent is 1.0386936664581299\n",
      "Finish epoch 19, total training cost is 5414.258594512939, time spent is 1.031217336654663\n",
      "Finish epoch 20, total training cost is 5242.938688278198, time spent is 1.028792381286621\n",
      "Training kappa score = 0.7238746751957204\n",
      "Testing kappa score = -0.025454914936760442\n",
      "Finish epoch 21, total training cost is 5055.110651016235, time spent is 1.0458006858825684\n",
      "Finish epoch 22, total training cost is 4783.923426628113, time spent is 1.0336999893188477\n",
      "Finish epoch 23, total training cost is 4526.091247558594, time spent is 1.0221226215362549\n",
      "Finish epoch 24, total training cost is 4501.556131362915, time spent is 1.0446984767913818\n",
      "Finish epoch 25, total training cost is 4394.351489067078, time spent is 1.0424633026123047\n",
      "Training kappa score = 0.7235923935085358\n",
      "Testing kappa score = 0.01048436896093674\n",
      "Finish epoch 26, total training cost is 4357.538123130798, time spent is 1.052811622619629\n",
      "Finish epoch 27, total training cost is 4298.4981117248535, time spent is 1.06846022605896\n",
      "Finish epoch 28, total training cost is 4257.451812744141, time spent is 1.0762381553649902\n",
      "Finish epoch 29, total training cost is 4304.548271179199, time spent is 1.0684757232666016\n",
      "Finish epoch 30, total training cost is 4057.8070402145386, time spent is 1.0648374557495117\n",
      "Training kappa score = 0.7730021189538095\n",
      "Testing kappa score = -0.055280514616073306\n",
      "Finish epoch 31, total training cost is 3700.3772525787354, time spent is 1.0711963176727295\n",
      "Finish epoch 32, total training cost is 4053.691752433777, time spent is 1.0606002807617188\n",
      "Finish epoch 33, total training cost is 3976.3184146881104, time spent is 1.073979139328003\n",
      "Finish epoch 34, total training cost is 3993.055263519287, time spent is 1.0746486186981201\n",
      "Finish epoch 35, total training cost is 3887.4738998413086, time spent is 1.0634710788726807\n",
      "Training kappa score = 0.7778377080401226\n",
      "Testing kappa score = -0.028640556894635516\n",
      "Finish epoch 36, total training cost is 3582.9195728302, time spent is 1.0739374160766602\n",
      "Finish epoch 37, total training cost is 3819.362090110779, time spent is 1.0775160789489746\n",
      "Finish epoch 38, total training cost is 3488.757730484009, time spent is 1.0732166767120361\n",
      "Finish epoch 39, total training cost is 3477.6584134101868, time spent is 1.0793356895446777\n",
      "Finish epoch 40, total training cost is 3695.587194442749, time spent is 1.0669615268707275\n",
      "Training kappa score = 0.7691305105170567\n",
      "Testing kappa score = -0.004411880564090476\n",
      "Finish epoch 41, total training cost is 3564.0855407714844, time spent is 1.073638677597046\n",
      "Finish epoch 42, total training cost is 3555.627431869507, time spent is 1.029923915863037\n",
      "Finish epoch 43, total training cost is 3317.4977588653564, time spent is 1.0387327671051025\n",
      "Finish epoch 44, total training cost is 3438.2118978500366, time spent is 1.0437414646148682\n",
      "Finish epoch 45, total training cost is 3609.0471801757812, time spent is 1.0118014812469482\n",
      "Training kappa score = 0.7960502417812232\n",
      "Testing kappa score = -0.015057788670566952\n",
      "Finish epoch 46, total training cost is 3549.1075773239136, time spent is 1.0206639766693115\n",
      "Finish epoch 47, total training cost is 3102.4075422286987, time spent is 1.0159146785736084\n",
      "Finish epoch 48, total training cost is 3078.9074125289917, time spent is 1.0211350917816162\n",
      "Finish epoch 49, total training cost is 3191.666624069214, time spent is 1.0185902118682861\n",
      "Finish epoch 50, total training cost is 3017.9181804656982, time spent is 1.014455795288086\n",
      "Training kappa score = 0.789317748869345\n",
      "Testing kappa score = -0.03831471054787605\n",
      "Finish epoch 51, total training cost is 2935.1748547554016, time spent is 1.0331306457519531\n",
      "Finish epoch 52, total training cost is 3024.8896293640137, time spent is 1.0170109272003174\n",
      "Finish epoch 53, total training cost is 3007.0126934051514, time spent is 1.0182411670684814\n",
      "Finish epoch 54, total training cost is 2985.8190774917603, time spent is 1.0244297981262207\n",
      "Finish epoch 55, total training cost is 2967.162157058716, time spent is 1.0322582721710205\n",
      "Training kappa score = 0.7932561952907674\n",
      "Testing kappa score = -0.014958455717523966\n",
      "Finish epoch 56, total training cost is 2842.394266605377, time spent is 1.0325124263763428\n",
      "Finish epoch 57, total training cost is 3077.479440689087, time spent is 1.03102707862854\n",
      "Finish epoch 58, total training cost is 2737.407627105713, time spent is 1.0261390209197998\n",
      "Finish epoch 59, total training cost is 2730.6012268066406, time spent is 1.0297784805297852\n",
      "Finish epoch 60, total training cost is 2706.3322191238403, time spent is 1.0308027267456055\n",
      "Training kappa score = 0.8026028747710737\n",
      "Testing kappa score = -0.06064621259505554\n",
      "Finish epoch 61, total training cost is 2641.1122512817383, time spent is 1.0232410430908203\n",
      "Finish epoch 62, total training cost is 2627.5008778572083, time spent is 1.0124692916870117\n",
      "Finish epoch 63, total training cost is 2619.0908336639404, time spent is 1.023364782333374\n",
      "Finish epoch 64, total training cost is 2615.385640144348, time spent is 1.0202345848083496\n",
      "Finish epoch 65, total training cost is 2487.7758378982544, time spent is 1.024298906326294\n",
      "Training kappa score = 0.7430324028476066\n",
      "Testing kappa score = -0.04232092710516966\n",
      "Finish epoch 66, total training cost is 2632.8657569885254, time spent is 1.0192394256591797\n",
      "Finish epoch 67, total training cost is 2457.6674394607544, time spent is 1.0194575786590576\n",
      "Finish epoch 68, total training cost is 2332.3626527786255, time spent is 1.015303134918213\n",
      "Finish epoch 69, total training cost is 2479.1157014369965, time spent is 1.0198426246643066\n",
      "Finish epoch 70, total training cost is 2449.0509901046753, time spent is 1.007120132446289\n",
      "Training kappa score = 0.7882617531591806\n",
      "Testing kappa score = -0.06033835014458444\n",
      "Finish epoch 71, total training cost is 2494.6205592155457, time spent is 1.0227086544036865\n",
      "Finish epoch 72, total training cost is 2340.3281128406525, time spent is 1.0251145362854004\n",
      "Finish epoch 73, total training cost is 2607.2303409576416, time spent is 1.0199873447418213\n",
      "Finish epoch 74, total training cost is 2400.1817650794983, time spent is 1.0166947841644287\n",
      "Finish epoch 75, total training cost is 2647.8929052352905, time spent is 1.0223286151885986\n",
      "Training kappa score = 0.7689250390394248\n",
      "Testing kappa score = 0.009312444887632498\n",
      "Finish epoch 76, total training cost is 2343.585120201111, time spent is 1.0342400074005127\n",
      "Finish epoch 77, total training cost is 2296.8870668411255, time spent is 1.0299468040466309\n",
      "Finish epoch 78, total training cost is 2377.410177230835, time spent is 1.026200294494629\n",
      "Finish epoch 79, total training cost is 2258.589614868164, time spent is 1.035154104232788\n",
      "Finish epoch 80, total training cost is 2392.433750629425, time spent is 1.0245275497436523\n",
      "Training kappa score = 0.8097547417840376\n",
      "Testing kappa score = -0.024331729891006626\n",
      "                                               names  essay_id\n",
      "0  2, \"In this memoir of Narciso Rodriguez, @PERS...         1\n",
      "1  1, The mood created by the author is showing h...         2\n",
      "2  3, \"The mood created by the author in the memo...         3\n",
      "3  4, \"In the memoir \"\"@PERSON1 Rodriguez\"\" from ...         4\n",
      "4  2, \"The author's interpretation of their child...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 355 \n",
      "mean sentence size: 127\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 452 \n",
      "mean sentence size: 129\n",
      "\n",
      "361\n",
      "The size of training data: 1439\n",
      "The size of testing data: 361\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 1, total training cost is 173472.69305419922, time spent is 1.415177583694458\n",
      "Finish epoch 2, total training cost is 37981.51354217529, time spent is 1.1124610900878906\n",
      "Finish epoch 3, total training cost is 21728.71327972412, time spent is 1.2111601829528809\n",
      "Finish epoch 4, total training cost is 17966.831520080566, time spent is 1.248978614807129\n",
      "Finish epoch 5, total training cost is 15990.292465209961, time spent is 1.2252047061920166\n",
      "Training kappa score = 0.6728224394698445\n",
      "Testing kappa score = -0.001458694208202438\n",
      "Finish epoch 6, total training cost is 15446.243965148926, time spent is 1.2411582469940186\n",
      "Finish epoch 7, total training cost is 13065.088693618774, time spent is 1.24190354347229\n",
      "Finish epoch 8, total training cost is 12806.755908966064, time spent is 1.235290288925171\n",
      "Finish epoch 9, total training cost is 12158.926773071289, time spent is 1.2362499237060547\n",
      "Finish epoch 10, total training cost is 11900.534423828125, time spent is 1.2983496189117432\n",
      "Training kappa score = 0.6705197609201469\n",
      "Testing kappa score = 0.018543937797221077\n",
      "Finish epoch 11, total training cost is 9717.520492553711, time spent is 1.2309000492095947\n",
      "Finish epoch 12, total training cost is 10187.91555404663, time spent is 1.1297268867492676\n",
      "Finish epoch 13, total training cost is 9282.733470916748, time spent is 1.1522645950317383\n",
      "Finish epoch 14, total training cost is 10420.608394622803, time spent is 1.1661112308502197\n",
      "Finish epoch 15, total training cost is 9953.301164627075, time spent is 1.1768388748168945\n",
      "Training kappa score = 0.7184341606420614\n",
      "Testing kappa score = 0.013604233217965445\n",
      "Finish epoch 16, total training cost is 9780.487915039062, time spent is 1.104659080505371\n",
      "Finish epoch 17, total training cost is 9135.724605560303, time spent is 1.1443963050842285\n",
      "Finish epoch 18, total training cost is 7983.014772415161, time spent is 1.0016722679138184\n",
      "Finish epoch 19, total training cost is 8231.557796478271, time spent is 0.9893531799316406\n",
      "Finish epoch 20, total training cost is 8237.858543395996, time spent is 0.994469165802002\n",
      "Training kappa score = 0.7446379360104166\n",
      "Testing kappa score = 0.03439996918639243\n",
      "Finish epoch 21, total training cost is 7142.117521286011, time spent is 0.9681422710418701\n",
      "Finish epoch 22, total training cost is 7156.082847595215, time spent is 1.0335655212402344\n",
      "Finish epoch 23, total training cost is 7422.1654052734375, time spent is 0.994518518447876\n",
      "Finish epoch 24, total training cost is 6338.717477798462, time spent is 0.9898126125335693\n",
      "Finish epoch 25, total training cost is 6487.010854721069, time spent is 0.990410327911377\n",
      "Training kappa score = 0.7538116361904779\n",
      "Testing kappa score = 0.05379142807565496\n",
      "Finish epoch 26, total training cost is 6131.634415626526, time spent is 1.008652687072754\n",
      "Finish epoch 27, total training cost is 6604.832998275757, time spent is 1.0013196468353271\n",
      "Finish epoch 28, total training cost is 5933.02737903595, time spent is 0.9937167167663574\n",
      "Finish epoch 29, total training cost is 5934.031542778015, time spent is 0.9838430881500244\n",
      "Finish epoch 30, total training cost is 5361.9421043396, time spent is 0.9862756729125977\n",
      "Training kappa score = 0.5963516470713386\n",
      "Testing kappa score = 0.045210941306314756\n",
      "Finish epoch 31, total training cost is 6143.249542236328, time spent is 0.9888720512390137\n",
      "Finish epoch 32, total training cost is 4877.098877906799, time spent is 0.9735338687896729\n",
      "Finish epoch 33, total training cost is 4419.850544929504, time spent is 0.983212947845459\n",
      "Finish epoch 34, total training cost is 4811.3971824646, time spent is 0.9812664985656738\n",
      "Finish epoch 35, total training cost is 4709.9063177108765, time spent is 0.9921886920928955\n",
      "Training kappa score = 0.7422884100554257\n",
      "Testing kappa score = 0.02411385927309584\n",
      "Finish epoch 36, total training cost is 4710.778455734253, time spent is 0.983635663986206\n",
      "Finish epoch 37, total training cost is 4603.527139663696, time spent is 0.9928293228149414\n",
      "Finish epoch 38, total training cost is 4443.986970901489, time spent is 0.9893648624420166\n",
      "Finish epoch 39, total training cost is 4156.068048477173, time spent is 0.9922585487365723\n",
      "Finish epoch 40, total training cost is 4321.0699281692505, time spent is 0.9839668273925781\n",
      "Training kappa score = 0.7090564896180853\n",
      "Testing kappa score = 0.04278443637211271\n",
      "Finish epoch 41, total training cost is 3859.2294130325317, time spent is 1.0185794830322266\n",
      "Finish epoch 42, total training cost is 3572.3137879371643, time spent is 0.9940109252929688\n",
      "Finish epoch 43, total training cost is 3233.197958946228, time spent is 0.9848132133483887\n",
      "Finish epoch 44, total training cost is 3409.2583589553833, time spent is 0.9944655895233154\n",
      "Finish epoch 45, total training cost is 3116.8846967220306, time spent is 0.9826991558074951\n",
      "Training kappa score = 0.703774109283827\n",
      "Testing kappa score = 0.006231343464104477\n",
      "Finish epoch 46, total training cost is 2956.9908213615417, time spent is 1.0244941711425781\n",
      "Finish epoch 47, total training cost is 3157.207601547241, time spent is 0.9896707534790039\n",
      "Finish epoch 48, total training cost is 3365.314489364624, time spent is 1.0549142360687256\n",
      "Finish epoch 49, total training cost is 3068.6680116653442, time spent is 0.9831564426422119\n",
      "Finish epoch 50, total training cost is 2749.813109397888, time spent is 0.9784717559814453\n",
      "Training kappa score = 0.784539444953247\n",
      "Testing kappa score = 0.04418167762196046\n",
      "Finish epoch 51, total training cost is 2818.79749917984, time spent is 0.979759693145752\n",
      "Finish epoch 52, total training cost is 2832.757508277893, time spent is 1.1239535808563232\n",
      "Finish epoch 53, total training cost is 2704.7129526138306, time spent is 1.3620240688323975\n",
      "Finish epoch 54, total training cost is 2647.954439163208, time spent is 1.3747484683990479\n",
      "Finish epoch 55, total training cost is 2623.0092544555664, time spent is 1.36818528175354\n",
      "Training kappa score = 0.7908511819552183\n",
      "Testing kappa score = 0.04311016902726095\n",
      "Finish epoch 56, total training cost is 2697.28102684021, time spent is 1.3282592296600342\n",
      "Finish epoch 57, total training cost is 2721.2708644866943, time spent is 1.3979082107543945\n",
      "Finish epoch 58, total training cost is 2586.2773008346558, time spent is 1.3820130825042725\n",
      "Finish epoch 59, total training cost is 2734.4374585151672, time spent is 1.324234962463379\n",
      "Finish epoch 60, total training cost is 2581.9526748657227, time spent is 1.3330206871032715\n",
      "Training kappa score = 0.7544934768493341\n",
      "Testing kappa score = 0.04707262581969718\n",
      "Finish epoch 61, total training cost is 2486.173007965088, time spent is 1.3794691562652588\n",
      "Finish epoch 62, total training cost is 2711.628873348236, time spent is 1.3116085529327393\n",
      "Finish epoch 63, total training cost is 2578.165192604065, time spent is 1.3024580478668213\n",
      "Finish epoch 64, total training cost is 2390.772482395172, time spent is 1.364030361175537\n",
      "Finish epoch 65, total training cost is 2309.0065631866455, time spent is 1.37955641746521\n",
      "Training kappa score = 0.788405428357119\n",
      "Testing kappa score = 0.03876246660826255\n",
      "Finish epoch 66, total training cost is 2429.3101649284363, time spent is 1.3394875526428223\n",
      "Finish epoch 67, total training cost is 2478.369198322296, time spent is 1.301659345626831\n",
      "Finish epoch 68, total training cost is 2303.626437187195, time spent is 1.3478848934173584\n",
      "Finish epoch 69, total training cost is 2538.347255706787, time spent is 1.3158533573150635\n",
      "Finish epoch 70, total training cost is 2337.991885662079, time spent is 1.3548145294189453\n",
      "Training kappa score = 0.7481454665693696\n",
      "Testing kappa score = 0.0223206679161696\n",
      "Finish epoch 71, total training cost is 2337.7324619293213, time spent is 1.375411033630371\n",
      "Finish epoch 72, total training cost is 2423.005877017975, time spent is 1.380091667175293\n",
      "Finish epoch 73, total training cost is 2323.3778829574585, time spent is 1.3387620449066162\n",
      "Finish epoch 74, total training cost is 2574.1666011810303, time spent is 1.3158152103424072\n",
      "Finish epoch 75, total training cost is 2339.451627254486, time spent is 1.3063101768493652\n",
      "Training kappa score = 0.7740641771663237\n",
      "Testing kappa score = 0.058868051467496296\n",
      "Finish epoch 76, total training cost is 2444.6511216163635, time spent is 1.4451298713684082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 77, total training cost is 2364.9556846618652, time spent is 1.3646318912506104\n",
      "Finish epoch 78, total training cost is 2222.6861991882324, time spent is 1.4344751834869385\n",
      "Finish epoch 79, total training cost is 2134.178134918213, time spent is 1.405188798904419\n",
      "Finish epoch 80, total training cost is 2247.467975616455, time spent is 1.3949832916259766\n",
      "Training kappa score = 0.783613105444707\n",
      "Testing kappa score = 0.05920372168650223\n",
      "Finish epoch 81, total training cost is 2162.817478656769, time spent is 1.3345553874969482\n",
      "Finish epoch 82, total training cost is 2125.5891585350037, time spent is 1.3557345867156982\n",
      "Finish epoch 83, total training cost is 2179.651045322418, time spent is 1.3392808437347412\n",
      "Finish epoch 84, total training cost is 1971.4283757209778, time spent is 1.3764870166778564\n",
      "Finish epoch 85, total training cost is 2044.6514706611633, time spent is 1.3982179164886475\n",
      "Training kappa score = 0.7919603521213532\n",
      "Testing kappa score = 0.05295208294616216\n",
      "Finish epoch 86, total training cost is 2036.6033825874329, time spent is 1.3718516826629639\n",
      "Finish epoch 87, total training cost is 2052.6926774978638, time spent is 1.3294789791107178\n",
      "Finish epoch 88, total training cost is 1995.5870652198792, time spent is 1.3903589248657227\n",
      "Finish epoch 89, total training cost is 1917.60777425766, time spent is 1.3251066207885742\n",
      "Finish epoch 90, total training cost is 2128.9365043640137, time spent is 1.3150391578674316\n",
      "Training kappa score = 0.8011282702747524\n",
      "Testing kappa score = 0.06160805137282077\n",
      "Finish epoch 91, total training cost is 1939.0519592761993, time spent is 1.40427565574646\n",
      "Finish epoch 92, total training cost is 2096.96186876297, time spent is 1.3811063766479492\n",
      "Finish epoch 93, total training cost is 2222.790313243866, time spent is 1.357877254486084\n",
      "Finish epoch 94, total training cost is 2356.415791988373, time spent is 1.4204649925231934\n",
      "Finish epoch 95, total training cost is 2148.924921989441, time spent is 1.3169972896575928\n",
      "Training kappa score = 0.7044935933049908\n",
      "Testing kappa score = 0.006373546477317249\n",
      "Finish epoch 96, total training cost is 2346.3390226364136, time spent is 1.369262933731079\n",
      "Finish epoch 97, total training cost is 2179.7692694664, time spent is 1.4202799797058105\n",
      "Finish epoch 98, total training cost is 2047.9484596252441, time spent is 1.416430950164795\n",
      "Finish epoch 99, total training cost is 2069.2051181793213, time spent is 1.383516550064087\n",
      "Finish epoch 100, total training cost is 1938.733127117157, time spent is 1.339811086654663\n",
      "Training kappa score = 0.7765510014224382\n",
      "Testing kappa score = 0.032044373176309704\n",
      "Finish epoch 101, total training cost is 1972.2840023040771, time spent is 1.301060438156128\n",
      "Finish epoch 102, total training cost is 1862.2959938049316, time spent is 1.3909399509429932\n",
      "Finish epoch 103, total training cost is 1792.170316696167, time spent is 1.3569376468658447\n",
      "Finish epoch 104, total training cost is 1756.706346988678, time spent is 1.388559103012085\n",
      "Finish epoch 105, total training cost is 1788.252848148346, time spent is 1.4408209323883057\n",
      "Training kappa score = 0.7776763931644564\n",
      "Testing kappa score = 0.04517457557314808\n",
      "Finish epoch 106, total training cost is 1662.714376449585, time spent is 1.4457342624664307\n",
      "Finish epoch 107, total training cost is 1781.5546407699585, time spent is 1.3716883659362793\n",
      "Finish epoch 108, total training cost is 1700.3864135742188, time spent is 1.4253599643707275\n",
      "Finish epoch 109, total training cost is 1731.5278985500336, time spent is 1.3769686222076416\n",
      "Finish epoch 110, total training cost is 1620.0246334075928, time spent is 1.365264892578125\n",
      "Training kappa score = 0.7524841149060805\n",
      "Testing kappa score = 0.0453755890611397\n",
      "Finish epoch 111, total training cost is 1681.5847854614258, time spent is 1.3490126132965088\n",
      "Finish epoch 112, total training cost is 1673.0130424499512, time spent is 1.42592191696167\n",
      "Finish epoch 113, total training cost is 1693.9922194480896, time spent is 1.4167132377624512\n",
      "Finish epoch 114, total training cost is 1737.017451286316, time spent is 1.4050605297088623\n",
      "Finish epoch 115, total training cost is 1664.996033668518, time spent is 1.450000286102295\n",
      "Training kappa score = 0.736834344251176\n",
      "Testing kappa score = 0.00806008426451732\n",
      "Finish epoch 116, total training cost is 2046.9631662368774, time spent is 1.3912756443023682\n",
      "Finish epoch 117, total training cost is 1753.0958642959595, time spent is 1.4057121276855469\n",
      "Finish epoch 118, total training cost is 1542.1617069244385, time spent is 1.4144542217254639\n",
      "Finish epoch 119, total training cost is 1622.5293624401093, time spent is 1.4109950065612793\n",
      "Finish epoch 120, total training cost is 1587.3436160087585, time spent is 1.40730619430542\n",
      "Training kappa score = 0.7393013849845754\n",
      "Testing kappa score = 0.019051357145013825\n",
      "Finish epoch 121, total training cost is 1684.4407796859741, time spent is 1.3697566986083984\n",
      "Finish epoch 122, total training cost is 1557.2035994529724, time spent is 1.444537878036499\n",
      "Finish epoch 123, total training cost is 1597.3911774158478, time spent is 1.3791770935058594\n",
      "Finish epoch 124, total training cost is 1596.8681817054749, time spent is 1.3741393089294434\n",
      "Finish epoch 125, total training cost is 1491.4207782745361, time spent is 1.4319028854370117\n",
      "Training kappa score = 0.7710439818342342\n",
      "Testing kappa score = 0.05849797865113582\n",
      "Finish epoch 126, total training cost is 1639.436625957489, time spent is 1.3595199584960938\n",
      "Finish epoch 127, total training cost is 1534.0606226921082, time spent is 1.426081657409668\n",
      "Finish epoch 128, total training cost is 1589.6377000808716, time spent is 1.3706493377685547\n",
      "Finish epoch 129, total training cost is 1579.9144513607025, time spent is 1.3827941417694092\n",
      "Finish epoch 130, total training cost is 1392.1017322540283, time spent is 1.3981623649597168\n",
      "Training kappa score = 0.7749701644726313\n",
      "Testing kappa score = 0.07006697578567722\n",
      "Finish epoch 131, total training cost is 1527.3311445713043, time spent is 1.3668303489685059\n",
      "Finish epoch 132, total training cost is 1592.4421854019165, time spent is 1.4260573387145996\n",
      "Finish epoch 133, total training cost is 1461.5152220726013, time spent is 1.4094810485839844\n",
      "Finish epoch 134, total training cost is 1841.1708874702454, time spent is 1.3493390083312988\n",
      "Finish epoch 135, total training cost is 1781.4697704315186, time spent is 1.37913179397583\n",
      "Training kappa score = 0.7466963553197146\n",
      "Testing kappa score = 0.03002004253568702\n",
      "Finish epoch 136, total training cost is 1737.4551882743835, time spent is 1.469228744506836\n",
      "Finish epoch 137, total training cost is 1487.3513684272766, time spent is 1.4358158111572266\n",
      "Finish epoch 138, total training cost is 1590.9426221847534, time spent is 1.4519720077514648\n",
      "Finish epoch 139, total training cost is 1627.315101146698, time spent is 1.4525530338287354\n",
      "Finish epoch 140, total training cost is 1681.1547012329102, time spent is 1.4262688159942627\n",
      "Training kappa score = 0.7681313183617369\n",
      "Testing kappa score = 0.07668153382480702\n",
      "Finish epoch 141, total training cost is 1524.84237241745, time spent is 1.4367403984069824\n",
      "Finish epoch 142, total training cost is 1423.2563695907593, time spent is 1.4211909770965576\n",
      "Finish epoch 143, total training cost is 1410.9033007621765, time spent is 1.4020814895629883\n",
      "Finish epoch 144, total training cost is 1388.8367991447449, time spent is 1.4215147495269775\n",
      "Finish epoch 145, total training cost is 1432.4178342819214, time spent is 1.4874682426452637\n",
      "Training kappa score = 0.7956958588829994\n",
      "Testing kappa score = 0.05560904714324644\n",
      "Finish epoch 146, total training cost is 1347.9130985736847, time spent is 1.4763946533203125\n",
      "Finish epoch 147, total training cost is 1432.5000433921814, time spent is 1.3965914249420166\n",
      "Finish epoch 148, total training cost is 1363.946662902832, time spent is 1.4702694416046143\n",
      "Finish epoch 149, total training cost is 1421.1814618110657, time spent is 1.3843977451324463\n",
      "Finish epoch 150, total training cost is 1372.2112407684326, time spent is 1.409243106842041\n",
      "Training kappa score = 0.7640550449133655\n",
      "Testing kappa score = 0.01026805203876724\n",
      "Finish epoch 151, total training cost is 1349.3947987556458, time spent is 1.4819018840789795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 152, total training cost is 1295.6140217781067, time spent is 1.3938302993774414\n",
      "Finish epoch 153, total training cost is 1324.0126214027405, time spent is 1.4780232906341553\n",
      "Finish epoch 154, total training cost is 1316.3609342575073, time spent is 1.3768489360809326\n",
      "Finish epoch 155, total training cost is 1282.31165933609, time spent is 1.459205150604248\n",
      "Training kappa score = 0.8051946009314511\n",
      "Testing kappa score = 0.03780290228733685\n",
      "Finish epoch 156, total training cost is 1333.825602531433, time spent is 1.4404609203338623\n",
      "Finish epoch 157, total training cost is 1317.5251786708832, time spent is 1.3605554103851318\n",
      "Finish epoch 158, total training cost is 1303.8814315795898, time spent is 1.468928337097168\n",
      "Finish epoch 159, total training cost is 1241.4993481636047, time spent is 1.3805360794067383\n",
      "Finish epoch 160, total training cost is 1246.6632685661316, time spent is 1.4815809726715088\n",
      "Training kappa score = 0.7992186409934421\n",
      "Testing kappa score = 0.04819985437837182\n",
      "Finish epoch 161, total training cost is 1220.4081983566284, time spent is 1.3927347660064697\n",
      "Finish epoch 162, total training cost is 1289.0976929664612, time spent is 1.3995106220245361\n",
      "Finish epoch 163, total training cost is 1266.5149750709534, time spent is 1.4057512283325195\n",
      "Finish epoch 164, total training cost is 1374.1889109611511, time spent is 1.4060916900634766\n",
      "Finish epoch 165, total training cost is 1429.9467949867249, time spent is 1.4129447937011719\n",
      "Training kappa score = 0.808563353197499\n",
      "Testing kappa score = 0.05476494091643269\n",
      "Finish epoch 166, total training cost is 2150.135354042053, time spent is 1.4325006008148193\n",
      "Finish epoch 167, total training cost is 2356.9164180755615, time spent is 1.438080072402954\n",
      "Finish epoch 168, total training cost is 2440.3068742752075, time spent is 1.4814612865447998\n",
      "Finish epoch 169, total training cost is 3794.9331731796265, time spent is 1.394223690032959\n",
      "Finish epoch 170, total training cost is 5664.358980178833, time spent is 1.4199039936065674\n",
      "Training kappa score = 0.5817194441064006\n",
      "Testing kappa score = -0.021613781881212768\n",
      "Finish epoch 171, total training cost is 5425.243646621704, time spent is 1.3981609344482422\n",
      "Finish epoch 172, total training cost is 4779.078019142151, time spent is 1.4348056316375732\n",
      "Finish epoch 173, total training cost is 4499.4287757873535, time spent is 1.3491737842559814\n",
      "Finish epoch 174, total training cost is 4062.0197229385376, time spent is 1.3991761207580566\n",
      "Finish epoch 175, total training cost is 4420.179350852966, time spent is 1.40120267868042\n",
      "Training kappa score = 0.7391899446234411\n",
      "Testing kappa score = 0.03614996926859282\n",
      "Finish epoch 176, total training cost is 4314.082834243774, time spent is 1.3805079460144043\n",
      "Finish epoch 177, total training cost is 4234.162741661072, time spent is 1.4158027172088623\n",
      "Finish epoch 178, total training cost is 3966.488042831421, time spent is 1.365891933441162\n",
      "Finish epoch 179, total training cost is 3468.8812227249146, time spent is 1.404984474182129\n",
      "Finish epoch 180, total training cost is 3796.453864097595, time spent is 1.3499391078948975\n",
      "Training kappa score = 0.7607700313609262\n",
      "Testing kappa score = 0.036050136520972664\n",
      "Finish epoch 181, total training cost is 3805.2518548965454, time spent is 1.414252519607544\n",
      "Finish epoch 182, total training cost is 3655.000346183777, time spent is 1.3291082382202148\n",
      "Finish epoch 183, total training cost is 3762.1077785491943, time spent is 1.3637635707855225\n",
      "Finish epoch 184, total training cost is 3619.4701042175293, time spent is 1.4093151092529297\n",
      "Finish epoch 185, total training cost is 3421.76740026474, time spent is 1.3771064281463623\n",
      "Training kappa score = 0.8067294700191754\n",
      "Testing kappa score = 0.02588584048748155\n",
      "Finish epoch 186, total training cost is 3602.0196437835693, time spent is 1.396178960800171\n",
      "Finish epoch 187, total training cost is 3383.213252067566, time spent is 1.3595969676971436\n",
      "Finish epoch 188, total training cost is 3492.065580368042, time spent is 1.3506319522857666\n",
      "Finish epoch 189, total training cost is 3376.4655723571777, time spent is 1.4407505989074707\n",
      "Finish epoch 190, total training cost is 3305.378108024597, time spent is 1.3874292373657227\n",
      "Training kappa score = 0.7757464816501549\n",
      "Testing kappa score = 0.02143994178378128\n",
      "Finish epoch 191, total training cost is 3630.622498035431, time spent is 1.387028455734253\n",
      "Finish epoch 192, total training cost is 3588.218684196472, time spent is 1.377079725265503\n",
      "Finish epoch 193, total training cost is 3804.432960510254, time spent is 1.4340240955352783\n",
      "Finish epoch 194, total training cost is 3607.7765440940857, time spent is 1.4560446739196777\n",
      "Finish epoch 195, total training cost is 3349.0741562843323, time spent is 1.3680250644683838\n",
      "Training kappa score = 0.7829668372812898\n",
      "Testing kappa score = 0.003359770130472839\n"
     ]
    }
   ],
   "source": [
    "import data_utils_adv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from qwk import quadratic_weighted_kappa\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "print( 'start to load flags\\n')\n",
    "\n",
    "# flags\n",
    "# tf.flags.DEFINE_float(\"epsilon\", 0.1, \"Epsilon value for Adam Optimizer.\")\n",
    "# tf.flags.DEFINE_float(\"l2_lambda\", 0.3, \"Lambda for l2 loss.\")\n",
    "# tf.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate\")\n",
    "# tf.flags.DEFINE_float(\"max_grad_norm\", 10.0, \"Clip gradients to this norm.\")\n",
    "# tf.flags.DEFINE_float(\"keep_prob\", 0.9, \"Keep probability for dropout\")\n",
    "# tf.flags.DEFINE_integer(\"evaluation_interval\", 5, \"Evaluate and print results every x epochs\")\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 15, \"Batch size for training.\")\n",
    "# tf.flags.DEFINE_integer(\"feature_size\", 100, \"Feature size\")\n",
    "# tf.flags.DEFINE_integer(\"num_samples\", 1, \"Number of samples selected from training for each score\")\n",
    "# tf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "# tf.flags.DEFINE_integer(\"epochs\", 200, \"Number of epochs to train for.\")\n",
    "# tf.flags.DEFINE_integer(\"embedding_size\", 300, \"Embedding size for embedding matrices.\")\n",
    "# tf.flags.DEFINE_integer(\"essay_set_id\", 1, \"essay set id, 1 <= id <= 8\")\n",
    "# tf.flags.DEFINE_integer(\"token_num\", 42, \"The number of token in glove (6, 42)\")\n",
    "# tf.flags.DEFINE_boolean(\"gated_addressing\", False, \"Simple gated addressing\")\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"is_regression\", False, \"The output is regression or classification\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "# # hyper-parameters\n",
    "# FLAGS = tf.flags.FLAGS\n",
    "\n",
    "early_stop_count = 0\n",
    "max_step_count = 10\n",
    "is_regression = False\n",
    "gated_addressing = False\n",
    "essay_set_id = 5\n",
    "batch_size = 15\n",
    "embedding_size = 300\n",
    "feature_size = 100\n",
    "l2_lambda = 0.3\n",
    "hops = 3\n",
    "reader = 'bow' # gru may not work\n",
    "epochs = 200\n",
    "num_samples = 1\n",
    "num_tokens = 42\n",
    "test_batch_size = batch_size\n",
    "random_state = 0\n",
    "\n",
    "if is_regression:\n",
    "    from memn2n_kv_regression import MemN2N_KV\n",
    "else:\n",
    "    from memn2n_kv import MemN2N_KV\n",
    "# print flags info\n",
    "orig_stdout = sys.stdout\n",
    "timestamp = time.strftime(\"%b_%d_%Y_%H:%M:%S\", time.localtime())\n",
    "folder_name = 'essay_set_{}_cv_{}_{}'.format(essay_set_id, num_samples, timestamp)\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs/adversary_training/\", folder_name))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# save output to a file\n",
    "#f = file(out_dir+'/out.txt', 'w')\n",
    "#sys.stdout = f\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "# print(\"\\nParameters:\")\n",
    "# for key in sorted(FLAGS.__flags.keys()):\n",
    "#     print(\"{}={}\".format(key, getattr(FLAGS, key)))\n",
    "# print(\"\")\n",
    "\n",
    "# with open(out_dir+'/params', 'w') as f:\n",
    "#     for key in sorted(FLAGS.__flags.keys()):\n",
    "#         f.write(\"{}={}\".format(key, getattr(FLAGS, key)))\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "        \n",
    "import os \n",
    "files = os.listdir('train_adv/prompt5/')\n",
    "for testcase in files:\n",
    "    training_path = 'train_adv/prompt5/'+testcase\n",
    "    \n",
    "    if testcase == 'noDisfluency&grammar_5_train_valid_reduce.csv':\n",
    "        names = 'training_noDisGramm'\n",
    "    elif testcase == 'noDisfluency_5_train_valid_reduce.csv':\n",
    "        names = 'training_noDis'\n",
    "    elif testcase == 'mixture_all_5_train_valid_reduce.csv':\n",
    "        names = 'training_mixture'\n",
    "    elif testcase == 'nocontractions_5_train_valid.csv':\n",
    "        names = 'training_noContr'\n",
    "    elif testcase == 'all_nochange_5_train_valid.csv':\n",
    "        names = 'training_SynContr'\n",
    "        \n",
    "#     training_path = 'train_adv/prompt1/noDisfluency&grammar_1_train_valid_reduce.csv'\n",
    "    essay_list, resolved_scores, essay_id = data_utils_adv.load_training_data(training_path, essay_set_id)\n",
    "\n",
    "    # print(essay_id)\n",
    "    testing_path = 'aes_data/essay5/fold_0/test.txt'\n",
    "    essay_list_test, resolved_scores_test, essay_id_test = data_utils_adv.load_testing_data(testing_path, essay_set_id)\n",
    "\n",
    "    max_score = max(resolved_scores)\n",
    "    min_score = min(resolved_scores)\n",
    "    if essay_set_id == 7:\n",
    "        min_score, max_score = 0, 30\n",
    "    elif essay_set_id == 8:\n",
    "        min_score, max_score = 0, 60\n",
    "\n",
    "    print( 'max_score is {} \\t min_score is {}\\n'.format(max_score, min_score))\n",
    "    with open(out_dir+'/params', 'a') as f:\n",
    "        f.write('max_score is {} \\t min_score is {} \\n'.format(max_score, min_score))\n",
    "    \n",
    "    score_range = range(min_score, max_score+1)\n",
    "\n",
    "    #word_idx, _ = data_utils.build_vocab(essay_list, vocab_limit)\n",
    "\n",
    "    # load glove\n",
    "    word_idx, word2vec = data_utils_adv.load_glove(num_tokens, dim=embedding_size)\n",
    "\n",
    "    vocab_size = len(word_idx) + 1\n",
    "    # stat info on data set\n",
    "\n",
    "    sent_size_list = list(map(len, [essay for essay in essay_list]))\n",
    "    # print(\"sent size list\", sent_size_list)\n",
    "    max_sent_size = max(sent_size_list)\n",
    "    mean_sent_size = int(np.mean(sent_size_list))\n",
    "\n",
    "    print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
    "    with open(out_dir+'/params', 'a') as f:\n",
    "        f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
    "\n",
    "    print( 'The length of score range is {}'.format(len(score_range)))\n",
    "    E = data_utils_adv.vectorize_data(essay_list, word_idx, max_sent_size)\n",
    "\n",
    "    labeled_data = zip(E, resolved_scores, sent_size_list)\n",
    "    \n",
    "    # For Testing\n",
    "    sent_size_list_T = list(map(len, [essayT for essayT in essay_list_test]))\n",
    "    max_sent_size_T = max(sent_size_list_T)\n",
    "    mean_sent_size_T = int(np.mean(sent_size_list_T))\n",
    "\n",
    "    # max_sent_size_T = max_sent_size_T.ljust(122, ' ')\n",
    "    print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size_T, mean_sent_size_T))\n",
    "    with open(out_dir+'/params', 'a') as f:\n",
    "        f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size_T, mean_sent_size_T))\n",
    "\n",
    "    # print( 'The length of score range of Testing Daat is {}'.format(len(score_range)))\n",
    "    E_T = data_utils_adv.vectorize_data(essay_list_test, word_idx, max_sent_size_T)\n",
    "\n",
    "    labeled_data_T = zip(E_T, resolved_scores_test, sent_size_list_T)\n",
    "    \n",
    "    def train_step(m, e, s, ma):\n",
    "        start_time = time.time()\n",
    "        feed_dict = {\n",
    "            model._query: e,\n",
    "            model._memory_key: m,\n",
    "            model._score_encoding: s,\n",
    "            model._mem_attention_encoding: ma,\n",
    "            model.keep_prob: 0.9\n",
    "            #model.w_placeholder: word2vec\n",
    "        }\n",
    "        _, step, predict_op, cost = sess.run([train_op, global_step, model.predict_op, model.cost], feed_dict)\n",
    "        end_time = time.time()\n",
    "        time_spent = end_time - start_time\n",
    "        return predict_op, cost, time_spent\n",
    "\n",
    "    def test_step(e, m):\n",
    "        feed_dict = {\n",
    "            model._query: e,\n",
    "            model._memory_key: m,\n",
    "            model.keep_prob: 1\n",
    "            #model.w_placeholder: word2vec\n",
    "        }\n",
    "        preds, mem_attention_probs = sess.run([model.predict_op, model.mem_attention_probs], feed_dict)\n",
    "        if is_regression:\n",
    "            preds = np.clip(np.round(preds), min_score, max_score)\n",
    "            return preds, mem_attention_probs\n",
    "        else:\n",
    "            return preds, mem_attention_probs\n",
    "        fold_count = 0\n",
    "    # kf = KFold(n_splits=2, random_state=random_state)\n",
    "    early_stop_count = 0\n",
    "\n",
    "    trainE = []\n",
    "    train_scores = []\n",
    "    train_essay_id = []\n",
    "    best_kappa_scores = []\n",
    "\n",
    "    testE = []\n",
    "    test_scores = []\n",
    "    test_essay_id = []\n",
    "\n",
    "    print(len(E_T))\n",
    "    # print(len(resolved_scores_test))\n",
    "    # print(len(essay_list_test))\n",
    "    for test_index in range(len(essay_id_test)):\n",
    "        testE.append(E[test_index])\n",
    "    #     print(len(testE))\n",
    "        test_scores.append(resolved_scores_test[test_index])\n",
    "        test_essay_id.append(essay_id_test[test_index])\n",
    "\n",
    "    for train_index in range(len(essay_id)):\n",
    "    #     print(\"Train_index\", train_index)\n",
    "    #     result = [int(x) for x, in train_index[0]]\n",
    "    #     print(result)\n",
    "    #     early_stop_count = 0\n",
    "    #     fold_count += 1\n",
    "    #     trainE = []\n",
    "    #     testE = []\n",
    "    #     train_scores = []\n",
    "    #     test_scores = []\n",
    "    #     train_essay_id = []\n",
    "    #     test_essay_id = []\n",
    "\n",
    "        trainE.append(E[train_index])\n",
    "        train_scores.append(resolved_scores[train_index])\n",
    "        train_essay_id.append(essay_id[train_index])\n",
    "\n",
    "    #     for ite in train_index[0]:\n",
    "    # #         print(ite)\n",
    "    #         trainE.append(E[ite])\n",
    "    #         train_scores.append(resolved_scores[ite])\n",
    "    #         train_essay_id.append(essay_id[ite])\n",
    "    #         print(\"Training Samples\", E[ite], resolved_scores[ite], essay_id[ite])\n",
    "\n",
    "    #     for ite in test_index:\n",
    "    #         testE.append(E[ite])\n",
    "    #         test_scores.append(resolved_scores[ite])\n",
    "    #         test_essay_id.append(essay_id[ite])\n",
    "\n",
    "    #trainE, testE, train_scores, test_scores, train_essay_id, test_essay_id = cross_validation.train_test_split(\n",
    "    #    E, resolved_scores, essay_id, test_size=.2, random_state=random_state)\n",
    "\n",
    "    memory = []\n",
    "    memory_score = []\n",
    "    memory_sent_size = []\n",
    "    memory_essay_ids = []\n",
    "    # pick sampled essay for each score\n",
    "    for i in score_range:\n",
    "    # test point: limit the number of samples in memory for 8\n",
    "        for j in range(num_samples):\n",
    "            if i in train_scores:\n",
    "                score_idx = train_scores.index(i)\n",
    "                score = train_scores.pop(score_idx)\n",
    "                essay = trainE.pop(score_idx)\n",
    "                sent_size = sent_size_list.pop(score_idx)\n",
    "                memory.append(essay)\n",
    "                memory_score.append(score)\n",
    "                memory_essay_ids.append(train_essay_id.pop(score_idx))\n",
    "                memory_sent_size.append(sent_size)\n",
    "    memory_size = len(memory)\n",
    "    if is_regression:\n",
    "    # bad naming\n",
    "        train_scores_encoding = train_scores\n",
    "    else:\n",
    "        train_scores_encoding = list(map(lambda x: score_range.index(x), train_scores))\n",
    "\n",
    "        # data size\n",
    "    n_train = len(trainE)\n",
    "    n_test = len(testE)\n",
    "\n",
    "    print( 'The size of training data: {}'.format(n_train))\n",
    "    print( 'The size of testing data: {}'.format(n_test))\n",
    "    with open(out_dir+'/params_{}'.format(names), 'a') as f:\n",
    "        f.write('The size of training data: {}\\n'.format(n_train))\n",
    "        f.write('The size of testing data: {}\\n'.format(n_test))\n",
    "        f.write('\\nEssay scores in memory:\\n{}'.format(memory_score))\n",
    "        f.write('\\nEssay ids in memory:\\n{}'.format(memory_essay_ids))\n",
    "        f.write('\\nEssay ids in training:\\n{}'.format(train_essay_id))\n",
    "        f.write('\\nEssay ids in testing:\\n{}'.format(test_essay_id))\n",
    "\n",
    "    batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "    batches = [(start, end) for start, end in batches]\n",
    "    print(batches)\n",
    "    x = 1\n",
    "    if x == 1:\n",
    "        with tf.Graph().as_default():\n",
    "            session_conf = tf.ConfigProto(\n",
    "                allow_soft_placement=True,\n",
    "                log_device_placement=False)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            # decay learning rate\n",
    "            starter_learning_rate = 0.0001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
    "\n",
    "            # test point\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.1)\n",
    "            #optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "            best_kappa_so_far = 0.0\n",
    "            with tf.Session(config=session_conf) as sess:\n",
    "                model = MemN2N_KV(batch_size, vocab_size, max_sent_size, max_sent_size, memory_size,\n",
    "                                  memory_size, embedding_size, len(score_range), feature_size, hops, reader, l2_lambda)\n",
    "\n",
    "                grads_and_vars = optimizer.compute_gradients(model.loss_op, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "                grads_and_vars = [(tf.clip_by_norm(g, 10.0), v)\n",
    "                                  for g, v in grads_and_vars if g is not None]\n",
    "                #grads_and_vars = [(add_gradient_noise(g, 1e-4), v) for g, v in grads_and_vars]\n",
    "                train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
    "                sess.run(tf.global_variables_initializer(), feed_dict={model.w_placeholder: word2vec})\n",
    "                saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "                for i in range(1, epochs+1):\n",
    "                    train_cost = 0\n",
    "                    total_time = 0\n",
    "                    np.random.shuffle(batches)\n",
    "                    for start, end in batches:\n",
    "                        e = trainE[start:end]\n",
    "                        s = train_scores_encoding[start:end]\n",
    "                        s_num = train_scores[start:end]\n",
    "                        #batched_memory = []\n",
    "                        # batch sized memory\n",
    "                        #for _ in range(len(e)):\n",
    "                        #    batched_memory.append(memory)\n",
    "                        mem_atten_encoding = []\n",
    "                        for ite in s_num:\n",
    "                            mem_encoding = np.zeros(memory_size)\n",
    "                            for j_idx, j in enumerate(memory_score):\n",
    "                                if j == ite:\n",
    "                                    mem_encoding[j_idx] = 1\n",
    "                            mem_atten_encoding.append(mem_encoding)\n",
    "                        batched_memory = [memory] * (end-start)\n",
    "                        _, cost, time_spent = train_step(batched_memory, e, s, mem_atten_encoding)\n",
    "                        total_time += time_spent\n",
    "                        train_cost += cost\n",
    "                    print( 'Finish epoch {}, total training cost is {}, time spent is {}'.format(i, train_cost, total_time))\n",
    "                    # evaluation\n",
    "                    if i % 5 == 0 or i == 200:\n",
    "                        # test on training data\n",
    "                        train_preds = []\n",
    "                        for start in range(0, n_train, test_batch_size):\n",
    "                            end = min(n_train, start+test_batch_size)\n",
    "\n",
    "                            #batched_memory = []\n",
    "                            #for _ in range(end-start):\n",
    "                            #    batched_memory.append(memory)\n",
    "                            batched_memory = [memory] * (end-start)\n",
    "    #                         print(\"BM\", len(batched_memory))\n",
    "                            preds, _ = test_step(trainE[start:end], batched_memory)\n",
    "                            if type(preds) is np.float32:\n",
    "                                train_preds.append(preds)\n",
    "                            else:\n",
    "                                for ite in preds:\n",
    "                                    train_preds.append(ite)\n",
    "                        if not is_regression:\n",
    "                            train_preds = np.add(train_preds, min_score)\n",
    "                        #train_kappp_score = kappa(train_scores, train_preds, 'quadratic')\n",
    "                        train_kappp_score = quadratic_weighted_kappa(\n",
    "                            train_scores, train_preds, min_score, max_score)\n",
    "                        # test on test data\n",
    "                        test_preds = []\n",
    "                        test_atten_probs = []\n",
    "                        for start in range(0, n_test, test_batch_size):\n",
    "                            end = min(n_test, start+test_batch_size)\n",
    "\n",
    "                            #batched_memory = []\n",
    "                            #for _ in range(end-start):\n",
    "                            #    batched_memory.append(memory)\n",
    "                            batched_memory = [memory] * (end-start)\n",
    "    #                         print(\"Test\", len(testE[start:end]))\n",
    "\n",
    "                            preds, mem_attention_probs = test_step(testE[start:end], batched_memory)\n",
    "                            if type(preds) is np.float32:\n",
    "                                test_preds.append(preds)\n",
    "                            else:\n",
    "                                for ite in preds:\n",
    "                                    test_preds.append(ite)\n",
    "                            for ite in mem_attention_probs:\n",
    "                                test_atten_probs.append(ite)\n",
    "                        if not is_regression:\n",
    "                            test_preds = np.add(test_preds, min_score)\n",
    "                        #test_kappp_score = kappa(test_scores, test_preds, 'quadratic')\n",
    "                        test_kappp_score = quadratic_weighted_kappa(\n",
    "                            test_scores, test_preds, min_score, max_score)\n",
    "                        stat_dict = {'pred_score': test_preds}\n",
    "                        stat_df = pd.DataFrame(stat_dict)\n",
    "                        # save the model if it gets best kappa\n",
    "                        if(test_kappp_score > best_kappa_so_far):\n",
    "                            early_stop_count = 0\n",
    "                            best_kappa_so_far = test_kappp_score\n",
    "                            # stats on test\n",
    "                            stat_df.to_csv(out_dir+'/predScore_'+names)\n",
    "                            with open(out_dir+'/mem_atten', 'a') as f:\n",
    "                                for idx, ite in enumerate(test_essay_id):\n",
    "                                    f.write('{}\\n'.format(ite))\n",
    "                                    f.write('{}\\n'.format(test_atten_probs[idx]))\n",
    "    #                         saver.save(sess, out_dir+'/checkpoints', global_step)\n",
    "    #                         model.save('prompt1.h5')\n",
    "    #                         tf.saved_model.save(model, 'runs/')\n",
    "                        else:\n",
    "                            early_stop_count += 1\n",
    "                        print(\"Training kappa score = {}\".format(train_kappp_score))\n",
    "                        print(\"Testing kappa score = {}\".format(test_kappp_score))\n",
    "                        with open(out_dir+'/eval_'.format(names), 'a') as f:\n",
    "                            f.write(\"Training kappa score = {}\\n\".format(train_kappp_score))\n",
    "                            f.write(\"Testing kappa score = {}\\n\".format(test_kappp_score))\n",
    "                            f.write(\"Best Testing kappa score so far = {}\\n\".format(best_kappa_so_far))\n",
    "                            f.write('*'*10)\n",
    "                            f.write('\\n')\n",
    "                    if early_stop_count > max_step_count:\n",
    "                        break\n",
    "                best_kappa_scores.append(best_kappa_so_far)\n",
    "\n",
    "    # model\n",
    "    with open(out_dir+'/eval_'.format(names), 'a') as f:\n",
    "        f.write('5 fold cv {}\\n'.format(best_kappa_scores))\n",
    "        f.write('final result is {}'.format(np.mean(np.array(best_kappa_scores))))\n",
    "\n",
    "    #sys.stdout = orig_stdout\n",
    "    #f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to load flags\n",
      "\n",
      "Writing to /mnt/data/rajivratn/memory_networks/automated-essay-grading/runs/adversary_training/essay_set_6_cv_1_Mar_25_2020_16:10:26\n",
      "\n",
      "                                               names  essay_id\n",
      "0  2, \"There were many obstacles that the builder...         1\n",
      "1  1, \"In the passage The Mooring Mast by Marcia ...         2\n",
      "2  3, The obstacles the builders face were great....         3\n",
      "3  4, \"While building the Empire State Building, ...         4\n",
      "4  3, \"Builders of the Empire State Building face...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 497 \n",
      "mean sentence size: 149\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 418 \n",
      "mean sentence size: 156\n",
      "\n",
      "360\n",
      "The size of training data: 1435\n",
      "The size of testing data: 360\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n",
      "Finish epoch 1, total training cost is 45484.095153808594, time spent is 2.271609306335449\n",
      "Finish epoch 2, total training cost is 21748.864387512207, time spent is 1.5172131061553955\n",
      "Finish epoch 3, total training cost is 15481.105041503906, time spent is 1.5087316036224365\n",
      "Finish epoch 4, total training cost is 14115.766666412354, time spent is 1.592329740524292\n",
      "Finish epoch 5, total training cost is 11833.549299240112, time spent is 1.5192668437957764\n",
      "Training kappa score = 0.18340813564151892\n",
      "Testing kappa score = 0.00889748549323044\n",
      "Finish epoch 6, total training cost is 10652.3625831604, time spent is 1.4903357028961182\n",
      "Finish epoch 7, total training cost is 9971.135234832764, time spent is 1.5153088569641113\n",
      "Finish epoch 8, total training cost is 9109.685863494873, time spent is 1.487424612045288\n",
      "Finish epoch 9, total training cost is 8918.648197174072, time spent is 1.4755690097808838\n",
      "Finish epoch 10, total training cost is 8628.876754760742, time spent is 1.4596147537231445\n",
      "Training kappa score = 0.10657675322057514\n",
      "Testing kappa score = 0.010301003344481696\n",
      "Finish epoch 11, total training cost is 8174.004367828369, time spent is 1.5006325244903564\n",
      "Finish epoch 12, total training cost is 7767.104524612427, time spent is 1.5183124542236328\n",
      "Finish epoch 13, total training cost is 7092.302047729492, time spent is 1.5258264541625977\n",
      "Finish epoch 14, total training cost is 6520.690116882324, time spent is 1.5044066905975342\n",
      "Finish epoch 15, total training cost is 6808.726463317871, time spent is 1.4737658500671387\n",
      "Training kappa score = 0.5617480972256321\n",
      "Testing kappa score = 0.039394527459732265\n",
      "Finish epoch 16, total training cost is 6037.824710845947, time spent is 1.5297574996948242\n",
      "Finish epoch 17, total training cost is 5897.788286209106, time spent is 1.4693493843078613\n",
      "Finish epoch 18, total training cost is 5777.884094238281, time spent is 1.5452799797058105\n",
      "Finish epoch 19, total training cost is 5803.304836273193, time spent is 1.541975736618042\n",
      "Finish epoch 20, total training cost is 5419.8586502075195, time spent is 1.510063648223877\n",
      "Training kappa score = 0.6233635448136958\n",
      "Testing kappa score = 0.042297417631344536\n",
      "Finish epoch 21, total training cost is 5383.081829071045, time spent is 1.471466302871704\n",
      "Finish epoch 22, total training cost is 5520.102727890015, time spent is 1.5862555503845215\n",
      "Finish epoch 23, total training cost is 5160.2550621032715, time spent is 1.4945120811462402\n",
      "Finish epoch 24, total training cost is 5094.686836242676, time spent is 1.4812517166137695\n",
      "Finish epoch 25, total training cost is 5034.679151535034, time spent is 1.5509843826293945\n",
      "Training kappa score = 0.6655243317340644\n",
      "Testing kappa score = 0.09250824564158955\n",
      "Finish epoch 26, total training cost is 5026.533122062683, time spent is 1.5646283626556396\n",
      "Finish epoch 27, total training cost is 4532.332809448242, time spent is 1.5278162956237793\n",
      "Finish epoch 28, total training cost is 4853.459564208984, time spent is 1.5425748825073242\n",
      "Finish epoch 29, total training cost is 4359.232581615448, time spent is 1.5274906158447266\n",
      "Finish epoch 30, total training cost is 4286.063092708588, time spent is 1.5311670303344727\n",
      "Training kappa score = 0.4774582628520997\n",
      "Testing kappa score = 0.017543859649123195\n",
      "Finish epoch 31, total training cost is 4305.593735694885, time spent is 1.6228978633880615\n",
      "Finish epoch 32, total training cost is 4450.254106521606, time spent is 1.4798517227172852\n",
      "Finish epoch 33, total training cost is 4205.807460784912, time spent is 1.4493048191070557\n",
      "Finish epoch 34, total training cost is 4155.273059844971, time spent is 1.5396637916564941\n",
      "Finish epoch 35, total training cost is 3936.819019317627, time spent is 1.5320758819580078\n",
      "Training kappa score = 0.6637519415520912\n",
      "Testing kappa score = 0.056364233336981484\n",
      "Finish epoch 36, total training cost is 4171.846385955811, time spent is 1.4640960693359375\n",
      "Finish epoch 37, total training cost is 4129.240233421326, time spent is 1.5075736045837402\n",
      "Finish epoch 38, total training cost is 3967.2675762176514, time spent is 1.46769380569458\n",
      "Finish epoch 39, total training cost is 4160.564226150513, time spent is 1.5121002197265625\n",
      "Finish epoch 40, total training cost is 3724.8068656921387, time spent is 1.4601728916168213\n",
      "Training kappa score = 0.4940990641950288\n",
      "Testing kappa score = 0.06975206611570284\n",
      "Finish epoch 41, total training cost is 3684.079071998596, time spent is 1.4839625358581543\n",
      "Finish epoch 42, total training cost is 3809.980951309204, time spent is 1.5202217102050781\n",
      "Finish epoch 43, total training cost is 3518.6743488311768, time spent is 1.4955768585205078\n",
      "Finish epoch 44, total training cost is 3724.9730224609375, time spent is 1.5041840076446533\n",
      "Finish epoch 45, total training cost is 3700.9018144607544, time spent is 1.561718225479126\n",
      "Training kappa score = 0.6879874205861094\n",
      "Testing kappa score = 0.0653716847217034\n",
      "Finish epoch 46, total training cost is 3657.8388118743896, time spent is 1.5082569122314453\n",
      "Finish epoch 47, total training cost is 3627.065888404846, time spent is 1.5054399967193604\n",
      "Finish epoch 48, total training cost is 3330.0040187835693, time spent is 1.4476978778839111\n",
      "Finish epoch 49, total training cost is 3526.696785926819, time spent is 1.5272293090820312\n",
      "Finish epoch 50, total training cost is 3509.0967197418213, time spent is 1.4675400257110596\n",
      "Training kappa score = 0.7432397141568339\n",
      "Testing kappa score = 0.07761748827329851\n",
      "Finish epoch 51, total training cost is 3351.136749267578, time spent is 1.4744763374328613\n",
      "Finish epoch 52, total training cost is 3329.314232826233, time spent is 1.5122454166412354\n",
      "Finish epoch 53, total training cost is 3482.6410489082336, time spent is 1.5113561153411865\n",
      "Finish epoch 54, total training cost is 3319.2166061401367, time spent is 1.4848017692565918\n",
      "Finish epoch 55, total training cost is 3185.852586746216, time spent is 1.5199713706970215\n",
      "Training kappa score = 0.7077625041979827\n",
      "Testing kappa score = 0.06859574468085139\n",
      "Finish epoch 56, total training cost is 3318.662226676941, time spent is 1.4927988052368164\n",
      "Finish epoch 57, total training cost is 3372.799892425537, time spent is 1.5651757717132568\n",
      "Finish epoch 58, total training cost is 3323.905430316925, time spent is 1.5134825706481934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 59, total training cost is 3131.666890144348, time spent is 1.5392062664031982\n",
      "Finish epoch 60, total training cost is 2905.452236175537, time spent is 1.4069557189941406\n",
      "Training kappa score = 0.6316201993057887\n",
      "Testing kappa score = 0.05481171548117192\n",
      "Finish epoch 61, total training cost is 2950.4787697792053, time spent is 1.5139482021331787\n",
      "Finish epoch 62, total training cost is 3081.251805305481, time spent is 1.5364594459533691\n",
      "Finish epoch 63, total training cost is 2859.178699016571, time spent is 1.531299114227295\n",
      "Finish epoch 64, total training cost is 2730.209800004959, time spent is 1.5733888149261475\n",
      "Finish epoch 65, total training cost is 3015.867064476013, time spent is 1.4680366516113281\n",
      "Training kappa score = 0.6857727532626468\n",
      "Testing kappa score = 0.0827023878858475\n",
      "Finish epoch 66, total training cost is 3034.8584566116333, time spent is 1.470674991607666\n",
      "Finish epoch 67, total training cost is 3080.430736541748, time spent is 1.4992783069610596\n",
      "Finish epoch 68, total training cost is 2938.205725669861, time spent is 1.5406138896942139\n",
      "Finish epoch 69, total training cost is 2857.359230995178, time spent is 1.5096101760864258\n",
      "Finish epoch 70, total training cost is 2811.7313165664673, time spent is 1.5508077144622803\n",
      "Training kappa score = 0.6646158871889802\n",
      "Testing kappa score = 0.04845019560637953\n",
      "Finish epoch 71, total training cost is 2825.4881930351257, time spent is 1.540255069732666\n",
      "Finish epoch 72, total training cost is 2565.637833595276, time spent is 1.52388334274292\n",
      "Finish epoch 73, total training cost is 2546.871162414551, time spent is 1.5057711601257324\n",
      "Finish epoch 74, total training cost is 2482.5516991615295, time spent is 1.638984203338623\n",
      "Finish epoch 75, total training cost is 2667.319593667984, time spent is 1.6276679039001465\n",
      "Training kappa score = 0.7484681140782208\n",
      "Testing kappa score = 0.060227484616818816\n",
      "Finish epoch 76, total training cost is 2613.40726852417, time spent is 1.5960819721221924\n",
      "Finish epoch 77, total training cost is 2694.941592693329, time spent is 1.601181983947754\n",
      "Finish epoch 78, total training cost is 2621.8690643310547, time spent is 1.5467793941497803\n",
      "Finish epoch 79, total training cost is 2454.13307762146, time spent is 1.5183045864105225\n",
      "Finish epoch 80, total training cost is 2607.488410949707, time spent is 1.6020312309265137\n",
      "Training kappa score = 0.7419778935579686\n",
      "Testing kappa score = 0.06642888987962525\n",
      "                                               names  essay_id\n",
      "0  2, \"There were many obstacles that the builder...         1\n",
      "1  1, \"In the passage The Mooring Mast by Marcia ...         2\n",
      "2  3, The obstacles the builders face were great....         3\n",
      "3  4, \"While building the Empire State Building, ...         4\n",
      "4  3, \"Builders of the Empire State Building face...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 471 \n",
      "mean sentence size: 163\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 418 \n",
      "mean sentence size: 156\n",
      "\n",
      "360\n",
      "The size of training data: 1435\n",
      "The size of testing data: 360\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n",
      "Finish epoch 1, total training cost is 36473.66151428223, time spent is 2.1736795902252197\n",
      "Finish epoch 2, total training cost is 21878.117156982422, time spent is 1.4973838329315186\n",
      "Finish epoch 3, total training cost is 15528.657369613647, time spent is 1.5007624626159668\n",
      "Finish epoch 4, total training cost is 13473.493144989014, time spent is 1.4815833568572998\n",
      "Finish epoch 5, total training cost is 11522.077659606934, time spent is 1.5475986003875732\n",
      "Training kappa score = 0.10563322480917059\n",
      "Testing kappa score = 0.027076734443649775\n",
      "Finish epoch 6, total training cost is 10795.865844726562, time spent is 1.5126547813415527\n",
      "Finish epoch 7, total training cost is 9787.171550750732, time spent is 1.4823081493377686\n",
      "Finish epoch 8, total training cost is 8700.848190307617, time spent is 1.5657520294189453\n",
      "Finish epoch 9, total training cost is 8469.67833328247, time spent is 1.4679663181304932\n",
      "Finish epoch 10, total training cost is 7610.966730117798, time spent is 1.5260939598083496\n",
      "Training kappa score = 0.20970490113257279\n",
      "Testing kappa score = -0.004907975460122671\n",
      "Finish epoch 11, total training cost is 7286.915568351746, time spent is 1.5040781497955322\n",
      "Finish epoch 12, total training cost is 6931.318328857422, time spent is 1.508650541305542\n",
      "Finish epoch 13, total training cost is 6791.701950073242, time spent is 1.5774195194244385\n",
      "Finish epoch 14, total training cost is 7919.119028091431, time spent is 1.5127789974212646\n",
      "Finish epoch 15, total training cost is 7250.288856506348, time spent is 1.4948456287384033\n",
      "Training kappa score = 0.33347113132148276\n",
      "Testing kappa score = -0.09388097233864201\n",
      "Finish epoch 16, total training cost is 7536.18332862854, time spent is 1.4362611770629883\n",
      "Finish epoch 17, total training cost is 7339.126012802124, time spent is 1.5369439125061035\n",
      "Finish epoch 18, total training cost is 7176.449296951294, time spent is 1.426464319229126\n",
      "Finish epoch 19, total training cost is 6301.237097740173, time spent is 1.4902570247650146\n",
      "Finish epoch 20, total training cost is 7985.26721572876, time spent is 1.5236880779266357\n",
      "Training kappa score = 0.4216407512679039\n",
      "Testing kappa score = -0.021189336978810758\n",
      "Finish epoch 21, total training cost is 6842.16171836853, time spent is 1.4891350269317627\n",
      "Finish epoch 22, total training cost is 6229.815994262695, time spent is 1.5275483131408691\n",
      "Finish epoch 23, total training cost is 5942.190032958984, time spent is 1.504492998123169\n",
      "Finish epoch 24, total training cost is 5508.297971725464, time spent is 1.515951156616211\n",
      "Finish epoch 25, total training cost is 4955.230870246887, time spent is 1.4994456768035889\n",
      "Training kappa score = 0.44995459942725424\n",
      "Testing kappa score = -0.07069118080818049\n",
      "Finish epoch 26, total training cost is 5013.007291793823, time spent is 1.4829611778259277\n",
      "Finish epoch 27, total training cost is 4958.146190643311, time spent is 1.4308197498321533\n",
      "Finish epoch 28, total training cost is 4807.104542732239, time spent is 1.5055832862854004\n",
      "Finish epoch 29, total training cost is 5277.4280796051025, time spent is 1.4439725875854492\n",
      "Finish epoch 30, total training cost is 4874.572721481323, time spent is 1.4518721103668213\n",
      "Training kappa score = 0.5425191410160601\n",
      "Testing kappa score = -0.0813240702901512\n",
      "Finish epoch 31, total training cost is 5148.966997146606, time spent is 1.4549908638000488\n",
      "Finish epoch 32, total training cost is 4592.054347991943, time spent is 1.5123722553253174\n",
      "Finish epoch 33, total training cost is 4554.465797424316, time spent is 1.4917757511138916\n",
      "Finish epoch 34, total training cost is 4330.308794975281, time spent is 1.4620990753173828\n",
      "Finish epoch 35, total training cost is 4654.724962234497, time spent is 1.4544203281402588\n",
      "Training kappa score = 0.5701658710710087\n",
      "Testing kappa score = -0.08224930861415558\n",
      "Finish epoch 36, total training cost is 4318.743015289307, time spent is 1.4798574447631836\n",
      "Finish epoch 37, total training cost is 4434.63982963562, time spent is 1.509474515914917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 38, total training cost is 4301.263426780701, time spent is 1.5182216167449951\n",
      "Finish epoch 39, total training cost is 4108.064617156982, time spent is 1.4580650329589844\n",
      "Finish epoch 40, total training cost is 3615.581630706787, time spent is 1.5044653415679932\n",
      "Training kappa score = 0.6382632351208646\n",
      "Testing kappa score = -0.07313410421985256\n",
      "Finish epoch 41, total training cost is 3613.9499530792236, time spent is 1.490046501159668\n",
      "Finish epoch 42, total training cost is 3954.2040281295776, time spent is 1.5390737056732178\n",
      "Finish epoch 43, total training cost is 3887.3633422851562, time spent is 1.5434565544128418\n",
      "Finish epoch 44, total training cost is 3661.968816757202, time spent is 1.5292844772338867\n",
      "Finish epoch 45, total training cost is 3635.516655921936, time spent is 1.5078160762786865\n",
      "Training kappa score = 0.6434786841752972\n",
      "Testing kappa score = -0.08827270414652744\n",
      "Finish epoch 46, total training cost is 3616.3947954177856, time spent is 1.5118181705474854\n",
      "Finish epoch 47, total training cost is 3584.1564378738403, time spent is 1.4891538619995117\n",
      "Finish epoch 48, total training cost is 3886.8760681152344, time spent is 1.4832727909088135\n",
      "Finish epoch 49, total training cost is 3522.7084817886353, time spent is 1.4574675559997559\n",
      "Finish epoch 50, total training cost is 3358.109854698181, time spent is 1.4627184867858887\n",
      "Training kappa score = 0.6033778262053936\n",
      "Testing kappa score = -0.047375663455867745\n",
      "Finish epoch 51, total training cost is 3240.766354560852, time spent is 1.488069772720337\n",
      "Finish epoch 52, total training cost is 3278.2526364326477, time spent is 1.487964391708374\n",
      "Finish epoch 53, total training cost is 3500.906937599182, time spent is 1.470181941986084\n",
      "Finish epoch 54, total training cost is 3228.2153549194336, time spent is 1.503530740737915\n",
      "Finish epoch 55, total training cost is 3243.4273252487183, time spent is 1.5246801376342773\n",
      "Training kappa score = 0.630614494351061\n",
      "Testing kappa score = -0.0742143515201148\n",
      "Finish epoch 56, total training cost is 3212.5385971069336, time spent is 1.4609370231628418\n",
      "Finish epoch 57, total training cost is 3234.7859926223755, time spent is 1.520054817199707\n",
      "Finish epoch 58, total training cost is 3226.3221502304077, time spent is 1.4323451519012451\n",
      "Finish epoch 59, total training cost is 3094.3716039657593, time spent is 1.5056159496307373\n",
      "Finish epoch 60, total training cost is 2915.96778011322, time spent is 1.4364416599273682\n",
      "Training kappa score = 0.6909947027663332\n",
      "Testing kappa score = -0.0618285019258058\n",
      "                                               names  essay_id\n",
      "0  2, \"There were many obstacles that the builder...         1\n",
      "1  1, \"In the passage The Mooring Mast by Marcia ...         2\n",
      "2  3, The obstacles the builders face were great....         3\n",
      "3  4, \"While building the Empire State Building, ...         4\n",
      "4  3, \"Builders of the Empire State Building face...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 375 \n",
      "mean sentence size: 134\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 418 \n",
      "mean sentence size: 156\n",
      "\n",
      "360\n",
      "The size of training data: 1435\n",
      "The size of testing data: 360\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n",
      "Finish epoch 1, total training cost is 62983.3034362793, time spent is 1.4488165378570557\n",
      "Finish epoch 2, total training cost is 23052.793014526367, time spent is 1.0037634372711182\n",
      "Finish epoch 3, total training cost is 16067.19733428955, time spent is 1.038888692855835\n",
      "Finish epoch 4, total training cost is 13210.11716079712, time spent is 1.136573076248169\n",
      "Finish epoch 5, total training cost is 11327.997055053711, time spent is 1.0539205074310303\n",
      "Training kappa score = 0.3946362068534579\n",
      "Testing kappa score = -0.03140293637846647\n",
      "Finish epoch 6, total training cost is 10201.261611938477, time spent is 1.0463848114013672\n",
      "Finish epoch 7, total training cost is 8980.129005432129, time spent is 1.055434226989746\n",
      "Finish epoch 8, total training cost is 9100.697525024414, time spent is 1.0386884212493896\n",
      "Finish epoch 9, total training cost is 8490.53038406372, time spent is 1.0556824207305908\n",
      "Finish epoch 10, total training cost is 8362.843547821045, time spent is 1.046987533569336\n",
      "Training kappa score = 0.5988337019907501\n",
      "Testing kappa score = -0.039856695029108824\n",
      "Finish epoch 11, total training cost is 7383.727827072144, time spent is 1.0542843341827393\n",
      "Finish epoch 12, total training cost is 7236.703960418701, time spent is 1.0554625988006592\n",
      "Finish epoch 13, total training cost is 6694.438440322876, time spent is 1.0617961883544922\n",
      "Finish epoch 14, total training cost is 7176.733648300171, time spent is 1.0368947982788086\n",
      "Finish epoch 15, total training cost is 6782.363286972046, time spent is 1.0412626266479492\n",
      "Training kappa score = 0.716708189343406\n",
      "Testing kappa score = 0.01664932362122762\n",
      "Finish epoch 16, total training cost is 6254.134960174561, time spent is 1.0377397537231445\n",
      "Finish epoch 17, total training cost is 5965.2897300720215, time spent is 1.0514020919799805\n",
      "Finish epoch 18, total training cost is 6372.696189880371, time spent is 1.059309720993042\n",
      "Finish epoch 19, total training cost is 5927.548606872559, time spent is 1.04721999168396\n",
      "Finish epoch 20, total training cost is 6015.245313644409, time spent is 1.0572550296783447\n",
      "Training kappa score = 0.6866996543750155\n",
      "Testing kappa score = -0.04605263157894757\n",
      "Finish epoch 21, total training cost is 5391.052888870239, time spent is 1.0467822551727295\n",
      "Finish epoch 22, total training cost is 5225.026247024536, time spent is 1.0500028133392334\n",
      "Finish epoch 23, total training cost is 5345.439059257507, time spent is 1.0389981269836426\n",
      "Finish epoch 24, total training cost is 5128.629632949829, time spent is 1.0383117198944092\n",
      "Finish epoch 25, total training cost is 5444.801374435425, time spent is 1.0509905815124512\n",
      "Training kappa score = 0.7238871913892407\n",
      "Testing kappa score = -0.04054714215925759\n",
      "Finish epoch 26, total training cost is 5102.120446205139, time spent is 1.0383501052856445\n",
      "Finish epoch 27, total training cost is 4650.161217689514, time spent is 1.0432546138763428\n",
      "Finish epoch 28, total training cost is 5117.308073043823, time spent is 1.0528032779693604\n",
      "Finish epoch 29, total training cost is 4854.408919334412, time spent is 1.0473194122314453\n",
      "Finish epoch 30, total training cost is 4440.850373268127, time spent is 1.0466177463531494\n",
      "Training kappa score = 0.7471595177826323\n",
      "Testing kappa score = -0.05753519902659465\n",
      "Finish epoch 31, total training cost is 4340.828570365906, time spent is 1.0416748523712158\n",
      "Finish epoch 32, total training cost is 4394.802898406982, time spent is 1.040029764175415\n",
      "Finish epoch 33, total training cost is 4154.412166595459, time spent is 1.0470459461212158\n",
      "Finish epoch 34, total training cost is 4032.118148803711, time spent is 1.0515732765197754\n",
      "Finish epoch 35, total training cost is 3982.229465484619, time spent is 1.0415465831756592\n",
      "Training kappa score = 0.5389458479219984\n",
      "Testing kappa score = -0.06463152873837386\n",
      "Finish epoch 36, total training cost is 4190.554535865784, time spent is 1.049353837966919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 37, total training cost is 4094.447693347931, time spent is 1.0542528629302979\n",
      "Finish epoch 38, total training cost is 4070.273871898651, time spent is 1.0901556015014648\n",
      "Finish epoch 39, total training cost is 4033.920313835144, time spent is 1.0656661987304688\n",
      "Finish epoch 40, total training cost is 3839.652353286743, time spent is 1.084526538848877\n",
      "Training kappa score = 0.7974814179199665\n",
      "Testing kappa score = -0.01543374135178266\n",
      "Finish epoch 41, total training cost is 3985.985689163208, time spent is 1.1954619884490967\n",
      "Finish epoch 42, total training cost is 3779.2055315971375, time spent is 1.1862547397613525\n",
      "Finish epoch 43, total training cost is 3716.2521181106567, time spent is 1.1834065914154053\n",
      "Finish epoch 44, total training cost is 3514.840979576111, time spent is 1.1490166187286377\n",
      "Finish epoch 45, total training cost is 3598.12975025177, time spent is 1.1355550289154053\n",
      "Training kappa score = 0.778992761435392\n",
      "Testing kappa score = 0.004358028830036775\n",
      "Finish epoch 46, total training cost is 3417.5262870788574, time spent is 1.1553783416748047\n",
      "Finish epoch 47, total training cost is 3668.1145095825195, time spent is 1.1957921981811523\n",
      "Finish epoch 48, total training cost is 3398.656825065613, time spent is 1.1721570491790771\n",
      "Finish epoch 49, total training cost is 3219.004216194153, time spent is 1.1511938571929932\n",
      "Finish epoch 50, total training cost is 3310.9050130844116, time spent is 1.1258347034454346\n",
      "Training kappa score = 0.7854017089849702\n",
      "Testing kappa score = 0.0019504040122596544\n",
      "Finish epoch 51, total training cost is 3165.176297187805, time spent is 1.1477713584899902\n",
      "Finish epoch 52, total training cost is 3414.240020751953, time spent is 1.1886053085327148\n",
      "Finish epoch 53, total training cost is 3106.870195388794, time spent is 1.2241878509521484\n",
      "Finish epoch 54, total training cost is 3445.8117818832397, time spent is 1.1319303512573242\n",
      "Finish epoch 55, total training cost is 3238.5364141464233, time spent is 1.1778802871704102\n",
      "Training kappa score = 0.8014751870942697\n",
      "Testing kappa score = -0.01393814432989715\n",
      "Finish epoch 56, total training cost is 3492.29811668396, time spent is 1.144881248474121\n",
      "Finish epoch 57, total training cost is 3154.48877620697, time spent is 1.2914619445800781\n",
      "Finish epoch 58, total training cost is 3270.2903954982758, time spent is 1.1632840633392334\n",
      "Finish epoch 59, total training cost is 3099.5265979766846, time spent is 1.2520925998687744\n",
      "Finish epoch 60, total training cost is 2889.6597576141357, time spent is 1.3373723030090332\n",
      "Training kappa score = 0.7666164766616477\n",
      "Testing kappa score = -0.04173459406586244\n",
      "Finish epoch 61, total training cost is 3089.112802505493, time spent is 1.3248190879821777\n",
      "Finish epoch 62, total training cost is 2962.289876937866, time spent is 1.3559839725494385\n",
      "Finish epoch 63, total training cost is 2643.7858147621155, time spent is 1.3685200214385986\n",
      "Finish epoch 64, total training cost is 2947.2778002023697, time spent is 1.333406925201416\n",
      "Finish epoch 65, total training cost is 3028.754063606262, time spent is 1.313544511795044\n",
      "Training kappa score = 0.7900301831611706\n",
      "Testing kappa score = -0.04103735356958671\n",
      "Finish epoch 66, total training cost is 2836.61732006073, time spent is 1.3337700366973877\n",
      "Finish epoch 67, total training cost is 2838.796004295349, time spent is 1.297886848449707\n",
      "Finish epoch 68, total training cost is 2719.051379084587, time spent is 1.3114073276519775\n",
      "Finish epoch 69, total training cost is 2799.6330046653748, time spent is 1.3032360076904297\n",
      "Finish epoch 70, total training cost is 2923.526472091675, time spent is 1.2750828266143799\n",
      "Training kappa score = 0.8049931243776377\n",
      "Testing kappa score = 0.0014456089627757907\n",
      "                                               names  essay_id\n",
      "0  2, \"There were many obstacles that the builder...         1\n",
      "1  1, \"In the passage The Mooring Mast by Marcia ...         2\n",
      "2  3, The obstacles the builders face were great....         3\n",
      "3  4, \"While building the Empire State Building, ...         4\n",
      "4  3, \"Builders of the Empire State Building face...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 489 \n",
      "mean sentence size: 162\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 418 \n",
      "mean sentence size: 156\n",
      "\n",
      "360\n",
      "The size of training data: 1435\n",
      "The size of testing data: 360\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n",
      "Finish epoch 1, total training cost is 97109.814453125, time spent is 2.3350043296813965\n",
      "Finish epoch 2, total training cost is 27877.033409118652, time spent is 1.6219379901885986\n",
      "Finish epoch 3, total training cost is 19466.330783843994, time spent is 1.6624352931976318\n",
      "Finish epoch 4, total training cost is 16260.411136627197, time spent is 1.6629908084869385\n",
      "Finish epoch 5, total training cost is 13531.909564971924, time spent is 1.63374662399292\n",
      "Training kappa score = 0.3581780538302277\n",
      "Testing kappa score = -0.03858875413450935\n",
      "Finish epoch 6, total training cost is 12477.272331237793, time spent is 1.6349377632141113\n",
      "Finish epoch 7, total training cost is 11095.704690933228, time spent is 1.6035172939300537\n",
      "Finish epoch 8, total training cost is 10615.473052978516, time spent is 1.6308648586273193\n",
      "Finish epoch 9, total training cost is 9905.802995681763, time spent is 1.5920844078063965\n",
      "Finish epoch 10, total training cost is 9514.81031036377, time spent is 1.6227490901947021\n",
      "Training kappa score = 0.5878627534304788\n",
      "Testing kappa score = -0.07828843106180661\n",
      "Finish epoch 11, total training cost is 9895.749855041504, time spent is 1.680025339126587\n",
      "Finish epoch 12, total training cost is 8714.674579620361, time spent is 1.6583240032196045\n",
      "Finish epoch 13, total training cost is 8452.481628417969, time spent is 1.692323923110962\n",
      "Finish epoch 14, total training cost is 8885.655590057373, time spent is 1.6313023567199707\n",
      "Finish epoch 15, total training cost is 7728.299928665161, time spent is 1.5983824729919434\n",
      "Training kappa score = 0.6416155322099584\n",
      "Testing kappa score = -0.02546068364045606\n",
      "Finish epoch 16, total training cost is 8601.016651153564, time spent is 1.6090350151062012\n",
      "Finish epoch 17, total training cost is 7643.346328735352, time spent is 1.6050200462341309\n",
      "Finish epoch 18, total training cost is 7279.226543426514, time spent is 1.578186273574829\n",
      "Finish epoch 19, total training cost is 7460.914064407349, time spent is 1.6319217681884766\n",
      "Finish epoch 20, total training cost is 7593.864652633667, time spent is 1.5337560176849365\n",
      "Training kappa score = 0.4535565630464047\n",
      "Testing kappa score = -0.01255230125522977\n",
      "Finish epoch 21, total training cost is 7159.259021759033, time spent is 1.556037187576294\n",
      "Finish epoch 22, total training cost is 7248.031721115112, time spent is 1.606623888015747\n",
      "Finish epoch 23, total training cost is 6646.206859588623, time spent is 1.5314323902130127\n",
      "Finish epoch 24, total training cost is 6739.250902175903, time spent is 1.5853385925292969\n",
      "Finish epoch 25, total training cost is 6927.9980182647705, time spent is 1.579188585281372\n",
      "Training kappa score = 0.5585615347404935\n",
      "Testing kappa score = -0.019434481608140652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 26, total training cost is 6635.303821563721, time spent is 1.5615010261535645\n",
      "Finish epoch 27, total training cost is 6407.345380783081, time spent is 1.5936484336853027\n",
      "Finish epoch 28, total training cost is 6396.937549591064, time spent is 1.566103458404541\n",
      "Finish epoch 29, total training cost is 5981.424373626709, time spent is 1.5937881469726562\n",
      "Finish epoch 30, total training cost is 6014.813238143921, time spent is 1.5524652004241943\n",
      "Training kappa score = 0.6842953413419279\n",
      "Testing kappa score = -0.07090232115978745\n",
      "Finish epoch 31, total training cost is 5760.373719215393, time spent is 1.5472314357757568\n",
      "Finish epoch 32, total training cost is 6262.245010375977, time spent is 1.5121705532073975\n",
      "Finish epoch 33, total training cost is 5995.535252571106, time spent is 1.6373240947723389\n",
      "Finish epoch 34, total training cost is 5987.09326171875, time spent is 1.6373391151428223\n",
      "Finish epoch 35, total training cost is 5818.428031921387, time spent is 1.6203546524047852\n",
      "Training kappa score = 0.5933976084216264\n",
      "Testing kappa score = -0.004699480583724824\n",
      "Finish epoch 36, total training cost is 5599.404262542725, time spent is 1.587937355041504\n",
      "Finish epoch 37, total training cost is 5729.418870925903, time spent is 1.5685629844665527\n",
      "Finish epoch 38, total training cost is 5549.408012390137, time spent is 1.5784611701965332\n",
      "Finish epoch 39, total training cost is 5147.042890548706, time spent is 1.5819401741027832\n",
      "Finish epoch 40, total training cost is 5611.3483543396, time spent is 1.5699198246002197\n",
      "Training kappa score = 0.5382915345440569\n",
      "Testing kappa score = -0.050525881625077096\n",
      "Finish epoch 41, total training cost is 5232.870962142944, time spent is 1.5766725540161133\n",
      "Finish epoch 42, total training cost is 5062.360290527344, time spent is 1.4924333095550537\n",
      "Finish epoch 43, total training cost is 5366.0217571258545, time spent is 1.5506041049957275\n",
      "Finish epoch 44, total training cost is 5329.113918304443, time spent is 1.5056357383728027\n",
      "Finish epoch 45, total training cost is 4852.851243019104, time spent is 1.558591365814209\n",
      "Training kappa score = 0.6178217821782179\n",
      "Testing kappa score = -0.04206418039895943\n",
      "Finish epoch 46, total training cost is 4761.365661621094, time spent is 1.5727651119232178\n",
      "Finish epoch 47, total training cost is 5126.118488311768, time spent is 1.553572177886963\n",
      "Finish epoch 48, total training cost is 4789.69141960144, time spent is 1.5301995277404785\n",
      "Finish epoch 49, total training cost is 5078.276830673218, time spent is 1.5482285022735596\n",
      "Finish epoch 50, total training cost is 4712.9150648117065, time spent is 1.4808945655822754\n",
      "Training kappa score = 0.7203463640609419\n",
      "Testing kappa score = -0.06468231253577561\n",
      "Finish epoch 51, total training cost is 4984.86617565155, time spent is 1.5577640533447266\n",
      "Finish epoch 52, total training cost is 4804.467383384705, time spent is 1.4939985275268555\n",
      "Finish epoch 53, total training cost is 4589.119672775269, time spent is 1.539947509765625\n",
      "Finish epoch 54, total training cost is 4459.525133609772, time spent is 1.5476038455963135\n",
      "Finish epoch 55, total training cost is 4387.345281600952, time spent is 1.3686177730560303\n",
      "Training kappa score = 0.7041570905174486\n",
      "Testing kappa score = -0.049026377572158664\n",
      "                                               names  essay_id\n",
      "0  2, \"There were many obstacles that the builder...         1\n",
      "1  1, \"In the passage The Mooring Mast by Marcia ...         2\n",
      "2  3, The obstacles the builders face were great....         3\n",
      "3  4, \"While building the Empire State Building, ...         4\n",
      "4  3, \"Builders of the Empire State Building face...         5\n",
      "max_score is 4 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 471 \n",
      "mean sentence size: 161\n",
      "\n",
      "The length of score range is 5\n",
      "max sentence size: 418 \n",
      "mean sentence size: 156\n",
      "\n",
      "360\n",
      "The size of training data: 1435\n",
      "The size of testing data: 360\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365), (1365, 1380), (1380, 1395), (1395, 1410), (1410, 1425)]\n",
      "Finish epoch 1, total training cost is 84234.2580871582, time spent is 2.073592185974121\n",
      "Finish epoch 2, total training cost is 39413.20700073242, time spent is 1.4053146839141846\n",
      "Finish epoch 3, total training cost is 30643.87713623047, time spent is 1.397735595703125\n",
      "Finish epoch 4, total training cost is 24167.978240966797, time spent is 1.407757043838501\n",
      "Finish epoch 5, total training cost is 22375.01625061035, time spent is 1.4121787548065186\n",
      "Training kappa score = 0.6402654148062453\n",
      "Testing kappa score = -0.0580718443844952\n",
      "Finish epoch 6, total training cost is 19800.188751220703, time spent is 1.3899767398834229\n",
      "Finish epoch 7, total training cost is 18749.039573669434, time spent is 1.3771922588348389\n",
      "Finish epoch 8, total training cost is 17476.410110473633, time spent is 1.3626935482025146\n",
      "Finish epoch 9, total training cost is 17035.41803741455, time spent is 1.3718395233154297\n",
      "Finish epoch 10, total training cost is 16430.36042022705, time spent is 1.3744635581970215\n",
      "Training kappa score = 0.6914908648255024\n",
      "Testing kappa score = -0.05577655522640712\n",
      "Finish epoch 11, total training cost is 14092.648573875427, time spent is 1.393862247467041\n",
      "Finish epoch 12, total training cost is 14059.636255264282, time spent is 1.3697175979614258\n",
      "Finish epoch 13, total training cost is 13734.980270385742, time spent is 1.3815245628356934\n",
      "Finish epoch 14, total training cost is 13052.952781677246, time spent is 1.3762731552124023\n",
      "Finish epoch 15, total training cost is 12410.633285522461, time spent is 1.397996187210083\n",
      "Training kappa score = 0.676534401468686\n",
      "Testing kappa score = -0.03494926719278468\n",
      "Finish epoch 16, total training cost is 11926.152799606323, time spent is 1.3937735557556152\n",
      "Finish epoch 17, total training cost is 11598.746643066406, time spent is 1.3774163722991943\n",
      "Finish epoch 18, total training cost is 11659.528617858887, time spent is 1.377650499343872\n",
      "Finish epoch 19, total training cost is 10627.683080673218, time spent is 1.3809044361114502\n",
      "Finish epoch 20, total training cost is 10881.539264678955, time spent is 1.3662385940551758\n",
      "Training kappa score = 0.6133469542906016\n",
      "Testing kappa score = -0.03747323340471076\n",
      "Finish epoch 21, total training cost is 10660.231063842773, time spent is 1.4556884765625\n",
      "Finish epoch 22, total training cost is 9816.101455688477, time spent is 1.3964393138885498\n",
      "Finish epoch 23, total training cost is 10035.801918029785, time spent is 1.389920949935913\n",
      "Finish epoch 24, total training cost is 9515.153995513916, time spent is 1.3756885528564453\n",
      "Finish epoch 25, total training cost is 9849.053619384766, time spent is 1.375495433807373\n",
      "Training kappa score = 0.5975422210307999\n",
      "Testing kappa score = -0.08805521540687988\n",
      "Finish epoch 26, total training cost is 10109.329044342041, time spent is 1.3643779754638672\n",
      "Finish epoch 27, total training cost is 9851.627407073975, time spent is 1.3669393062591553\n",
      "Finish epoch 28, total training cost is 8866.625659942627, time spent is 1.3795688152313232\n",
      "Finish epoch 29, total training cost is 9387.800300598145, time spent is 1.34602689743042\n",
      "Finish epoch 30, total training cost is 8638.361841201782, time spent is 1.391169548034668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kappa score = 0.7204164367788519\n",
      "Testing kappa score = -0.04546298014473904\n",
      "Finish epoch 31, total training cost is 8497.028980255127, time spent is 1.344900131225586\n",
      "Finish epoch 32, total training cost is 8266.655042648315, time spent is 1.3794763088226318\n",
      "Finish epoch 33, total training cost is 8394.484655380249, time spent is 1.3464391231536865\n",
      "Finish epoch 34, total training cost is 8766.467636108398, time spent is 1.371720314025879\n",
      "Finish epoch 35, total training cost is 8265.612783908844, time spent is 1.3991289138793945\n",
      "Training kappa score = 0.5970736779560308\n",
      "Testing kappa score = -0.028215028215028415\n",
      "Finish epoch 36, total training cost is 7978.009653091431, time spent is 1.3646271228790283\n",
      "Finish epoch 37, total training cost is 7826.025053024292, time spent is 1.3626809120178223\n",
      "Finish epoch 38, total training cost is 7170.438581466675, time spent is 1.3664445877075195\n",
      "Finish epoch 39, total training cost is 7813.267788887024, time spent is 1.3618099689483643\n",
      "Finish epoch 40, total training cost is 7747.7478103637695, time spent is 1.3590097427368164\n",
      "Training kappa score = 0.6787250033883269\n",
      "Testing kappa score = -0.02242647058823577\n",
      "Finish epoch 41, total training cost is 7801.575981140137, time spent is 1.3417418003082275\n",
      "Finish epoch 42, total training cost is 7335.335735321045, time spent is 1.388075828552246\n",
      "Finish epoch 43, total training cost is 7323.45449256897, time spent is 1.419797658920288\n",
      "Finish epoch 44, total training cost is 7289.922887802124, time spent is 1.3689656257629395\n",
      "Finish epoch 45, total training cost is 7049.613954544067, time spent is 1.4189398288726807\n",
      "Training kappa score = 0.6641670437649492\n",
      "Testing kappa score = -0.08049647963997986\n",
      "Finish epoch 46, total training cost is 7428.914348602295, time spent is 1.224609375\n",
      "Finish epoch 47, total training cost is 7084.202405929565, time spent is 1.156935453414917\n",
      "Finish epoch 48, total training cost is 7098.054840087891, time spent is 1.1650207042694092\n",
      "Finish epoch 49, total training cost is 7305.087591171265, time spent is 1.1542017459869385\n",
      "Finish epoch 50, total training cost is 7509.920993804932, time spent is 1.1315207481384277\n",
      "Training kappa score = 0.6832419040877722\n",
      "Testing kappa score = -0.08227294803302576\n",
      "Finish epoch 51, total training cost is 6657.602935791016, time spent is 1.1368603706359863\n",
      "Finish epoch 52, total training cost is 6545.39250087738, time spent is 1.1549875736236572\n",
      "Finish epoch 53, total training cost is 6880.868297576904, time spent is 1.1585805416107178\n",
      "Finish epoch 54, total training cost is 6825.462836265564, time spent is 1.159456729888916\n",
      "Finish epoch 55, total training cost is 6456.837779998779, time spent is 1.1396963596343994\n",
      "Training kappa score = 0.38940282743965293\n",
      "Testing kappa score = -0.023819301848049257\n"
     ]
    }
   ],
   "source": [
    "import data_utils_adv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from qwk import quadratic_weighted_kappa\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "print( 'start to load flags\\n')\n",
    "\n",
    "# flags\n",
    "# tf.flags.DEFINE_float(\"epsilon\", 0.1, \"Epsilon value for Adam Optimizer.\")\n",
    "# tf.flags.DEFINE_float(\"l2_lambda\", 0.3, \"Lambda for l2 loss.\")\n",
    "# tf.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate\")\n",
    "# tf.flags.DEFINE_float(\"max_grad_norm\", 10.0, \"Clip gradients to this norm.\")\n",
    "# tf.flags.DEFINE_float(\"keep_prob\", 0.9, \"Keep probability for dropout\")\n",
    "# tf.flags.DEFINE_integer(\"evaluation_interval\", 5, \"Evaluate and print results every x epochs\")\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 15, \"Batch size for training.\")\n",
    "# tf.flags.DEFINE_integer(\"feature_size\", 100, \"Feature size\")\n",
    "# tf.flags.DEFINE_integer(\"num_samples\", 1, \"Number of samples selected from training for each score\")\n",
    "# tf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "# tf.flags.DEFINE_integer(\"epochs\", 200, \"Number of epochs to train for.\")\n",
    "# tf.flags.DEFINE_integer(\"embedding_size\", 300, \"Embedding size for embedding matrices.\")\n",
    "# tf.flags.DEFINE_integer(\"essay_set_id\", 1, \"essay set id, 1 <= id <= 8\")\n",
    "# tf.flags.DEFINE_integer(\"token_num\", 42, \"The number of token in glove (6, 42)\")\n",
    "# tf.flags.DEFINE_boolean(\"gated_addressing\", False, \"Simple gated addressing\")\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"is_regression\", False, \"The output is regression or classification\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "# # hyper-parameters\n",
    "# FLAGS = tf.flags.FLAGS\n",
    "\n",
    "early_stop_count = 0\n",
    "max_step_count = 10\n",
    "is_regression = False\n",
    "gated_addressing = False\n",
    "essay_set_id = 6\n",
    "batch_size = 15\n",
    "embedding_size = 300\n",
    "feature_size = 100\n",
    "l2_lambda = 0.3\n",
    "hops = 3\n",
    "reader = 'bow' # gru may not work\n",
    "epochs = 200\n",
    "num_samples = 1\n",
    "num_tokens = 42\n",
    "test_batch_size = batch_size\n",
    "random_state = 0\n",
    "\n",
    "if is_regression:\n",
    "    from memn2n_kv_regression import MemN2N_KV\n",
    "else:\n",
    "    from memn2n_kv import MemN2N_KV\n",
    "# print flags info\n",
    "orig_stdout = sys.stdout\n",
    "timestamp = time.strftime(\"%b_%d_%Y_%H:%M:%S\", time.localtime())\n",
    "folder_name = 'essay_set_{}_cv_{}_{}'.format(essay_set_id, num_samples, timestamp)\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs/adversary_training/\", folder_name))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# save output to a file\n",
    "#f = file(out_dir+'/out.txt', 'w')\n",
    "#sys.stdout = f\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "# print(\"\\nParameters:\")\n",
    "# for key in sorted(FLAGS.__flags.keys()):\n",
    "#     print(\"{}={}\".format(key, getattr(FLAGS, key)))\n",
    "# print(\"\")\n",
    "\n",
    "# with open(out_dir+'/params', 'w') as f:\n",
    "#     for key in sorted(FLAGS.__flags.keys()):\n",
    "#         f.write(\"{}={}\".format(key, getattr(FLAGS, key)))\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "        \n",
    "import os \n",
    "files = os.listdir('train_adv/prompt6/')\n",
    "for testcase in files:\n",
    "    training_path = 'train_adv/prompt6/'+testcase\n",
    "    \n",
    "    if testcase == 'noDisfluency&grammar_6_train_valid_reduce.csv':\n",
    "        names = 'training_noDisGramm'\n",
    "    elif testcase == 'noDisfluency_6_train_valid_reduce.csv':\n",
    "        names = 'training_noDis'\n",
    "    elif testcase == 'mixture_all_6_train_valid_reduce.csv':\n",
    "        names = 'training_mixture'\n",
    "    elif testcase == 'nocontractions_6_train_valid.csv':\n",
    "        names = 'training_noContr'\n",
    "    elif testcase == 'all_nochange_6_train_valid.csv':\n",
    "        names = 'training_SynContr'\n",
    "        \n",
    "#     training_path = 'train_adv/prompt1/noDisfluency&grammar_1_train_valid_reduce.csv'\n",
    "    essay_list, resolved_scores, essay_id = data_utils_adv.load_training_data(training_path, essay_set_id)\n",
    "\n",
    "    # print(essay_id)\n",
    "    testing_path = 'aes_data/essay6/fold_0/test.txt'\n",
    "    essay_list_test, resolved_scores_test, essay_id_test = data_utils_adv.load_testing_data(testing_path, essay_set_id)\n",
    "\n",
    "    max_score = max(resolved_scores)\n",
    "    min_score = min(resolved_scores)\n",
    "    if essay_set_id == 7:\n",
    "        min_score, max_score = 0, 30\n",
    "    elif essay_set_id == 8:\n",
    "        min_score, max_score = 0, 60\n",
    "\n",
    "    print( 'max_score is {} \\t min_score is {}\\n'.format(max_score, min_score))\n",
    "    with open(out_dir+'/params', 'a') as f:\n",
    "        f.write('max_score is {} \\t min_score is {} \\n'.format(max_score, min_score))\n",
    "    \n",
    "    score_range = range(min_score, max_score+1)\n",
    "\n",
    "    #word_idx, _ = data_utils.build_vocab(essay_list, vocab_limit)\n",
    "\n",
    "    # load glove\n",
    "    word_idx, word2vec = data_utils_adv.load_glove(num_tokens, dim=embedding_size)\n",
    "\n",
    "    vocab_size = len(word_idx) + 1\n",
    "    # stat info on data set\n",
    "\n",
    "    sent_size_list = list(map(len, [essay for essay in essay_list]))\n",
    "    # print(\"sent size list\", sent_size_list)\n",
    "    max_sent_size = max(sent_size_list)\n",
    "    mean_sent_size = int(np.mean(sent_size_list))\n",
    "\n",
    "    print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
    "    with open(out_dir+'/params', 'a') as f:\n",
    "        f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
    "\n",
    "    print( 'The length of score range is {}'.format(len(score_range)))\n",
    "    E = data_utils_adv.vectorize_data(essay_list, word_idx, max_sent_size)\n",
    "\n",
    "    labeled_data = zip(E, resolved_scores, sent_size_list)\n",
    "    \n",
    "    # For Testing\n",
    "    sent_size_list_T = list(map(len, [essayT for essayT in essay_list_test]))\n",
    "    max_sent_size_T = max(sent_size_list_T)\n",
    "    mean_sent_size_T = int(np.mean(sent_size_list_T))\n",
    "\n",
    "    # max_sent_size_T = max_sent_size_T.ljust(122, ' ')\n",
    "    print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size_T, mean_sent_size_T))\n",
    "    with open(out_dir+'/params', 'a') as f:\n",
    "        f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size_T, mean_sent_size_T))\n",
    "\n",
    "    # print( 'The length of score range of Testing Daat is {}'.format(len(score_range)))\n",
    "    E_T = data_utils_adv.vectorize_data(essay_list_test, word_idx, max_sent_size_T)\n",
    "\n",
    "    labeled_data_T = zip(E_T, resolved_scores_test, sent_size_list_T)\n",
    "    \n",
    "    def train_step(m, e, s, ma):\n",
    "        start_time = time.time()\n",
    "        feed_dict = {\n",
    "            model._query: e,\n",
    "            model._memory_key: m,\n",
    "            model._score_encoding: s,\n",
    "            model._mem_attention_encoding: ma,\n",
    "            model.keep_prob: 0.9\n",
    "            #model.w_placeholder: word2vec\n",
    "        }\n",
    "        _, step, predict_op, cost = sess.run([train_op, global_step, model.predict_op, model.cost], feed_dict)\n",
    "        end_time = time.time()\n",
    "        time_spent = end_time - start_time\n",
    "        return predict_op, cost, time_spent\n",
    "\n",
    "    def test_step(e, m):\n",
    "        feed_dict = {\n",
    "            model._query: e,\n",
    "            model._memory_key: m,\n",
    "            model.keep_prob: 1\n",
    "            #model.w_placeholder: word2vec\n",
    "        }\n",
    "        preds, mem_attention_probs = sess.run([model.predict_op, model.mem_attention_probs], feed_dict)\n",
    "        if is_regression:\n",
    "            preds = np.clip(np.round(preds), min_score, max_score)\n",
    "            return preds, mem_attention_probs\n",
    "        else:\n",
    "            return preds, mem_attention_probs\n",
    "        fold_count = 0\n",
    "    # kf = KFold(n_splits=2, random_state=random_state)\n",
    "    early_stop_count = 0\n",
    "\n",
    "    trainE = []\n",
    "    train_scores = []\n",
    "    train_essay_id = []\n",
    "    best_kappa_scores = []\n",
    "\n",
    "    testE = []\n",
    "    test_scores = []\n",
    "    test_essay_id = []\n",
    "\n",
    "    print(len(E_T))\n",
    "    # print(len(resolved_scores_test))\n",
    "    # print(len(essay_list_test))\n",
    "    for test_index in range(len(essay_id_test)):\n",
    "        testE.append(E[test_index])\n",
    "    #     print(len(testE))\n",
    "        test_scores.append(resolved_scores_test[test_index])\n",
    "        test_essay_id.append(essay_id_test[test_index])\n",
    "\n",
    "    for train_index in range(len(essay_id)):\n",
    "    #     print(\"Train_index\", train_index)\n",
    "    #     result = [int(x) for x, in train_index[0]]\n",
    "    #     print(result)\n",
    "    #     early_stop_count = 0\n",
    "    #     fold_count += 1\n",
    "    #     trainE = []\n",
    "    #     testE = []\n",
    "    #     train_scores = []\n",
    "    #     test_scores = []\n",
    "    #     train_essay_id = []\n",
    "    #     test_essay_id = []\n",
    "\n",
    "        trainE.append(E[train_index])\n",
    "        train_scores.append(resolved_scores[train_index])\n",
    "        train_essay_id.append(essay_id[train_index])\n",
    "\n",
    "    #     for ite in train_index[0]:\n",
    "    # #         print(ite)\n",
    "    #         trainE.append(E[ite])\n",
    "    #         train_scores.append(resolved_scores[ite])\n",
    "    #         train_essay_id.append(essay_id[ite])\n",
    "    #         print(\"Training Samples\", E[ite], resolved_scores[ite], essay_id[ite])\n",
    "\n",
    "    #     for ite in test_index:\n",
    "    #         testE.append(E[ite])\n",
    "    #         test_scores.append(resolved_scores[ite])\n",
    "    #         test_essay_id.append(essay_id[ite])\n",
    "\n",
    "    #trainE, testE, train_scores, test_scores, train_essay_id, test_essay_id = cross_validation.train_test_split(\n",
    "    #    E, resolved_scores, essay_id, test_size=.2, random_state=random_state)\n",
    "\n",
    "    memory = []\n",
    "    memory_score = []\n",
    "    memory_sent_size = []\n",
    "    memory_essay_ids = []\n",
    "    # pick sampled essay for each score\n",
    "    for i in score_range:\n",
    "    # test point: limit the number of samples in memory for 8\n",
    "        for j in range(num_samples):\n",
    "            if i in train_scores:\n",
    "                score_idx = train_scores.index(i)\n",
    "                score = train_scores.pop(score_idx)\n",
    "                essay = trainE.pop(score_idx)\n",
    "                sent_size = sent_size_list.pop(score_idx)\n",
    "                memory.append(essay)\n",
    "                memory_score.append(score)\n",
    "                memory_essay_ids.append(train_essay_id.pop(score_idx))\n",
    "                memory_sent_size.append(sent_size)\n",
    "    memory_size = len(memory)\n",
    "    if is_regression:\n",
    "    # bad naming\n",
    "        train_scores_encoding = train_scores\n",
    "    else:\n",
    "        train_scores_encoding = list(map(lambda x: score_range.index(x), train_scores))\n",
    "\n",
    "        # data size\n",
    "    n_train = len(trainE)\n",
    "    n_test = len(testE)\n",
    "\n",
    "    print( 'The size of training data: {}'.format(n_train))\n",
    "    print( 'The size of testing data: {}'.format(n_test))\n",
    "    with open(out_dir+'/params_{}'.format(names), 'a') as f:\n",
    "        f.write('The size of training data: {}\\n'.format(n_train))\n",
    "        f.write('The size of testing data: {}\\n'.format(n_test))\n",
    "        f.write('\\nEssay scores in memory:\\n{}'.format(memory_score))\n",
    "        f.write('\\nEssay ids in memory:\\n{}'.format(memory_essay_ids))\n",
    "        f.write('\\nEssay ids in training:\\n{}'.format(train_essay_id))\n",
    "        f.write('\\nEssay ids in testing:\\n{}'.format(test_essay_id))\n",
    "\n",
    "    batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "    batches = [(start, end) for start, end in batches]\n",
    "    print(batches)\n",
    "    x = 1\n",
    "    if x == 1:\n",
    "        with tf.Graph().as_default():\n",
    "            session_conf = tf.ConfigProto(\n",
    "                allow_soft_placement=True,\n",
    "                log_device_placement=False)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            # decay learning rate\n",
    "            starter_learning_rate = 0.0001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
    "\n",
    "            # test point\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.1)\n",
    "            #optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "            best_kappa_so_far = 0.0\n",
    "            with tf.Session(config=session_conf) as sess:\n",
    "                model = MemN2N_KV(batch_size, vocab_size, max_sent_size, max_sent_size, memory_size,\n",
    "                                  memory_size, embedding_size, len(score_range), feature_size, hops, reader, l2_lambda)\n",
    "\n",
    "                grads_and_vars = optimizer.compute_gradients(model.loss_op, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "                grads_and_vars = [(tf.clip_by_norm(g, 10.0), v)\n",
    "                                  for g, v in grads_and_vars if g is not None]\n",
    "                #grads_and_vars = [(add_gradient_noise(g, 1e-4), v) for g, v in grads_and_vars]\n",
    "                train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
    "                sess.run(tf.global_variables_initializer(), feed_dict={model.w_placeholder: word2vec})\n",
    "                saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "                for i in range(1, epochs+1):\n",
    "                    train_cost = 0\n",
    "                    total_time = 0\n",
    "                    np.random.shuffle(batches)\n",
    "                    for start, end in batches:\n",
    "                        e = trainE[start:end]\n",
    "                        s = train_scores_encoding[start:end]\n",
    "                        s_num = train_scores[start:end]\n",
    "                        #batched_memory = []\n",
    "                        # batch sized memory\n",
    "                        #for _ in range(len(e)):\n",
    "                        #    batched_memory.append(memory)\n",
    "                        mem_atten_encoding = []\n",
    "                        for ite in s_num:\n",
    "                            mem_encoding = np.zeros(memory_size)\n",
    "                            for j_idx, j in enumerate(memory_score):\n",
    "                                if j == ite:\n",
    "                                    mem_encoding[j_idx] = 1\n",
    "                            mem_atten_encoding.append(mem_encoding)\n",
    "                        batched_memory = [memory] * (end-start)\n",
    "                        _, cost, time_spent = train_step(batched_memory, e, s, mem_atten_encoding)\n",
    "                        total_time += time_spent\n",
    "                        train_cost += cost\n",
    "                    print( 'Finish epoch {}, total training cost is {}, time spent is {}'.format(i, train_cost, total_time))\n",
    "                    # evaluation\n",
    "                    if i % 5 == 0 or i == 200:\n",
    "                        # test on training data\n",
    "                        train_preds = []\n",
    "                        for start in range(0, n_train, test_batch_size):\n",
    "                            end = min(n_train, start+test_batch_size)\n",
    "\n",
    "                            #batched_memory = []\n",
    "                            #for _ in range(end-start):\n",
    "                            #    batched_memory.append(memory)\n",
    "                            batched_memory = [memory] * (end-start)\n",
    "    #                         print(\"BM\", len(batched_memory))\n",
    "                            preds, _ = test_step(trainE[start:end], batched_memory)\n",
    "                            if type(preds) is np.float32:\n",
    "                                train_preds.append(preds)\n",
    "                            else:\n",
    "                                for ite in preds:\n",
    "                                    train_preds.append(ite)\n",
    "                        if not is_regression:\n",
    "                            train_preds = np.add(train_preds, min_score)\n",
    "                        #train_kappp_score = kappa(train_scores, train_preds, 'quadratic')\n",
    "                        train_kappp_score = quadratic_weighted_kappa(\n",
    "                            train_scores, train_preds, min_score, max_score)\n",
    "                        # test on test data\n",
    "                        test_preds = []\n",
    "                        test_atten_probs = []\n",
    "                        for start in range(0, n_test, test_batch_size):\n",
    "                            end = min(n_test, start+test_batch_size)\n",
    "\n",
    "                            #batched_memory = []\n",
    "                            #for _ in range(end-start):\n",
    "                            #    batched_memory.append(memory)\n",
    "                            batched_memory = [memory] * (end-start)\n",
    "    #                         print(\"Test\", len(testE[start:end]))\n",
    "\n",
    "                            preds, mem_attention_probs = test_step(testE[start:end], batched_memory)\n",
    "                            if type(preds) is np.float32:\n",
    "                                test_preds.append(preds)\n",
    "                            else:\n",
    "                                for ite in preds:\n",
    "                                    test_preds.append(ite)\n",
    "                            for ite in mem_attention_probs:\n",
    "                                test_atten_probs.append(ite)\n",
    "                        if not is_regression:\n",
    "                            test_preds = np.add(test_preds, min_score)\n",
    "                        #test_kappp_score = kappa(test_scores, test_preds, 'quadratic')\n",
    "                        test_kappp_score = quadratic_weighted_kappa(\n",
    "                            test_scores, test_preds, min_score, max_score)\n",
    "                        stat_dict = {'pred_score': test_preds}\n",
    "                        stat_df = pd.DataFrame(stat_dict)\n",
    "                        # save the model if it gets best kappa\n",
    "                        if(test_kappp_score > best_kappa_so_far):\n",
    "                            early_stop_count = 0\n",
    "                            best_kappa_so_far = test_kappp_score\n",
    "                            # stats on test\n",
    "                            stat_df.to_csv(out_dir+'/predScore_'+names)\n",
    "                            with open(out_dir+'/mem_atten', 'a') as f:\n",
    "                                for idx, ite in enumerate(test_essay_id):\n",
    "                                    f.write('{}\\n'.format(ite))\n",
    "                                    f.write('{}\\n'.format(test_atten_probs[idx]))\n",
    "    #                         saver.save(sess, out_dir+'/checkpoints', global_step)\n",
    "    #                         model.save('prompt1.h5')\n",
    "    #                         tf.saved_model.save(model, 'runs/')\n",
    "                        else:\n",
    "                            early_stop_count += 1\n",
    "                        print(\"Training kappa score = {}\".format(train_kappp_score))\n",
    "                        print(\"Testing kappa score = {}\".format(test_kappp_score))\n",
    "                        with open(out_dir+'/eval_'.format(names), 'a') as f:\n",
    "                            f.write(\"Training kappa score = {}\\n\".format(train_kappp_score))\n",
    "                            f.write(\"Testing kappa score = {}\\n\".format(test_kappp_score))\n",
    "                            f.write(\"Best Testing kappa score so far = {}\\n\".format(best_kappa_so_far))\n",
    "                            f.write('*'*10)\n",
    "                            f.write('\\n')\n",
    "                    if early_stop_count > max_step_count:\n",
    "                        break\n",
    "                best_kappa_scores.append(best_kappa_so_far)\n",
    "\n",
    "    # model\n",
    "    with open(out_dir+'/eval_'.format(names), 'a') as f:\n",
    "        f.write('5 fold cv {}\\n'.format(best_kappa_scores))\n",
    "        f.write('final result is {}'.format(np.mean(np.array(best_kappa_scores))))\n",
    "\n",
    "    #sys.stdout = orig_stdout\n",
    "    #f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to load flags\n",
      "\n",
      "Writing to /mnt/data/rajivratn/memory_networks/automated-essay-grading/runs/adversary_training/essay_set_7_cv_1_Mar_25_2020_16:46:26\n",
      "\n",
      "                                               names  essay_id\n",
      "0  17, \"I going to write about a time when I went...         1\n",
      "1  12, \"One time I was patience it was when I wan...         2\n",
      "2  14, \"The time that I was patient was not long ...         3\n",
      "3  17, \"Im writing about the time I was patient a...         4\n",
      "4  15, \"Patience is when you are waiting for some...         5\n",
      "max_score is 30 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 671 \n",
      "mean sentence size: 156\n",
      "\n",
      "The length of score range is 31\n",
      "max sentence size: 474 \n",
      "mean sentence size: 176\n",
      "\n",
      "314\n",
      "The size of training data: 1232\n",
      "The size of testing data: 314\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230)]\n",
      "Finish epoch 1, total training cost is 190271.6297607422, time spent is 4.802974462509155\n",
      "Finish epoch 2, total training cost is 79441.85961914062, time spent is 4.245795249938965\n",
      "Finish epoch 3, total training cost is 53968.66485595703, time spent is 4.457377672195435\n",
      "Finish epoch 4, total training cost is 43658.2392578125, time spent is 4.294506072998047\n",
      "Finish epoch 5, total training cost is 39814.90579223633, time spent is 4.46391224861145\n",
      "Training kappa score = 0.08660412239088955\n",
      "Testing kappa score = -0.025344804193306203\n",
      "Finish epoch 6, total training cost is 36264.38153076172, time spent is 4.31017541885376\n",
      "Finish epoch 7, total training cost is 33935.861740112305, time spent is 4.4105384349823\n",
      "Finish epoch 8, total training cost is 34960.98992919922, time spent is 4.281608581542969\n",
      "Finish epoch 9, total training cost is 29518.19725036621, time spent is 4.414020299911499\n",
      "Finish epoch 10, total training cost is 27954.001037597656, time spent is 4.355191230773926\n",
      "Training kappa score = -0.0418870768883195\n",
      "Testing kappa score = -0.017438945139941264\n",
      "Finish epoch 11, total training cost is 27082.763290405273, time spent is 4.342458963394165\n",
      "Finish epoch 12, total training cost is 26244.419815063477, time spent is 4.090595006942749\n",
      "Finish epoch 13, total training cost is 25085.607345581055, time spent is 4.147230863571167\n",
      "Finish epoch 14, total training cost is 24777.70526123047, time spent is 4.169440984725952\n",
      "Finish epoch 15, total training cost is 23571.002670288086, time spent is 4.275063753128052\n",
      "Training kappa score = -0.04706787711670368\n",
      "Testing kappa score = -0.010954098713012339\n",
      "Finish epoch 16, total training cost is 21619.223037719727, time spent is 4.229844331741333\n",
      "Finish epoch 17, total training cost is 21520.170570373535, time spent is 4.235971212387085\n",
      "Finish epoch 18, total training cost is 20061.196212768555, time spent is 4.152761697769165\n",
      "Finish epoch 19, total training cost is 19428.032470703125, time spent is 4.215487003326416\n",
      "Finish epoch 20, total training cost is 19456.63198852539, time spent is 4.2812864780426025\n",
      "Training kappa score = 0.07450346073782466\n",
      "Testing kappa score = -0.06050795873845227\n",
      "Finish epoch 21, total training cost is 17956.363677978516, time spent is 4.189441919326782\n",
      "Finish epoch 22, total training cost is 17960.564575195312, time spent is 4.251815319061279\n",
      "Finish epoch 23, total training cost is 18015.1746673584, time spent is 4.148767471313477\n",
      "Finish epoch 24, total training cost is 16791.021591186523, time spent is 4.2221198081970215\n",
      "Finish epoch 25, total training cost is 16611.40016937256, time spent is 4.25412917137146\n",
      "Training kappa score = 0.058759949354218244\n",
      "Testing kappa score = 0.014310087307601416\n",
      "Finish epoch 26, total training cost is 15569.270210266113, time spent is 4.3190758228302\n",
      "Finish epoch 27, total training cost is 15357.862815856934, time spent is 4.154433965682983\n",
      "Finish epoch 28, total training cost is 15731.351402282715, time spent is 4.160188913345337\n",
      "Finish epoch 29, total training cost is 15291.361427307129, time spent is 4.187849998474121\n",
      "Finish epoch 30, total training cost is 15072.24080657959, time spent is 4.122626781463623\n",
      "Training kappa score = -0.04919270083415439\n",
      "Testing kappa score = 0.02618201496971606\n",
      "Finish epoch 31, total training cost is 14678.373428344727, time spent is 4.25548243522644\n",
      "Finish epoch 32, total training cost is 13131.91372680664, time spent is 4.309808015823364\n",
      "Finish epoch 33, total training cost is 12200.62166595459, time spent is 4.291285037994385\n",
      "Finish epoch 34, total training cost is 11849.335166931152, time spent is 4.135183811187744\n",
      "Finish epoch 35, total training cost is 12366.641990661621, time spent is 4.283488035202026\n",
      "Training kappa score = 0.03561661408920469\n",
      "Testing kappa score = 0.018940000073068064\n",
      "Finish epoch 36, total training cost is 11380.926162719727, time spent is 4.248405694961548\n",
      "Finish epoch 37, total training cost is 11327.760932922363, time spent is 4.250765800476074\n",
      "Finish epoch 38, total training cost is 10571.627571105957, time spent is 4.12944483757019\n",
      "Finish epoch 39, total training cost is 10064.371643066406, time spent is 4.020448207855225\n",
      "Finish epoch 40, total training cost is 10101.520683288574, time spent is 4.180811166763306\n",
      "Training kappa score = 0.13723663289565358\n",
      "Testing kappa score = -0.1137426720973782\n",
      "Finish epoch 41, total training cost is 9600.286735534668, time spent is 4.263416290283203\n",
      "Finish epoch 42, total training cost is 8151.963642120361, time spent is 4.119670867919922\n",
      "Finish epoch 43, total training cost is 7587.299877166748, time spent is 4.172398090362549\n",
      "Finish epoch 44, total training cost is 6877.643424987793, time spent is 4.231679677963257\n",
      "Finish epoch 45, total training cost is 5906.054050445557, time spent is 4.095270872116089\n",
      "Training kappa score = 0.28585818953035624\n",
      "Testing kappa score = 0.006496662401681785\n",
      "Finish epoch 46, total training cost is 5185.364505767822, time spent is 4.254143953323364\n",
      "Finish epoch 47, total training cost is 4810.610824584961, time spent is 4.116811275482178\n",
      "Finish epoch 48, total training cost is 4535.876159667969, time spent is 4.106461524963379\n",
      "Finish epoch 49, total training cost is 4393.718475341797, time spent is 4.148492097854614\n",
      "Finish epoch 50, total training cost is 4328.730857849121, time spent is 4.246140718460083\n",
      "Training kappa score = 0.44132356696603603\n",
      "Testing kappa score = 0.048961468692519494\n",
      "Finish epoch 51, total training cost is 4241.3253173828125, time spent is 4.1363205909729\n",
      "Finish epoch 52, total training cost is 4166.6984214782715, time spent is 4.124178171157837\n",
      "Finish epoch 53, total training cost is 4137.019386291504, time spent is 4.1981775760650635\n",
      "Finish epoch 54, total training cost is 4043.005790710449, time spent is 4.236896276473999\n",
      "Finish epoch 55, total training cost is 3968.9587631225586, time spent is 4.095510721206665\n",
      "Training kappa score = 0.27990422970250006\n",
      "Testing kappa score = 0.01058128760030852\n",
      "Finish epoch 56, total training cost is 3823.453716278076, time spent is 4.14680027961731\n",
      "Finish epoch 57, total training cost is 3898.230312347412, time spent is 4.2007293701171875\n",
      "Finish epoch 58, total training cost is 3825.5864295959473, time spent is 4.190762996673584\n",
      "Finish epoch 59, total training cost is 3790.206386566162, time spent is 4.213367223739624\n",
      "Finish epoch 60, total training cost is 3760.1071128845215, time spent is 4.199715852737427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kappa score = 0.4027103547614065\n",
      "Testing kappa score = 0.07355573804023108\n",
      "Finish epoch 61, total training cost is 3724.9933853149414, time spent is 4.126752614974976\n",
      "Finish epoch 62, total training cost is 3803.0560913085938, time spent is 4.102953195571899\n",
      "Finish epoch 63, total training cost is 3818.1110038757324, time spent is 4.114755630493164\n",
      "Finish epoch 64, total training cost is 3791.4207038879395, time spent is 4.221102476119995\n",
      "Finish epoch 65, total training cost is 3864.5725288391113, time spent is 4.18124794960022\n",
      "Training kappa score = 0.46455322502843\n",
      "Testing kappa score = 0.018711082242807153\n",
      "Finish epoch 66, total training cost is 3872.540382385254, time spent is 4.1348466873168945\n",
      "Finish epoch 67, total training cost is 3922.044403076172, time spent is 4.2014853954315186\n",
      "Finish epoch 68, total training cost is 3744.005672454834, time spent is 4.229610443115234\n",
      "Finish epoch 69, total training cost is 3657.995620727539, time spent is 4.354031801223755\n",
      "Finish epoch 70, total training cost is 3574.579376220703, time spent is 4.153580665588379\n",
      "Training kappa score = 0.43897311688147234\n",
      "Testing kappa score = 0.022680225249206765\n",
      "Finish epoch 71, total training cost is 3446.821060180664, time spent is 4.231691360473633\n",
      "Finish epoch 72, total training cost is 3446.4760971069336, time spent is 4.0996413230896\n",
      "Finish epoch 73, total training cost is 3372.6122283935547, time spent is 4.270550012588501\n",
      "Finish epoch 74, total training cost is 3381.9859313964844, time spent is 4.153581142425537\n",
      "Finish epoch 75, total training cost is 3381.253391265869, time spent is 4.223555564880371\n",
      "Training kappa score = 0.5124133154519155\n",
      "Testing kappa score = 0.06579139506504417\n",
      "Finish epoch 76, total training cost is 3302.0811653137207, time spent is 4.161979675292969\n",
      "Finish epoch 77, total training cost is 3292.4977645874023, time spent is 4.142719268798828\n",
      "Finish epoch 78, total training cost is 3350.481719970703, time spent is 4.223435878753662\n",
      "Finish epoch 79, total training cost is 3381.2357482910156, time spent is 4.232518911361694\n",
      "Finish epoch 80, total training cost is 3306.976121902466, time spent is 4.103961229324341\n",
      "Training kappa score = 0.5152384038033371\n",
      "Testing kappa score = 0.05004585360828595\n",
      "Finish epoch 81, total training cost is 3347.5211601257324, time spent is 4.16421365737915\n",
      "Finish epoch 82, total training cost is 3465.2696952819824, time spent is 4.273982763290405\n",
      "Finish epoch 83, total training cost is 3456.938316345215, time spent is 4.302597284317017\n",
      "Finish epoch 84, total training cost is 3427.1281242370605, time spent is 4.360938310623169\n",
      "Finish epoch 85, total training cost is 3353.951614379883, time spent is 4.15333890914917\n",
      "Training kappa score = 0.5535184471267537\n",
      "Testing kappa score = 0.033746692772778863\n",
      "Finish epoch 86, total training cost is 3278.5508422851562, time spent is 4.245480537414551\n",
      "Finish epoch 87, total training cost is 3275.6567726135254, time spent is 4.133918285369873\n",
      "Finish epoch 88, total training cost is 3244.6863555908203, time spent is 4.118271589279175\n",
      "Finish epoch 89, total training cost is 3233.510799407959, time spent is 4.171878099441528\n",
      "Finish epoch 90, total training cost is 3167.1781253814697, time spent is 4.100254535675049\n",
      "Training kappa score = 0.5509107560801869\n",
      "Testing kappa score = 0.04811053822740696\n",
      "Finish epoch 91, total training cost is 3201.1825942993164, time spent is 4.076114177703857\n",
      "Finish epoch 92, total training cost is 3178.2437992095947, time spent is 4.156580448150635\n",
      "Finish epoch 93, total training cost is 3157.736837387085, time spent is 4.0986974239349365\n",
      "Finish epoch 94, total training cost is 3182.278860092163, time spent is 4.086964845657349\n",
      "Finish epoch 95, total training cost is 3138.168472290039, time spent is 4.124510049819946\n",
      "Training kappa score = 0.6150747213987864\n",
      "Testing kappa score = 0.06145378464957563\n",
      "Finish epoch 96, total training cost is 3101.8613414764404, time spent is 3.9886507987976074\n",
      "Finish epoch 97, total training cost is 3093.8127994537354, time spent is 3.9442567825317383\n",
      "Finish epoch 98, total training cost is 3099.37819480896, time spent is 4.001704931259155\n",
      "Finish epoch 99, total training cost is 3098.6988525390625, time spent is 4.080528020858765\n",
      "Finish epoch 100, total training cost is 3147.6269092559814, time spent is 4.10592246055603\n",
      "Training kappa score = 0.6244853134025948\n",
      "Testing kappa score = 0.0347230620826775\n",
      "Finish epoch 101, total training cost is 3068.2915439605713, time spent is 4.1897382736206055\n",
      "Finish epoch 102, total training cost is 3050.431468963623, time spent is 4.268840551376343\n",
      "Finish epoch 103, total training cost is 3058.806604385376, time spent is 4.2045183181762695\n",
      "Finish epoch 104, total training cost is 3051.6701831817627, time spent is 4.066425561904907\n",
      "Finish epoch 105, total training cost is 3044.695571899414, time spent is 4.166141033172607\n",
      "Training kappa score = 0.5794212187588295\n",
      "Testing kappa score = 0.024358173037953845\n",
      "Finish epoch 106, total training cost is 3099.0777549743652, time spent is 4.130425453186035\n",
      "Finish epoch 107, total training cost is 3044.202247619629, time spent is 3.9349308013916016\n",
      "Finish epoch 108, total training cost is 3050.7039070129395, time spent is 3.9781882762908936\n",
      "Finish epoch 109, total training cost is 3004.3677673339844, time spent is 4.066287517547607\n",
      "Finish epoch 110, total training cost is 3017.511938095093, time spent is 3.9964759349823\n",
      "Training kappa score = 0.5963474700688574\n",
      "Testing kappa score = 0.0709583823935579\n",
      "Finish epoch 111, total training cost is 3013.8084201812744, time spent is 4.062815427780151\n",
      "Finish epoch 112, total training cost is 3008.2453174591064, time spent is 3.983581781387329\n",
      "Finish epoch 113, total training cost is 3031.9348945617676, time spent is 4.101675987243652\n",
      "Finish epoch 114, total training cost is 3030.103048324585, time spent is 4.100122451782227\n",
      "Finish epoch 115, total training cost is 3043.3146419525146, time spent is 4.154072046279907\n",
      "Training kappa score = 0.6237465646586942\n",
      "Testing kappa score = 0.03597189837562642\n",
      "                                               names  essay_id\n",
      "0  17, \"I going to write about a time when I went...         1\n",
      "1  12, \"One time I was patience it was when I wan...         2\n",
      "2  14, \"The time that I was patient was not long ...         3\n",
      "3  17, \"Im writing about the time I was patient a...         4\n",
      "4  15, \"Patience is when you are waiting for some...         5\n",
      "max_score is 30 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 659 \n",
      "mean sentence size: 178\n",
      "\n",
      "The length of score range is 31\n",
      "max sentence size: 474 \n",
      "mean sentence size: 176\n",
      "\n",
      "314\n",
      "The size of training data: 1232\n",
      "The size of testing data: 314\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230)]\n",
      "Finish epoch 1, total training cost is 241002.72094726562, time spent is 3.3412187099456787\n",
      "Finish epoch 2, total training cost is 113301.65002441406, time spent is 3.8813438415527344\n",
      "Finish epoch 3, total training cost is 75653.45706176758, time spent is 3.1072397232055664\n",
      "Finish epoch 4, total training cost is 53885.583404541016, time spent is 3.5510458946228027\n",
      "Finish epoch 5, total training cost is 41831.14044189453, time spent is 3.948514938354492\n",
      "Training kappa score = 0.32656931310740367\n",
      "Testing kappa score = 0.02464608376649291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 6, total training cost is 34785.67184448242, time spent is 3.9219610691070557\n",
      "Finish epoch 7, total training cost is 31546.91323852539, time spent is 3.6396896839141846\n",
      "Finish epoch 8, total training cost is 29043.278915405273, time spent is 3.9607186317443848\n",
      "Finish epoch 9, total training cost is 27869.665451049805, time spent is 3.9315459728240967\n",
      "Finish epoch 10, total training cost is 25432.110870361328, time spent is 3.6106555461883545\n",
      "Training kappa score = 0.30723072022662046\n",
      "Testing kappa score = 0.02077266219800955\n",
      "Finish epoch 11, total training cost is 24102.97640991211, time spent is 3.7751681804656982\n",
      "Finish epoch 12, total training cost is 22112.002471923828, time spent is 3.851398468017578\n",
      "Finish epoch 13, total training cost is 22005.22523498535, time spent is 3.5266153812408447\n",
      "Finish epoch 14, total training cost is 21169.799758911133, time spent is 3.85526180267334\n",
      "Finish epoch 15, total training cost is 20026.70458984375, time spent is 3.5637192726135254\n",
      "Training kappa score = 0.3420561075987143\n",
      "Testing kappa score = -0.0018612071257644303\n",
      "Finish epoch 16, total training cost is 19427.547164916992, time spent is 3.630704641342163\n",
      "Finish epoch 17, total training cost is 18991.92268371582, time spent is 3.871060371398926\n",
      "Finish epoch 18, total training cost is 17926.924339294434, time spent is 3.4871559143066406\n",
      "Finish epoch 19, total training cost is 17887.721252441406, time spent is 3.835357189178467\n",
      "Finish epoch 20, total training cost is 17272.629051208496, time spent is 3.5021257400512695\n",
      "Training kappa score = 0.6127750378419305\n",
      "Testing kappa score = -0.023336552953583167\n",
      "Finish epoch 21, total training cost is 17590.960815429688, time spent is 3.741724729537964\n",
      "Finish epoch 22, total training cost is 16373.351692199707, time spent is 3.7328133583068848\n",
      "Finish epoch 23, total training cost is 16843.395599365234, time spent is 3.573096513748169\n",
      "Finish epoch 24, total training cost is 16264.158958435059, time spent is 3.236544132232666\n",
      "Finish epoch 25, total training cost is 15224.76838684082, time spent is 3.2306368350982666\n",
      "Training kappa score = 0.3724932450577524\n",
      "Testing kappa score = 0.03184645098924033\n",
      "Finish epoch 26, total training cost is 14844.813201904297, time spent is 4.1955084800720215\n",
      "Finish epoch 27, total training cost is 15239.571891784668, time spent is 4.125032901763916\n",
      "Finish epoch 28, total training cost is 14861.004264831543, time spent is 4.20846152305603\n",
      "Finish epoch 29, total training cost is 14934.91008758545, time spent is 4.026221036911011\n",
      "Finish epoch 30, total training cost is 14523.570533752441, time spent is 4.276355743408203\n",
      "Training kappa score = 0.37558201011405323\n",
      "Testing kappa score = 0.007021297685658623\n",
      "Finish epoch 31, total training cost is 13763.445343017578, time spent is 3.560620069503784\n",
      "Finish epoch 32, total training cost is 13326.880279541016, time spent is 3.6129019260406494\n",
      "Finish epoch 33, total training cost is 13438.372901916504, time spent is 3.6019630432128906\n",
      "Finish epoch 34, total training cost is 13451.887496948242, time spent is 3.565046787261963\n",
      "Finish epoch 35, total training cost is 13333.908729553223, time spent is 3.629361152648926\n",
      "Training kappa score = 0.4707576862949453\n",
      "Testing kappa score = -0.0011714377333036374\n",
      "Finish epoch 36, total training cost is 13039.511413574219, time spent is 3.5757434368133545\n",
      "Finish epoch 37, total training cost is 13349.529998779297, time spent is 3.570549488067627\n",
      "Finish epoch 38, total training cost is 12561.002449035645, time spent is 3.645519733428955\n",
      "Finish epoch 39, total training cost is 12111.595375061035, time spent is 3.690565824508667\n",
      "Finish epoch 40, total training cost is 12325.736419677734, time spent is 3.5606398582458496\n",
      "Training kappa score = 0.5078667026529284\n",
      "Testing kappa score = 0.03343494947174386\n",
      "Finish epoch 41, total training cost is 11831.249572753906, time spent is 3.661724090576172\n",
      "Finish epoch 42, total training cost is 11589.680526733398, time spent is 3.6120569705963135\n",
      "Finish epoch 43, total training cost is 11639.04875946045, time spent is 3.6147098541259766\n",
      "Finish epoch 44, total training cost is 11543.369438171387, time spent is 3.593235969543457\n",
      "Finish epoch 45, total training cost is 11156.232383728027, time spent is 3.668666124343872\n",
      "Training kappa score = 0.5438402081854383\n",
      "Testing kappa score = 0.06778513785986273\n",
      "Finish epoch 46, total training cost is 11502.102516174316, time spent is 3.5952653884887695\n",
      "Finish epoch 47, total training cost is 11509.280967712402, time spent is 3.719632863998413\n",
      "Finish epoch 48, total training cost is 10984.64380645752, time spent is 3.6371586322784424\n",
      "Finish epoch 49, total training cost is 10699.88737487793, time spent is 3.814406394958496\n",
      "Finish epoch 50, total training cost is 10766.529335021973, time spent is 3.5996155738830566\n",
      "Training kappa score = 0.294572209800411\n",
      "Testing kappa score = 0.01717936399645814\n",
      "Finish epoch 51, total training cost is 10592.690673828125, time spent is 3.5965628623962402\n",
      "Finish epoch 52, total training cost is 10579.664512634277, time spent is 3.6493117809295654\n",
      "Finish epoch 53, total training cost is 10173.87459564209, time spent is 3.767122268676758\n",
      "Finish epoch 54, total training cost is 10200.731491088867, time spent is 3.680002212524414\n",
      "Finish epoch 55, total training cost is 10413.833015441895, time spent is 3.571408987045288\n",
      "Training kappa score = 0.3632656923620716\n",
      "Testing kappa score = 0.045335667410793756\n",
      "Finish epoch 56, total training cost is 10086.148979187012, time spent is 3.809993028640747\n",
      "Finish epoch 57, total training cost is 10040.916442871094, time spent is 3.6055893898010254\n",
      "Finish epoch 58, total training cost is 10084.346015930176, time spent is 3.5972602367401123\n",
      "Finish epoch 59, total training cost is 9509.05428314209, time spent is 3.593766212463379\n",
      "Finish epoch 60, total training cost is 9489.176574707031, time spent is 3.6527504920959473\n",
      "Training kappa score = 0.3978033669313027\n",
      "Testing kappa score = 0.04110024031948889\n",
      "Finish epoch 61, total training cost is 9240.953475952148, time spent is 3.5719661712646484\n",
      "Finish epoch 62, total training cost is 9364.29891204834, time spent is 3.6020662784576416\n",
      "Finish epoch 63, total training cost is 8826.970817565918, time spent is 3.5851588249206543\n",
      "Finish epoch 64, total training cost is 9237.55859375, time spent is 3.792459011077881\n",
      "Finish epoch 65, total training cost is 8931.711959838867, time spent is 3.6303369998931885\n",
      "Training kappa score = 0.5176196634964723\n",
      "Testing kappa score = -0.002486348054274501\n",
      "Finish epoch 66, total training cost is 8993.97554397583, time spent is 3.5844619274139404\n",
      "Finish epoch 67, total training cost is 8839.931129455566, time spent is 3.599494218826294\n",
      "Finish epoch 68, total training cost is 8579.13542175293, time spent is 3.7808682918548584\n",
      "Finish epoch 69, total training cost is 8294.772438049316, time spent is 3.9218809604644775\n",
      "Finish epoch 70, total training cost is 8642.346664428711, time spent is 4.147125005722046\n",
      "Training kappa score = 0.303407158102739\n",
      "Testing kappa score = 0.023761654765516504\n",
      "Finish epoch 71, total training cost is 8734.773582458496, time spent is 4.264233350753784\n",
      "Finish epoch 72, total training cost is 8451.04001235962, time spent is 4.382421255111694\n",
      "Finish epoch 73, total training cost is 8324.490592956543, time spent is 4.325876474380493\n",
      "Finish epoch 74, total training cost is 8168.246192932129, time spent is 4.298507928848267\n",
      "Finish epoch 75, total training cost is 7943.543605804443, time spent is 4.25752854347229\n",
      "Training kappa score = 0.4511598822386409\n",
      "Testing kappa score = 0.013965688846927171\n",
      "Finish epoch 76, total training cost is 8148.507354736328, time spent is 4.420738697052002\n",
      "Finish epoch 77, total training cost is 7803.743396759033, time spent is 4.37917423248291\n",
      "Finish epoch 78, total training cost is 8134.0634841918945, time spent is 4.2898218631744385\n",
      "Finish epoch 79, total training cost is 8103.38533782959, time spent is 4.438058137893677\n",
      "Finish epoch 80, total training cost is 7512.120136260986, time spent is 4.4993555545806885\n",
      "Training kappa score = 0.5670357409962266\n",
      "Testing kappa score = 0.034342736924636186\n",
      "Finish epoch 81, total training cost is 7544.196479797363, time spent is 4.467595815658569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 82, total training cost is 7750.639678955078, time spent is 4.407215356826782\n",
      "Finish epoch 83, total training cost is 7731.642803192139, time spent is 4.493838548660278\n",
      "Finish epoch 84, total training cost is 7545.045570373535, time spent is 4.450666427612305\n",
      "Finish epoch 85, total training cost is 7480.469806671143, time spent is 4.50229024887085\n",
      "Training kappa score = 0.6186165975650542\n",
      "Testing kappa score = 0.011464736237582929\n",
      "Finish epoch 86, total training cost is 7252.374355316162, time spent is 4.417397975921631\n",
      "Finish epoch 87, total training cost is 7393.136589050293, time spent is 4.451233386993408\n",
      "Finish epoch 88, total training cost is 7316.365749359131, time spent is 4.466912269592285\n",
      "Finish epoch 89, total training cost is 7507.611297607422, time spent is 4.341086387634277\n",
      "Finish epoch 90, total training cost is 7084.871315002441, time spent is 4.359257698059082\n",
      "Training kappa score = 0.5963810002513196\n",
      "Testing kappa score = 0.03570471321077995\n",
      "Finish epoch 91, total training cost is 7009.717384338379, time spent is 4.5489068031311035\n",
      "Finish epoch 92, total training cost is 6801.965808868408, time spent is 4.461682558059692\n",
      "Finish epoch 93, total training cost is 6981.148124694824, time spent is 4.289062976837158\n",
      "Finish epoch 94, total training cost is 6684.700084686279, time spent is 4.371438503265381\n",
      "Finish epoch 95, total training cost is 6707.511459350586, time spent is 4.375018358230591\n",
      "Training kappa score = 0.30025036791127213\n",
      "Testing kappa score = 0.02471101010331833\n",
      "Finish epoch 96, total training cost is 6558.259632110596, time spent is 4.425810813903809\n",
      "Finish epoch 97, total training cost is 6674.522552490234, time spent is 4.326852798461914\n",
      "Finish epoch 98, total training cost is 6505.315189361572, time spent is 4.279312610626221\n",
      "Finish epoch 99, total training cost is 6675.630893707275, time spent is 4.393971681594849\n",
      "Finish epoch 100, total training cost is 6647.5231285095215, time spent is 4.3091795444488525\n",
      "Training kappa score = 0.5545086370774446\n",
      "Testing kappa score = 0.009862474899854146\n",
      "                                               names  essay_id\n",
      "0  17, \"I going to write about a time when I went...         1\n",
      "1  12, \"One time I was patience it was when I wan...         2\n",
      "2  14, \"The time that I was patient was not long ...         3\n",
      "3  17, \"Im writing about the time I was patient a...         4\n",
      "4  15, \"Patience is when you are waiting for some...         5\n",
      "max_score is 30 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 429 \n",
      "mean sentence size: 135\n",
      "\n",
      "The length of score range is 31\n",
      "max sentence size: 474 \n",
      "mean sentence size: 176\n",
      "\n",
      "314\n",
      "The size of training data: 1232\n",
      "The size of testing data: 314\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230)]\n",
      "Finish epoch 1, total training cost is 109378.1342163086, time spent is 3.551025867462158\n",
      "Finish epoch 2, total training cost is 53653.997619628906, time spent is 3.2328946590423584\n",
      "Finish epoch 3, total training cost is 37513.763763427734, time spent is 2.941903829574585\n",
      "Finish epoch 4, total training cost is 30001.007598876953, time spent is 2.8678958415985107\n",
      "Finish epoch 5, total training cost is 24295.311515808105, time spent is 2.974849224090576\n",
      "Training kappa score = 0.2265189438327696\n",
      "Testing kappa score = 0.0009912007664357736\n",
      "Finish epoch 6, total training cost is 21369.224739074707, time spent is 2.9453516006469727\n",
      "Finish epoch 7, total training cost is 17089.91951751709, time spent is 3.032069683074951\n",
      "Finish epoch 8, total training cost is 14691.251441955566, time spent is 2.8874077796936035\n",
      "Finish epoch 9, total training cost is 12958.155139923096, time spent is 2.9823861122131348\n",
      "Finish epoch 10, total training cost is 11872.503784179688, time spent is 2.9249465465545654\n",
      "Training kappa score = 0.19242111458183697\n",
      "Testing kappa score = 0.02283978094534944\n",
      "Finish epoch 11, total training cost is 9450.671585083008, time spent is 2.981651782989502\n",
      "Finish epoch 12, total training cost is 8403.04711151123, time spent is 2.9954707622528076\n",
      "Finish epoch 13, total training cost is 7394.9962158203125, time spent is 3.0599467754364014\n",
      "Finish epoch 14, total training cost is 6884.978527069092, time spent is 2.967384099960327\n",
      "Finish epoch 15, total training cost is 6297.867542266846, time spent is 3.0002198219299316\n",
      "Training kappa score = 0.22100924597534854\n",
      "Testing kappa score = 0.026887620036042503\n",
      "Finish epoch 16, total training cost is 5689.958599090576, time spent is 2.9902894496917725\n",
      "Finish epoch 17, total training cost is 5445.10160446167, time spent is 2.9959938526153564\n",
      "Finish epoch 18, total training cost is 5225.86714553833, time spent is 2.8912465572357178\n",
      "Finish epoch 19, total training cost is 5175.326679229736, time spent is 2.9661927223205566\n",
      "Finish epoch 20, total training cost is 4574.641185760498, time spent is 2.9100277423858643\n",
      "Training kappa score = 0.2722865003327194\n",
      "Testing kappa score = -0.017552189851784528\n",
      "Finish epoch 21, total training cost is 4304.592945098877, time spent is 2.9605047702789307\n",
      "Finish epoch 22, total training cost is 4275.238483428955, time spent is 2.955536127090454\n",
      "Finish epoch 23, total training cost is 4099.398643493652, time spent is 2.9715964794158936\n",
      "Finish epoch 24, total training cost is 4176.085750579834, time spent is 2.8929076194763184\n",
      "Finish epoch 25, total training cost is 3941.4665870666504, time spent is 2.940793514251709\n",
      "Training kappa score = 0.43869712563752306\n",
      "Testing kappa score = 0.06400894564641757\n",
      "Finish epoch 26, total training cost is 4018.0841484069824, time spent is 2.9839696884155273\n",
      "Finish epoch 27, total training cost is 3989.464797973633, time spent is 2.9690864086151123\n",
      "Finish epoch 28, total training cost is 3987.925163269043, time spent is 2.981837511062622\n",
      "Finish epoch 29, total training cost is 3901.3818016052246, time spent is 3.0354866981506348\n",
      "Finish epoch 30, total training cost is 3883.4711112976074, time spent is 3.014503002166748\n",
      "Training kappa score = 0.27770464569855935\n",
      "Testing kappa score = 0.06150479721985613\n",
      "Finish epoch 31, total training cost is 3655.257484436035, time spent is 2.8824479579925537\n",
      "Finish epoch 32, total training cost is 3498.6559715270996, time spent is 2.942471981048584\n",
      "Finish epoch 33, total training cost is 3428.9389877319336, time spent is 3.0526185035705566\n",
      "Finish epoch 34, total training cost is 3424.8847427368164, time spent is 2.822007656097412\n",
      "Finish epoch 35, total training cost is 3482.4041328430176, time spent is 2.9656031131744385\n",
      "Training kappa score = 0.5058305017333931\n",
      "Testing kappa score = 0.10349425653507172\n",
      "Finish epoch 36, total training cost is 3439.8500061035156, time spent is 3.0373239517211914\n",
      "Finish epoch 37, total training cost is 3302.976005554199, time spent is 2.931727647781372\n",
      "Finish epoch 38, total training cost is 3298.262462615967, time spent is 2.9823124408721924\n",
      "Finish epoch 39, total training cost is 3272.062255859375, time spent is 2.8259308338165283\n",
      "Finish epoch 40, total training cost is 3222.9163246154785, time spent is 2.9942305088043213\n",
      "Training kappa score = 0.6354332828425004\n",
      "Testing kappa score = 0.08111327936843993\n",
      "Finish epoch 41, total training cost is 3151.1228218078613, time spent is 3.013979911804199\n",
      "Finish epoch 42, total training cost is 3169.7071743011475, time spent is 3.06404972076416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 43, total training cost is 3145.0393676757812, time spent is 2.9377644062042236\n",
      "Finish epoch 44, total training cost is 3116.6124267578125, time spent is 2.9420840740203857\n",
      "Finish epoch 45, total training cost is 3079.987232208252, time spent is 2.9367010593414307\n",
      "Training kappa score = 0.667396651077204\n",
      "Testing kappa score = 0.08792848927775943\n",
      "Finish epoch 46, total training cost is 3073.6317176818848, time spent is 3.0624001026153564\n",
      "Finish epoch 47, total training cost is 3086.739070892334, time spent is 2.973160982131958\n",
      "Finish epoch 48, total training cost is 3068.588535308838, time spent is 2.9452667236328125\n",
      "Finish epoch 49, total training cost is 3001.579484939575, time spent is 3.0578155517578125\n",
      "Finish epoch 50, total training cost is 2958.3868980407715, time spent is 2.9917752742767334\n",
      "Training kappa score = 0.6618401021161694\n",
      "Testing kappa score = 0.08893508552834528\n",
      "Finish epoch 51, total training cost is 2966.9284172058105, time spent is 2.8602206707000732\n",
      "Finish epoch 52, total training cost is 2953.725854873657, time spent is 2.327529191970825\n",
      "Finish epoch 53, total training cost is 2944.859878540039, time spent is 2.379626750946045\n",
      "Finish epoch 54, total training cost is 2916.0119915008545, time spent is 2.3163836002349854\n",
      "Finish epoch 55, total training cost is 2959.1353645324707, time spent is 2.2943644523620605\n",
      "Training kappa score = 0.7266859521385147\n",
      "Testing kappa score = 0.08906005839384334\n",
      "Finish epoch 56, total training cost is 2902.7575855255127, time spent is 2.5020573139190674\n",
      "Finish epoch 57, total training cost is 2933.393798828125, time spent is 2.3304412364959717\n",
      "Finish epoch 58, total training cost is 2836.2592544555664, time spent is 2.3794353008270264\n",
      "Finish epoch 59, total training cost is 2852.66801071167, time spent is 2.309746742248535\n",
      "Finish epoch 60, total training cost is 2821.469835281372, time spent is 2.3069772720336914\n",
      "Training kappa score = 0.7417597629127537\n",
      "Testing kappa score = 0.09123878658938811\n",
      "Finish epoch 61, total training cost is 2824.5383014678955, time spent is 2.4280002117156982\n",
      "Finish epoch 62, total training cost is 2802.3489532470703, time spent is 2.456310987472534\n",
      "Finish epoch 63, total training cost is 2783.959342956543, time spent is 2.3417160511016846\n",
      "Finish epoch 64, total training cost is 2805.1067237854004, time spent is 2.3175694942474365\n",
      "Finish epoch 65, total training cost is 2789.094747543335, time spent is 2.337599277496338\n",
      "Training kappa score = 0.7852250309150761\n",
      "Testing kappa score = 0.0881791968321416\n",
      "Finish epoch 66, total training cost is 2782.1842937469482, time spent is 2.4082837104797363\n",
      "Finish epoch 67, total training cost is 2804.7525901794434, time spent is 2.445890188217163\n",
      "Finish epoch 68, total training cost is 2765.043655395508, time spent is 2.4331624507904053\n",
      "Finish epoch 69, total training cost is 2752.90691947937, time spent is 2.505793809890747\n",
      "Finish epoch 70, total training cost is 2770.9209632873535, time spent is 2.423884630203247\n",
      "Training kappa score = 0.7647696564880245\n",
      "Testing kappa score = 0.08969895968613706\n",
      "Finish epoch 71, total training cost is 2774.443769454956, time spent is 2.4428937435150146\n",
      "Finish epoch 72, total training cost is 2751.1459426879883, time spent is 2.408003568649292\n",
      "Finish epoch 73, total training cost is 2752.2899074554443, time spent is 2.4328935146331787\n",
      "Finish epoch 74, total training cost is 2727.819040298462, time spent is 2.517019033432007\n",
      "Finish epoch 75, total training cost is 2757.735246658325, time spent is 2.4108662605285645\n",
      "Training kappa score = 0.7872880981075541\n",
      "Testing kappa score = 0.067698878976865\n",
      "Finish epoch 76, total training cost is 2721.9772777557373, time spent is 2.4284732341766357\n",
      "Finish epoch 77, total training cost is 2711.9973220825195, time spent is 2.421250104904175\n",
      "Finish epoch 78, total training cost is 2676.499765396118, time spent is 2.400648832321167\n",
      "Finish epoch 79, total training cost is 2697.1783809661865, time spent is 2.6703360080718994\n",
      "Finish epoch 80, total training cost is 2693.5077724456787, time spent is 2.588179111480713\n",
      "Training kappa score = 0.7857153231873996\n",
      "Testing kappa score = 0.08574673838047175\n",
      "Finish epoch 81, total training cost is 2729.436719894409, time spent is 2.462675094604492\n",
      "Finish epoch 82, total training cost is 2689.5615425109863, time spent is 2.518446683883667\n",
      "Finish epoch 83, total training cost is 2667.7582416534424, time spent is 2.464832067489624\n",
      "Finish epoch 84, total training cost is 2683.4963245391846, time spent is 2.459674119949341\n",
      "Finish epoch 85, total training cost is 2719.329023361206, time spent is 2.5271875858306885\n",
      "Training kappa score = 0.812879499960528\n",
      "Testing kappa score = 0.07011837751777461\n",
      "Finish epoch 86, total training cost is 2712.660842895508, time spent is 2.424271583557129\n",
      "Finish epoch 87, total training cost is 2719.6950092315674, time spent is 2.4349772930145264\n",
      "Finish epoch 88, total training cost is 2737.804594039917, time spent is 2.4462177753448486\n",
      "Finish epoch 89, total training cost is 2680.7262744903564, time spent is 2.431299924850464\n",
      "Finish epoch 90, total training cost is 2721.3322467803955, time spent is 2.460780620574951\n",
      "Training kappa score = 0.8040462706148445\n",
      "Testing kappa score = 0.06878259255869534\n",
      "                                               names  essay_id\n",
      "0  17, \"I going to write about a time when I went...         1\n",
      "1  12, \"One time I was patience it was when I wan...         2\n",
      "2  14, \"The time that I was patient was not long ...         3\n",
      "3  17, \"Im writing about the time I was patient a...         4\n",
      "4  15, \"Patience is when you are waiting for some...         5\n",
      "max_score is 30 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 659 \n",
      "mean sentence size: 176\n",
      "\n",
      "The length of score range is 31\n",
      "max sentence size: 474 \n",
      "mean sentence size: 176\n",
      "\n",
      "314\n",
      "The size of training data: 1232\n",
      "The size of testing data: 314\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230)]\n",
      "Finish epoch 1, total training cost is 221488.12622070312, time spent is 3.9995226860046387\n",
      "Finish epoch 2, total training cost is 136564.56805419922, time spent is 4.323463201522827\n",
      "Finish epoch 3, total training cost is 86186.7271118164, time spent is 3.933298349380493\n",
      "Finish epoch 4, total training cost is 61441.833557128906, time spent is 4.19484543800354\n",
      "Finish epoch 5, total training cost is 50296.658447265625, time spent is 3.819199562072754\n",
      "Training kappa score = -0.03124456186423208\n",
      "Testing kappa score = 0.0031951738773038363\n",
      "Finish epoch 6, total training cost is 44715.65017700195, time spent is 3.7480599880218506\n",
      "Finish epoch 7, total training cost is 38344.10762023926, time spent is 3.802302360534668\n",
      "Finish epoch 8, total training cost is 35833.72897338867, time spent is 3.9432337284088135\n",
      "Finish epoch 9, total training cost is 33763.46408081055, time spent is 3.8156661987304688\n",
      "Finish epoch 10, total training cost is 31782.39195251465, time spent is 3.733144998550415\n",
      "Training kappa score = 0.1547437011883146\n",
      "Testing kappa score = 0.0033632051954033404\n",
      "Finish epoch 11, total training cost is 29811.870162963867, time spent is 3.6666226387023926\n",
      "Finish epoch 12, total training cost is 29367.937088012695, time spent is 3.9791810512542725\n",
      "Finish epoch 13, total training cost is 27566.27864074707, time spent is 3.718233823776245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 14, total training cost is 25973.388214111328, time spent is 3.8016529083251953\n",
      "Finish epoch 15, total training cost is 24485.328582763672, time spent is 3.7289929389953613\n",
      "Training kappa score = 0.24779449834674216\n",
      "Testing kappa score = 0.0020665367533473367\n",
      "Finish epoch 16, total training cost is 23682.577713012695, time spent is 4.055936098098755\n",
      "Finish epoch 17, total training cost is 23147.363555908203, time spent is 3.865281105041504\n",
      "Finish epoch 18, total training cost is 22391.457397460938, time spent is 3.7681193351745605\n",
      "Finish epoch 19, total training cost is 22079.16438293457, time spent is 3.80971097946167\n",
      "Finish epoch 20, total training cost is 21091.307250976562, time spent is 3.850612163543701\n",
      "Training kappa score = 0.3070223477058006\n",
      "Testing kappa score = -0.05141492444759832\n",
      "Finish epoch 21, total training cost is 20698.900848388672, time spent is 3.7395622730255127\n",
      "Finish epoch 22, total training cost is 20280.40461730957, time spent is 3.7845094203948975\n",
      "Finish epoch 23, total training cost is 19402.196090698242, time spent is 3.738764524459839\n",
      "Finish epoch 24, total training cost is 18903.806365966797, time spent is 3.845738410949707\n",
      "Finish epoch 25, total training cost is 17889.246780395508, time spent is 3.7810542583465576\n",
      "Training kappa score = 0.11320589143803939\n",
      "Testing kappa score = 0.014956151665832085\n",
      "Finish epoch 26, total training cost is 18242.842041015625, time spent is 3.745800256729126\n",
      "Finish epoch 27, total training cost is 17064.79653930664, time spent is 3.666407823562622\n",
      "Finish epoch 28, total training cost is 16780.649169921875, time spent is 3.9370005130767822\n",
      "Finish epoch 29, total training cost is 16408.812477111816, time spent is 3.759633779525757\n",
      "Finish epoch 30, total training cost is 16272.01976776123, time spent is 3.673626184463501\n",
      "Training kappa score = 0.04045256409756848\n",
      "Testing kappa score = -0.03777975827268287\n",
      "Finish epoch 31, total training cost is 15733.762260437012, time spent is 3.736128568649292\n",
      "Finish epoch 32, total training cost is 15283.345718383789, time spent is 3.815014362335205\n",
      "Finish epoch 33, total training cost is 15127.15396118164, time spent is 3.644293785095215\n",
      "Finish epoch 34, total training cost is 14914.188484191895, time spent is 3.685530424118042\n",
      "Finish epoch 35, total training cost is 13963.77936553955, time spent is 3.675571918487549\n",
      "Training kappa score = 0.3693714759313551\n",
      "Testing kappa score = -0.033327101288379346\n",
      "Finish epoch 36, total training cost is 14318.454208374023, time spent is 3.7806248664855957\n",
      "Finish epoch 37, total training cost is 14268.16423034668, time spent is 4.023248672485352\n",
      "Finish epoch 38, total training cost is 13645.845901489258, time spent is 3.8537042140960693\n",
      "Finish epoch 39, total training cost is 13212.180099487305, time spent is 3.6460487842559814\n",
      "Finish epoch 40, total training cost is 12808.971908569336, time spent is 3.7719531059265137\n",
      "Training kappa score = 0.2643246850782166\n",
      "Testing kappa score = -0.00561648120261915\n",
      "Finish epoch 41, total training cost is 12550.723609924316, time spent is 3.673020601272583\n",
      "Finish epoch 42, total training cost is 12317.21477508545, time spent is 3.681976556777954\n",
      "Finish epoch 43, total training cost is 12272.896957397461, time spent is 3.693206548690796\n",
      "Finish epoch 44, total training cost is 12143.288414001465, time spent is 3.740081787109375\n",
      "Finish epoch 45, total training cost is 11958.741539001465, time spent is 3.602389335632324\n",
      "Training kappa score = 0.3652491573942238\n",
      "Testing kappa score = -0.001413637201891138\n",
      "Finish epoch 46, total training cost is 11526.164001464844, time spent is 3.5394370555877686\n",
      "Finish epoch 47, total training cost is 11062.713653564453, time spent is 3.6335980892181396\n",
      "Finish epoch 48, total training cost is 10559.003776550293, time spent is 3.674328327178955\n",
      "Finish epoch 49, total training cost is 10753.270011901855, time spent is 3.0644454956054688\n",
      "Finish epoch 50, total training cost is 10555.55362701416, time spent is 3.797402858734131\n",
      "Training kappa score = 0.15820653083820413\n",
      "Testing kappa score = 0.0003531255812824208\n",
      "Finish epoch 51, total training cost is 10441.469818115234, time spent is 4.176220893859863\n",
      "Finish epoch 52, total training cost is 10205.741828918457, time spent is 4.129030704498291\n",
      "Finish epoch 53, total training cost is 9992.760269165039, time spent is 4.0028839111328125\n",
      "Finish epoch 54, total training cost is 9843.349670410156, time spent is 3.8973684310913086\n",
      "Finish epoch 55, total training cost is 9727.806045532227, time spent is 3.5835890769958496\n",
      "Training kappa score = 0.34250108605214524\n",
      "Testing kappa score = -0.06012832000020896\n",
      "Finish epoch 56, total training cost is 9523.267051696777, time spent is 3.5269572734832764\n",
      "Finish epoch 57, total training cost is 9452.452323913574, time spent is 3.4422788619995117\n",
      "Finish epoch 58, total training cost is 9376.996856689453, time spent is 3.568131446838379\n",
      "Finish epoch 59, total training cost is 9483.080825805664, time spent is 3.822770833969116\n",
      "Finish epoch 60, total training cost is 9424.035301208496, time spent is 3.4611501693725586\n",
      "Training kappa score = 0.19938506250342403\n",
      "Testing kappa score = -0.012745192633392133\n",
      "Finish epoch 61, total training cost is 9385.175430297852, time spent is 2.9423136711120605\n",
      "Finish epoch 62, total training cost is 8586.678733825684, time spent is 2.9597182273864746\n",
      "Finish epoch 63, total training cost is 8741.241607666016, time spent is 2.968863010406494\n",
      "Finish epoch 64, total training cost is 8662.760139465332, time spent is 3.3078763484954834\n",
      "Finish epoch 65, total training cost is 8244.38899230957, time spent is 4.2892539501190186\n",
      "Training kappa score = 0.4771584472002147\n",
      "Testing kappa score = 0.011167095175878439\n",
      "Finish epoch 66, total training cost is 7928.839389801025, time spent is 3.9056479930877686\n",
      "Finish epoch 67, total training cost is 7699.855930328369, time spent is 4.12603497505188\n",
      "Finish epoch 68, total training cost is 6702.6958084106445, time spent is 3.7556140422821045\n",
      "Finish epoch 69, total training cost is 5211.879455566406, time spent is 4.164120674133301\n",
      "Finish epoch 70, total training cost is 4531.460762023926, time spent is 4.255634546279907\n",
      "Training kappa score = 0.3402320156601838\n",
      "Testing kappa score = -0.004854895332099041\n",
      "Finish epoch 71, total training cost is 4446.671733856201, time spent is 4.109400510787964\n",
      "Finish epoch 72, total training cost is 4291.454795837402, time spent is 4.063037395477295\n",
      "Finish epoch 73, total training cost is 4104.245246887207, time spent is 4.07663369178772\n",
      "Finish epoch 74, total training cost is 4134.743072509766, time spent is 4.196391582489014\n",
      "Finish epoch 75, total training cost is 4106.994976043701, time spent is 4.094929933547974\n",
      "Training kappa score = 0.05816035780132167\n",
      "Testing kappa score = 0.011805514886409019\n",
      "Finish epoch 76, total training cost is 4047.2303047180176, time spent is 3.8468801975250244\n",
      "Finish epoch 77, total training cost is 3917.6153678894043, time spent is 4.3197338581085205\n",
      "Finish epoch 78, total training cost is 3871.69766998291, time spent is 3.75066876411438\n",
      "Finish epoch 79, total training cost is 3827.308780670166, time spent is 3.774113416671753\n",
      "Finish epoch 80, total training cost is 3746.447090148926, time spent is 4.126375198364258\n",
      "Training kappa score = 0.0027771896448066657\n",
      "Testing kappa score = -0.0011008459577783736\n",
      "                                               names  essay_id\n",
      "0  17, \"I going to write about a time when I went...         1\n",
      "1  12, \"One time I was patience it was when I wan...         2\n",
      "2  14, \"The time that I was patient was not long ...         3\n",
      "3  17, \"Im writing about the time I was patient a...         4\n",
      "4  15, \"Patience is when you are waiting for some...         5\n",
      "max_score is 30 \t min_score is 0\n",
      "\n",
      "==> glove is loaded\n",
      "max sentence size: 646 \n",
      "mean sentence size: 175\n",
      "\n",
      "The length of score range is 31\n",
      "max sentence size: 474 \n",
      "mean sentence size: 176\n",
      "\n",
      "314\n",
      "The size of training data: 1232\n",
      "The size of testing data: 314\n",
      "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 1, total training cost is 171016.28448486328, time spent is 3.706602096557617\n",
      "Finish epoch 2, total training cost is 81651.0484008789, time spent is 3.81820011138916\n",
      "Finish epoch 3, total training cost is 61514.73165893555, time spent is 3.82863712310791\n",
      "Finish epoch 4, total training cost is 54160.086669921875, time spent is 3.4851162433624268\n",
      "Finish epoch 5, total training cost is 49437.29476928711, time spent is 3.7843129634857178\n",
      "Training kappa score = 0.383461468752862\n",
      "Testing kappa score = -0.016640893684599645\n",
      "Finish epoch 6, total training cost is 42749.50100708008, time spent is 2.920825719833374\n",
      "Finish epoch 7, total training cost is 38023.21376037598, time spent is 2.944030284881592\n",
      "Finish epoch 8, total training cost is 34963.724197387695, time spent is 2.9228172302246094\n",
      "Finish epoch 9, total training cost is 33171.61065673828, time spent is 2.9184091091156006\n",
      "Finish epoch 10, total training cost is 31286.024963378906, time spent is 2.9079694747924805\n",
      "Training kappa score = 0.4021101069031363\n",
      "Testing kappa score = 0.012110701287281644\n",
      "Finish epoch 11, total training cost is 27586.538360595703, time spent is 2.9035370349884033\n",
      "Finish epoch 12, total training cost is 28091.405364990234, time spent is 2.908158540725708\n",
      "Finish epoch 13, total training cost is 26127.92431640625, time spent is 2.899317502975464\n",
      "Finish epoch 14, total training cost is 25622.56234741211, time spent is 3.853705406188965\n",
      "Finish epoch 15, total training cost is 24712.823944091797, time spent is 3.990323305130005\n",
      "Training kappa score = 0.5104592772586163\n",
      "Testing kappa score = 0.07267753250566489\n",
      "Finish epoch 16, total training cost is 23551.460205078125, time spent is 2.9145724773406982\n",
      "Finish epoch 17, total training cost is 22604.019989013672, time spent is 2.903911590576172\n",
      "Finish epoch 18, total training cost is 21634.266189575195, time spent is 3.3230948448181152\n",
      "Finish epoch 19, total training cost is 20577.088256835938, time spent is 3.6872849464416504\n",
      "Finish epoch 20, total training cost is 20550.257843017578, time spent is 3.061417818069458\n",
      "Training kappa score = 0.244880789743827\n",
      "Testing kappa score = 0.0658597918916658\n",
      "Finish epoch 21, total training cost is 20243.200653076172, time spent is 2.89699125289917\n",
      "Finish epoch 22, total training cost is 19425.85028076172, time spent is 2.92204213142395\n",
      "Finish epoch 23, total training cost is 18656.554313659668, time spent is 3.5153660774230957\n",
      "Finish epoch 24, total training cost is 19108.57110595703, time spent is 3.911827325820923\n",
      "Finish epoch 25, total training cost is 17203.58692932129, time spent is 3.921353816986084\n",
      "Training kappa score = 0.2519638379180623\n",
      "Testing kappa score = 0.07374631268436616\n",
      "Finish epoch 26, total training cost is 17430.47071838379, time spent is 3.3637895584106445\n",
      "Finish epoch 27, total training cost is 16600.449249267578, time spent is 4.017592430114746\n",
      "Finish epoch 28, total training cost is 16324.716033935547, time spent is 4.004692554473877\n",
      "Finish epoch 29, total training cost is 15403.44049835205, time spent is 3.8880956172943115\n",
      "Finish epoch 30, total training cost is 15071.293159484863, time spent is 3.90822172164917\n",
      "Training kappa score = 0.3289586454837602\n",
      "Testing kappa score = 0.05652531967459018\n",
      "Finish epoch 31, total training cost is 15027.401748657227, time spent is 2.9134254455566406\n",
      "Finish epoch 32, total training cost is 14229.82780456543, time spent is 2.911383867263794\n",
      "Finish epoch 33, total training cost is 14007.74870300293, time spent is 3.7362983226776123\n",
      "Finish epoch 34, total training cost is 13083.049995422363, time spent is 3.941553831100464\n",
      "Finish epoch 35, total training cost is 12907.009269714355, time spent is 3.963590383529663\n",
      "Training kappa score = 0.14765750404417088\n",
      "Testing kappa score = 0.06264169082214388\n",
      "Finish epoch 36, total training cost is 12721.977737426758, time spent is 2.8989171981811523\n",
      "Finish epoch 37, total training cost is 12157.19775390625, time spent is 3.8046388626098633\n",
      "Finish epoch 38, total training cost is 11283.208801269531, time spent is 3.2245383262634277\n",
      "Finish epoch 39, total training cost is 11376.577278137207, time spent is 2.9395482540130615\n",
      "Finish epoch 40, total training cost is 10711.731491088867, time spent is 2.9055588245391846\n",
      "Training kappa score = 0.1421930784294494\n",
      "Testing kappa score = 0.0006279196106105944\n",
      "Finish epoch 41, total training cost is 10991.511840820312, time spent is 3.596456527709961\n",
      "Finish epoch 42, total training cost is 10500.560073852539, time spent is 4.018863916397095\n",
      "Finish epoch 43, total training cost is 10109.175498962402, time spent is 3.868267774581909\n",
      "Finish epoch 44, total training cost is 10183.14682006836, time spent is 3.9133453369140625\n",
      "Finish epoch 45, total training cost is 10083.757133483887, time spent is 3.8158342838287354\n",
      "Training kappa score = 0.14953123144752856\n",
      "Testing kappa score = 0.05092537115554774\n",
      "Finish epoch 46, total training cost is 9817.996780395508, time spent is 2.984814405441284\n",
      "Finish epoch 47, total training cost is 9533.861724853516, time spent is 2.9922099113464355\n",
      "Finish epoch 48, total training cost is 9318.961349487305, time spent is 3.902219772338867\n",
      "Finish epoch 49, total training cost is 9094.200637817383, time spent is 3.8953678607940674\n",
      "Finish epoch 50, total training cost is 8729.983108520508, time spent is 3.836569309234619\n",
      "Training kappa score = 0.28434249739218165\n",
      "Testing kappa score = -0.013823377298021411\n",
      "Finish epoch 51, total training cost is 8357.103538513184, time spent is 3.34763765335083\n",
      "Finish epoch 52, total training cost is 8258.8127784729, time spent is 3.708970785140991\n",
      "Finish epoch 53, total training cost is 8237.084465026855, time spent is 3.989365577697754\n",
      "Finish epoch 54, total training cost is 8264.501846313477, time spent is 3.8616127967834473\n",
      "Finish epoch 55, total training cost is 7999.338310241699, time spent is 3.9964118003845215\n",
      "Training kappa score = -0.09647446525189274\n",
      "Testing kappa score = -0.0454342914828767\n",
      "Finish epoch 56, total training cost is 7670.23318862915, time spent is 3.547311782836914\n",
      "Finish epoch 57, total training cost is 7762.019630432129, time spent is 3.8783979415893555\n",
      "Finish epoch 58, total training cost is 7812.427787780762, time spent is 3.9726734161376953\n",
      "Finish epoch 59, total training cost is 7706.340419769287, time spent is 3.788362503051758\n",
      "Finish epoch 60, total training cost is 7469.221237182617, time spent is 3.665853261947632\n",
      "Training kappa score = -0.0029264782209705853\n",
      "Testing kappa score = 0.017458880712633795\n",
      "Finish epoch 61, total training cost is 7438.712982177734, time spent is 3.503864049911499\n",
      "Finish epoch 62, total training cost is 7234.978553771973, time spent is 3.4060895442962646\n",
      "Finish epoch 63, total training cost is 7210.198474884033, time spent is 3.736968755722046\n",
      "Finish epoch 64, total training cost is 6896.524921417236, time spent is 3.505851984024048\n",
      "Finish epoch 65, total training cost is 6834.959976196289, time spent is 3.9212377071380615\n",
      "Training kappa score = 0.2830015668261451\n",
      "Testing kappa score = 0.06348539453784496\n",
      "Finish epoch 66, total training cost is 6824.317554473877, time spent is 3.68341064453125\n",
      "Finish epoch 67, total training cost is 6605.811374664307, time spent is 3.8773887157440186\n",
      "Finish epoch 68, total training cost is 6870.208694458008, time spent is 3.9499502182006836\n",
      "Finish epoch 69, total training cost is 6687.721706390381, time spent is 3.848485231399536\n",
      "Finish epoch 70, total training cost is 6460.825645446777, time spent is 4.0299718379974365\n",
      "Training kappa score = 0.2430995720842838\n",
      "Testing kappa score = 0.039203473400564515\n",
      "Finish epoch 71, total training cost is 6427.472743988037, time spent is 4.005542516708374\n",
      "Finish epoch 72, total training cost is 6263.9862060546875, time spent is 3.9812819957733154\n",
      "Finish epoch 73, total training cost is 6527.095851898193, time spent is 3.7778024673461914\n",
      "Finish epoch 74, total training cost is 6285.398574829102, time spent is 4.026690244674683\n",
      "Finish epoch 75, total training cost is 5959.990474700928, time spent is 3.597999334335327\n",
      "Training kappa score = 0.24871706131532278\n",
      "Testing kappa score = -0.02538808216024835\n",
      "Finish epoch 76, total training cost is 5815.925064086914, time spent is 3.8441033363342285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 77, total training cost is 5928.319274902344, time spent is 3.8080549240112305\n",
      "Finish epoch 78, total training cost is 5850.366592407227, time spent is 3.8966474533081055\n",
      "Finish epoch 79, total training cost is 5295.135257720947, time spent is 3.9524412155151367\n",
      "Finish epoch 80, total training cost is 5025.569606781006, time spent is 4.035022497177124\n",
      "Training kappa score = 0.2351620045316889\n",
      "Testing kappa score = -0.018046523123638858\n"
     ]
    }
   ],
   "source": [
    "import data_utils_adv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from qwk import quadratic_weighted_kappa\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "print( 'start to load flags\\n')\n",
    "\n",
    "# flags\n",
    "# tf.flags.DEFINE_float(\"epsilon\", 0.1, \"Epsilon value for Adam Optimizer.\")\n",
    "# tf.flags.DEFINE_float(\"l2_lambda\", 0.3, \"Lambda for l2 loss.\")\n",
    "# tf.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate\")\n",
    "# tf.flags.DEFINE_float(\"max_grad_norm\", 10.0, \"Clip gradients to this norm.\")\n",
    "# tf.flags.DEFINE_float(\"keep_prob\", 0.9, \"Keep probability for dropout\")\n",
    "# tf.flags.DEFINE_integer(\"evaluation_interval\", 5, \"Evaluate and print results every x epochs\")\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 15, \"Batch size for training.\")\n",
    "# tf.flags.DEFINE_integer(\"feature_size\", 100, \"Feature size\")\n",
    "# tf.flags.DEFINE_integer(\"num_samples\", 1, \"Number of samples selected from training for each score\")\n",
    "# tf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "# tf.flags.DEFINE_integer(\"epochs\", 200, \"Number of epochs to train for.\")\n",
    "# tf.flags.DEFINE_integer(\"embedding_size\", 300, \"Embedding size for embedding matrices.\")\n",
    "# tf.flags.DEFINE_integer(\"essay_set_id\", 1, \"essay set id, 1 <= id <= 8\")\n",
    "# tf.flags.DEFINE_integer(\"token_num\", 42, \"The number of token in glove (6, 42)\")\n",
    "# tf.flags.DEFINE_boolean(\"gated_addressing\", False, \"Simple gated addressing\")\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"is_regression\", False, \"The output is regression or classification\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "# # hyper-parameters\n",
    "# FLAGS = tf.flags.FLAGS\n",
    "\n",
    "early_stop_count = 0\n",
    "max_step_count = 10\n",
    "is_regression = False\n",
    "gated_addressing = False\n",
    "essay_set_id = 7\n",
    "batch_size = 15\n",
    "embedding_size = 300\n",
    "feature_size = 100\n",
    "l2_lambda = 0.3\n",
    "hops = 3\n",
    "reader = 'bow' # gru may not work\n",
    "epochs = 200\n",
    "num_samples = 1\n",
    "num_tokens = 42\n",
    "test_batch_size = batch_size\n",
    "random_state = 0\n",
    "\n",
    "if is_regression:\n",
    "    from memn2n_kv_regression import MemN2N_KV\n",
    "else:\n",
    "    from memn2n_kv import MemN2N_KV\n",
    "# print flags info\n",
    "orig_stdout = sys.stdout\n",
    "timestamp = time.strftime(\"%b_%d_%Y_%H:%M:%S\", time.localtime())\n",
    "folder_name = 'essay_set_{}_cv_{}_{}'.format(essay_set_id, num_samples, timestamp)\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs/adversary_training/\", folder_name))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# save output to a file\n",
    "#f = file(out_dir+'/out.txt', 'w')\n",
    "#sys.stdout = f\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "# print(\"\\nParameters:\")\n",
    "# for key in sorted(FLAGS.__flags.keys()):\n",
    "#     print(\"{}={}\".format(key, getattr(FLAGS, key)))\n",
    "# print(\"\")\n",
    "\n",
    "# with open(out_dir+'/params', 'w') as f:\n",
    "#     for key in sorted(FLAGS.__flags.keys()):\n",
    "#         f.write(\"{}={}\".format(key, getattr(FLAGS, key)))\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "        \n",
    "import os \n",
    "files = os.listdir('train_adv/prompt7/')\n",
    "for testcase in files:\n",
    "    training_path = 'train_adv/prompt7/'+testcase\n",
    "    \n",
    "    if testcase == 'noDisfluency&grammar_7_train_valid_reduce.csv':\n",
    "        names = 'training_noDisGramm'\n",
    "    elif testcase == 'noDisfluency_7_train_valid_reduce.csv':\n",
    "        names = 'training_noDis'\n",
    "    elif testcase == 'mixture_all_7_train_valid_reduce.csv':\n",
    "        names = 'training_mixture'\n",
    "    elif testcase == 'nocontractions_7_train_valid.csv':\n",
    "        names = 'training_noContr'\n",
    "    elif testcase == 'all_nochange_7_train_valid.csv':\n",
    "        names = 'training_SynContr'\n",
    "        \n",
    "#     training_path = 'train_adv/prompt1/noDisfluency&grammar_1_train_valid_reduce.csv'\n",
    "    essay_list, resolved_scores, essay_id = data_utils_adv.load_training_data(training_path, essay_set_id)\n",
    "\n",
    "    # print(essay_id)\n",
    "    testing_path = 'aes_data/essay7/fold_0/test.txt'\n",
    "    essay_list_test, resolved_scores_test, essay_id_test = data_utils_adv.load_testing_data(testing_path, essay_set_id)\n",
    "\n",
    "    max_score = max(resolved_scores)\n",
    "    min_score = min(resolved_scores)\n",
    "    if essay_set_id == 7:\n",
    "        min_score, max_score = 0, 30\n",
    "    elif essay_set_id == 8:\n",
    "        min_score, max_score = 0, 60\n",
    "\n",
    "    print( 'max_score is {} \\t min_score is {}\\n'.format(max_score, min_score))\n",
    "    with open(out_dir+'/params', 'a') as f:\n",
    "        f.write('max_score is {} \\t min_score is {} \\n'.format(max_score, min_score))\n",
    "    \n",
    "    score_range = range(min_score, max_score+1)\n",
    "\n",
    "    #word_idx, _ = data_utils.build_vocab(essay_list, vocab_limit)\n",
    "\n",
    "    # load glove\n",
    "    word_idx, word2vec = data_utils_adv.load_glove(num_tokens, dim=embedding_size)\n",
    "\n",
    "    vocab_size = len(word_idx) + 1\n",
    "    # stat info on data set\n",
    "\n",
    "    sent_size_list = list(map(len, [essay for essay in essay_list]))\n",
    "    # print(\"sent size list\", sent_size_list)\n",
    "    max_sent_size = max(sent_size_list)\n",
    "    mean_sent_size = int(np.mean(sent_size_list))\n",
    "\n",
    "    print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
    "    with open(out_dir+'/params', 'a') as f:\n",
    "        f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
    "\n",
    "    print( 'The length of score range is {}'.format(len(score_range)))\n",
    "    E = data_utils_adv.vectorize_data(essay_list, word_idx, max_sent_size)\n",
    "\n",
    "    labeled_data = zip(E, resolved_scores, sent_size_list)\n",
    "    \n",
    "    # For Testing\n",
    "    sent_size_list_T = list(map(len, [essayT for essayT in essay_list_test]))\n",
    "    max_sent_size_T = max(sent_size_list_T)\n",
    "    mean_sent_size_T = int(np.mean(sent_size_list_T))\n",
    "\n",
    "    # max_sent_size_T = max_sent_size_T.ljust(122, ' ')\n",
    "    print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size_T, mean_sent_size_T))\n",
    "    with open(out_dir+'/params', 'a') as f:\n",
    "        f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size_T, mean_sent_size_T))\n",
    "\n",
    "    # print( 'The length of score range of Testing Daat is {}'.format(len(score_range)))\n",
    "    E_T = data_utils_adv.vectorize_data(essay_list_test, word_idx, max_sent_size_T)\n",
    "\n",
    "    labeled_data_T = zip(E_T, resolved_scores_test, sent_size_list_T)\n",
    "    \n",
    "    def train_step(m, e, s, ma):\n",
    "        start_time = time.time()\n",
    "        feed_dict = {\n",
    "            model._query: e,\n",
    "            model._memory_key: m,\n",
    "            model._score_encoding: s,\n",
    "            model._mem_attention_encoding: ma,\n",
    "            model.keep_prob: 0.9\n",
    "            #model.w_placeholder: word2vec\n",
    "        }\n",
    "        _, step, predict_op, cost = sess.run([train_op, global_step, model.predict_op, model.cost], feed_dict)\n",
    "        end_time = time.time()\n",
    "        time_spent = end_time - start_time\n",
    "        return predict_op, cost, time_spent\n",
    "\n",
    "    def test_step(e, m):\n",
    "        feed_dict = {\n",
    "            model._query: e,\n",
    "            model._memory_key: m,\n",
    "            model.keep_prob: 1\n",
    "            #model.w_placeholder: word2vec\n",
    "        }\n",
    "        preds, mem_attention_probs = sess.run([model.predict_op, model.mem_attention_probs], feed_dict)\n",
    "        if is_regression:\n",
    "            preds = np.clip(np.round(preds), min_score, max_score)\n",
    "            return preds, mem_attention_probs\n",
    "        else:\n",
    "            return preds, mem_attention_probs\n",
    "        fold_count = 0\n",
    "    # kf = KFold(n_splits=2, random_state=random_state)\n",
    "    early_stop_count = 0\n",
    "\n",
    "    trainE = []\n",
    "    train_scores = []\n",
    "    train_essay_id = []\n",
    "    best_kappa_scores = []\n",
    "\n",
    "    testE = []\n",
    "    test_scores = []\n",
    "    test_essay_id = []\n",
    "\n",
    "    print(len(E_T))\n",
    "    # print(len(resolved_scores_test))\n",
    "    # print(len(essay_list_test))\n",
    "    for test_index in range(len(essay_id_test)):\n",
    "        testE.append(E[test_index])\n",
    "    #     print(len(testE))\n",
    "        test_scores.append(resolved_scores_test[test_index])\n",
    "        test_essay_id.append(essay_id_test[test_index])\n",
    "\n",
    "    for train_index in range(len(essay_id)):\n",
    "    #     print(\"Train_index\", train_index)\n",
    "    #     result = [int(x) for x, in train_index[0]]\n",
    "    #     print(result)\n",
    "    #     early_stop_count = 0\n",
    "    #     fold_count += 1\n",
    "    #     trainE = []\n",
    "    #     testE = []\n",
    "    #     train_scores = []\n",
    "    #     test_scores = []\n",
    "    #     train_essay_id = []\n",
    "    #     test_essay_id = []\n",
    "\n",
    "        trainE.append(E[train_index])\n",
    "        train_scores.append(resolved_scores[train_index])\n",
    "        train_essay_id.append(essay_id[train_index])\n",
    "\n",
    "    #     for ite in train_index[0]:\n",
    "    # #         print(ite)\n",
    "    #         trainE.append(E[ite])\n",
    "    #         train_scores.append(resolved_scores[ite])\n",
    "    #         train_essay_id.append(essay_id[ite])\n",
    "    #         print(\"Training Samples\", E[ite], resolved_scores[ite], essay_id[ite])\n",
    "\n",
    "    #     for ite in test_index:\n",
    "    #         testE.append(E[ite])\n",
    "    #         test_scores.append(resolved_scores[ite])\n",
    "    #         test_essay_id.append(essay_id[ite])\n",
    "\n",
    "    #trainE, testE, train_scores, test_scores, train_essay_id, test_essay_id = cross_validation.train_test_split(\n",
    "    #    E, resolved_scores, essay_id, test_size=.2, random_state=random_state)\n",
    "\n",
    "    memory = []\n",
    "    memory_score = []\n",
    "    memory_sent_size = []\n",
    "    memory_essay_ids = []\n",
    "    # pick sampled essay for each score\n",
    "    for i in score_range:\n",
    "    # test point: limit the number of samples in memory for 8\n",
    "        for j in range(num_samples):\n",
    "            if i in train_scores:\n",
    "                score_idx = train_scores.index(i)\n",
    "                score = train_scores.pop(score_idx)\n",
    "                essay = trainE.pop(score_idx)\n",
    "                sent_size = sent_size_list.pop(score_idx)\n",
    "                memory.append(essay)\n",
    "                memory_score.append(score)\n",
    "                memory_essay_ids.append(train_essay_id.pop(score_idx))\n",
    "                memory_sent_size.append(sent_size)\n",
    "    memory_size = len(memory)\n",
    "    if is_regression:\n",
    "    # bad naming\n",
    "        train_scores_encoding = train_scores\n",
    "    else:\n",
    "        train_scores_encoding = list(map(lambda x: score_range.index(x), train_scores))\n",
    "\n",
    "        # data size\n",
    "    n_train = len(trainE)\n",
    "    n_test = len(testE)\n",
    "\n",
    "    print( 'The size of training data: {}'.format(n_train))\n",
    "    print( 'The size of testing data: {}'.format(n_test))\n",
    "    with open(out_dir+'/params_{}'.format(names), 'a') as f:\n",
    "        f.write('The size of training data: {}\\n'.format(n_train))\n",
    "        f.write('The size of testing data: {}\\n'.format(n_test))\n",
    "        f.write('\\nEssay scores in memory:\\n{}'.format(memory_score))\n",
    "        f.write('\\nEssay ids in memory:\\n{}'.format(memory_essay_ids))\n",
    "        f.write('\\nEssay ids in training:\\n{}'.format(train_essay_id))\n",
    "        f.write('\\nEssay ids in testing:\\n{}'.format(test_essay_id))\n",
    "\n",
    "    batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "    batches = [(start, end) for start, end in batches]\n",
    "    print(batches)\n",
    "    x = 1\n",
    "    if x == 1:\n",
    "        with tf.Graph().as_default():\n",
    "            session_conf = tf.ConfigProto(\n",
    "                allow_soft_placement=True,\n",
    "                log_device_placement=False)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            # decay learning rate\n",
    "            starter_learning_rate = 0.0001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
    "\n",
    "            # test point\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.1)\n",
    "            #optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "            best_kappa_so_far = 0.0\n",
    "            with tf.Session(config=session_conf) as sess:\n",
    "                model = MemN2N_KV(batch_size, vocab_size, max_sent_size, max_sent_size, memory_size,\n",
    "                                  memory_size, embedding_size, len(score_range), feature_size, hops, reader, l2_lambda)\n",
    "\n",
    "                grads_and_vars = optimizer.compute_gradients(model.loss_op, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "                grads_and_vars = [(tf.clip_by_norm(g, 10.0), v)\n",
    "                                  for g, v in grads_and_vars if g is not None]\n",
    "                #grads_and_vars = [(add_gradient_noise(g, 1e-4), v) for g, v in grads_and_vars]\n",
    "                train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
    "                sess.run(tf.global_variables_initializer(), feed_dict={model.w_placeholder: word2vec})\n",
    "                saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "                for i in range(1, epochs+1):\n",
    "                    train_cost = 0\n",
    "                    total_time = 0\n",
    "                    np.random.shuffle(batches)\n",
    "                    for start, end in batches:\n",
    "                        e = trainE[start:end]\n",
    "                        s = train_scores_encoding[start:end]\n",
    "                        s_num = train_scores[start:end]\n",
    "                        #batched_memory = []\n",
    "                        # batch sized memory\n",
    "                        #for _ in range(len(e)):\n",
    "                        #    batched_memory.append(memory)\n",
    "                        mem_atten_encoding = []\n",
    "                        for ite in s_num:\n",
    "                            mem_encoding = np.zeros(memory_size)\n",
    "                            for j_idx, j in enumerate(memory_score):\n",
    "                                if j == ite:\n",
    "                                    mem_encoding[j_idx] = 1\n",
    "                            mem_atten_encoding.append(mem_encoding)\n",
    "                        batched_memory = [memory] * (end-start)\n",
    "                        _, cost, time_spent = train_step(batched_memory, e, s, mem_atten_encoding)\n",
    "                        total_time += time_spent\n",
    "                        train_cost += cost\n",
    "                    print( 'Finish epoch {}, total training cost is {}, time spent is {}'.format(i, train_cost, total_time))\n",
    "                    # evaluation\n",
    "                    if i % 5 == 0 or i == 200:\n",
    "                        # test on training data\n",
    "                        train_preds = []\n",
    "                        for start in range(0, n_train, test_batch_size):\n",
    "                            end = min(n_train, start+test_batch_size)\n",
    "\n",
    "                            #batched_memory = []\n",
    "                            #for _ in range(end-start):\n",
    "                            #    batched_memory.append(memory)\n",
    "                            batched_memory = [memory] * (end-start)\n",
    "    #                         print(\"BM\", len(batched_memory))\n",
    "                            preds, _ = test_step(trainE[start:end], batched_memory)\n",
    "                            if type(preds) is np.float32:\n",
    "                                train_preds.append(preds)\n",
    "                            else:\n",
    "                                for ite in preds:\n",
    "                                    train_preds.append(ite)\n",
    "                        if not is_regression:\n",
    "                            train_preds = np.add(train_preds, min_score)\n",
    "                        #train_kappp_score = kappa(train_scores, train_preds, 'quadratic')\n",
    "                        train_kappp_score = quadratic_weighted_kappa(\n",
    "                            train_scores, train_preds, min_score, max_score)\n",
    "                        # test on test data\n",
    "                        test_preds = []\n",
    "                        test_atten_probs = []\n",
    "                        for start in range(0, n_test, test_batch_size):\n",
    "                            end = min(n_test, start+test_batch_size)\n",
    "\n",
    "                            #batched_memory = []\n",
    "                            #for _ in range(end-start):\n",
    "                            #    batched_memory.append(memory)\n",
    "                            batched_memory = [memory] * (end-start)\n",
    "    #                         print(\"Test\", len(testE[start:end]))\n",
    "\n",
    "                            preds, mem_attention_probs = test_step(testE[start:end], batched_memory)\n",
    "                            if type(preds) is np.float32:\n",
    "                                test_preds.append(preds)\n",
    "                            else:\n",
    "                                for ite in preds:\n",
    "                                    test_preds.append(ite)\n",
    "                            for ite in mem_attention_probs:\n",
    "                                test_atten_probs.append(ite)\n",
    "                        if not is_regression:\n",
    "                            test_preds = np.add(test_preds, min_score)\n",
    "                        #test_kappp_score = kappa(test_scores, test_preds, 'quadratic')\n",
    "                        test_kappp_score = quadratic_weighted_kappa(\n",
    "                            test_scores, test_preds, min_score, max_score)\n",
    "                        stat_dict = {'pred_score': test_preds}\n",
    "                        stat_df = pd.DataFrame(stat_dict)\n",
    "                        # save the model if it gets best kappa\n",
    "                        if(test_kappp_score > best_kappa_so_far):\n",
    "                            early_stop_count = 0\n",
    "                            best_kappa_so_far = test_kappp_score\n",
    "                            # stats on test\n",
    "                            stat_df.to_csv(out_dir+'/predScore_'+names)\n",
    "                            with open(out_dir+'/mem_atten', 'a') as f:\n",
    "                                for idx, ite in enumerate(test_essay_id):\n",
    "                                    f.write('{}\\n'.format(ite))\n",
    "                                    f.write('{}\\n'.format(test_atten_probs[idx]))\n",
    "    #                         saver.save(sess, out_dir+'/checkpoints', global_step)\n",
    "    #                         model.save('prompt1.h5')\n",
    "    #                         tf.saved_model.save(model, 'runs/')\n",
    "                        else:\n",
    "                            early_stop_count += 1\n",
    "                        print(\"Training kappa score = {}\".format(train_kappp_score))\n",
    "                        print(\"Testing kappa score = {}\".format(test_kappp_score))\n",
    "                        with open(out_dir+'/eval_'.format(names), 'a') as f:\n",
    "                            f.write(\"Training kappa score = {}\\n\".format(train_kappp_score))\n",
    "                            f.write(\"Testing kappa score = {}\\n\".format(test_kappp_score))\n",
    "                            f.write(\"Best Testing kappa score so far = {}\\n\".format(best_kappa_so_far))\n",
    "                            f.write('*'*10)\n",
    "                            f.write('\\n')\n",
    "                    if early_stop_count > max_step_count:\n",
    "                        break\n",
    "                best_kappa_scores.append(best_kappa_so_far)\n",
    "\n",
    "    # model\n",
    "    with open(out_dir+'/eval_'.format(names), 'a') as f:\n",
    "        f.write('5 fold cv {}\\n'.format(best_kappa_scores))\n",
    "        f.write('final result is {}'.format(np.mean(np.array(best_kappa_scores))))\n",
    "\n",
    "    #sys.stdout = orig_stdout\n",
    "    #f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
