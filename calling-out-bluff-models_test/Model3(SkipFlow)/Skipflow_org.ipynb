{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Saiteja-Reddy/Automatic-Text-Scoring.git\n",
    "import  keras.layers  as  klayers \n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from quadratic_weighted_kappa import QWK\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Tensor_layer(Layer):\n",
    "\tdef __init__(self,output_dim,input_dim=None, **kwargs):\n",
    "\t\tself.output_dim=output_dim\n",
    "\t\tself.input_dim=input_dim\n",
    "\t\tif self.input_dim:\n",
    "\t\t\tkwargs['input_shape']=(self.input_dim,)\n",
    "# \t\tprint(\"YAYY\", input_dim, output_dim)\n",
    "\t\tsuper(Neural_Tensor_layer,self).__init__(**kwargs)\n",
    "\n",
    "\tdef call(self,inputs,mask=None):\n",
    "\t\te1=inputs[0]\n",
    "\t\te2=inputs[1]\n",
    "\t\tbatch_size=K.shape(e1)[0]\n",
    "\t\tk=self.output_dim\n",
    "\t\t\n",
    "\n",
    "\t\tfeed_forward=K.dot(K.concatenate([e1,e2]),self.V)\n",
    "\n",
    "\t\tbilinear_tensor_products = [ K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1) ]\n",
    "\n",
    "\t\tfor i in range(k)[1:]:\t\n",
    "\t\t\tbtp=K.sum((e2*K.dot(e1,self.W[i]))+self.b,axis=1)\n",
    "\t\t\tbilinear_tensor_products.append(btp)\n",
    "\n",
    "\t\tresult=K.tanh(K.reshape(K.concatenate(bilinear_tensor_products,axis=0),(batch_size,k))+feed_forward)\n",
    "\n",
    "\t\treturn result\n",
    "    \n",
    "\tdef build(self,input_shape):\n",
    "\t\tmean=0.0\n",
    "\t\tstd=1.0\n",
    "\t\tk=self.output_dim\n",
    "\t\td=self.input_dim\n",
    "\t\t##truncnorm generate continuous random numbers in given range\n",
    "\t\tW_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k,d,d))\n",
    "\t\tV_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2*d,k))\n",
    "\t\tself.W=K.variable(W_val)\n",
    "\t\tself.V=K.variable(V_val)\n",
    "\t\tself.b=K.zeros((self.input_dim,))\n",
    "\t\tself.trainable_weights=[self.W,self.V,self.b]    \n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\tbatch_size=input_shape[0][0]\n",
    "\t\treturn(batch_size,self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Mean_Pooling(Layer): # conversion from (samples,timesteps,features) to (samples,features)\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Temporal_Mean_Pooling,self).__init__(**kwargs)\n",
    "\t\t# masked values in x (number_of_samples,time)\n",
    "\t\tself.supports_masking=True\n",
    "\t\t# Specifies number of dimensions to each layer\n",
    "\t\tself.input_spec=InputSpec(ndim=3)\n",
    "        \n",
    "\tdef call(self,x,mask=None):\n",
    "\t\tif mask is None:\n",
    "\t\t\tmask=K.mean(K.ones_like(x),axis=-1)\n",
    "\n",
    "\t\tmask=K.cast(mask,K.floatx())\n",
    "\t\t\t\t#dimension size single vec/number of samples\n",
    "\t\treturn K.sum(x,axis=-2)/K.sum(mask,axis=-1,keepdims=True)        \n",
    "\n",
    "\tdef compute_mask(self,input,mask):\n",
    "\t\treturn None\n",
    "    \n",
    "    \n",
    "\tdef compute_output_shape(self,input_shape):\n",
    "\t\treturn (input_shape[0],input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "MAX_NB_WORDS=4000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "VALIDATION_SPLIT=0.20\n",
    "DELTA=20\n",
    "\n",
    "texts=[]\n",
    "labels=[]\n",
    "sentences=[]\n",
    "\n",
    "originals = []\n",
    "\n",
    "fp1=open(\"glove.6B.300d.txt\",\"r\", encoding=\"utf-8\")\n",
    "glove_emb={}\n",
    "for line in fp1:\n",
    "\ttemp=line.split(\" \")\n",
    "\tglove_emb[temp[0]]=np.asarray([float(i) for i in temp[1:]])\n",
    "\n",
    "print(\"Embedding done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range min -  1.0  ; range max -  6.0\n"
     ]
    }
   ],
   "source": [
    "essay_type = '2'\n",
    "\n",
    "fp=open(\"data/training_set_rel3.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "originals = []\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        originals.append(float(temp[6]))\n",
    "# print(originals)\n",
    "fp.close()\n",
    "# print(originals)\n",
    "print(\"range min - \", min(originals) , \" ; range max - \", max(originals))\n",
    "\n",
    "range_min = min(originals)\n",
    "range_max = max(originals)\n",
    "\n",
    "# range_min = 1\n",
    "# range_max = 6\n",
    "\n",
    "# print(\"range min - \", range_min , \" ; range max - \", range_max)\n",
    "\n",
    "\n",
    "fp=open(\"data/training_set_rel3.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "sentences=[]\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        texts.append(temp[2])\n",
    "        labels.append((float(temp[6])-range_min)/(range_max-range_min)) ## why ??  - normalize to range [0-1]\n",
    "        line=temp[2].strip()\n",
    "        sentences.append(nltk.tokenize.word_tokenize(line))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "labels\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Write a persuasive essay to a newspaper reflecting your views on censorship in libraries. Do you believe that certain should be removed i think so be no that yes i think should no person that in chager the book, music, movies, magazines, ect., that be no agure      why do i think if you need that please  think i no thank you please if  i need why do we if know that if i failure the this test i who need to graduate please the children allow to home please yes.          Why do we need to be a prafece person please why do we need to do this why write this assgiment because you mean to be the best teaches ever and ever facebook is my password is @PERSON1  @NUM1 that why i need my myspace is the same thingh but different at same time please know that i need to know i really  i need to my e-mail address is  @EMAIL1 that is my e-mail please work m\"'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text labels appended 1800\n",
      "[0.6 0.  0.2 ... 0.2 0.4 0.4]\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(\"text labels appended %s\" %len(texts))\n",
    "\n",
    "labels=np.asarray(labels)\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "\ttemp1=np.zeros((1, EMBEDDING_DIM))\n",
    "\tfor w in i:\n",
    "\t\tif(w in glove_emb):\n",
    "\t\t\ttemp1+=glove_emb[w]\n",
    "\ttemp1/=len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14685 unique tokens.\n",
      "Shape of data tensor: (1800, 500)\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(num_words = MAX_NB_WORDS) #num_words=MAX_NB_WORDS) #limits vocabulory size\n",
    "tokenizer.fit_on_texts(texts) #encoding the text\n",
    "sequences=tokenizer.texts_to_sequences(texts) #returns list of sequences\n",
    "word_index=tokenizer.word_index #dictionary mapping, word and specific token for that word...\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #padding to max_length\n",
    "\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 500)\n",
      "(1800, 500)\n",
      "(1800,)\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "indices=np.arange(data.shape[0]) #with one argument, start=0, step =1\n",
    "print(data.shape)\n",
    "np.random.shuffle(indices)\n",
    "data=data[indices]\n",
    "print(data.shape)\n",
    "labels=labels[indices]\n",
    "# np.reshape(labels, ())\n",
    "print(labels.shape)\n",
    "validation_size=int(VALIDATION_SPLIT*data.shape[0])\n",
    "print(validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 500)\n",
      "(1440,)\n",
      "(360, 500)\n"
     ]
    }
   ],
   "source": [
    "x_train=data[:-validation_size] #data-validation data\n",
    "print(x_train.shape)\n",
    "# print(x_train)\n",
    "# print(labels)\n",
    "y_train=labels[:-validation_size]\n",
    "# print(y_train.transpose)\n",
    "print(y_train.shape)\n",
    "# y_train = np.reshape(y_train, (1427, 1))\n",
    "# print(y_train_new)\n",
    "# print(y_train)\n",
    "x_val=data[-validation_size:]\n",
    "print(x_val.shape)\n",
    "y_val=labels[-validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14685, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14685\n"
     ]
    }
   ],
   "source": [
    "for word,i in word_index.items():\n",
    "\tif(i>=len(word_index)):\n",
    "\t\tcontinue\n",
    "\tif word in glove_emb:\n",
    "\t\t\tembedding_matrix[i]=glove_emb[word]\n",
    "vocab_size=len(word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=True,\n",
    "\t\t\t\t\t\t\ttrainable=False)\n",
    "# print(embedding_layer.shape)\n",
    "side_embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=False,\n",
    "\t\t\t\t\t\t\ttrainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SKIPFLOW(lstm_dim=50, lr=1e-4, lr_decay=1e-6, k=4, eta=3, delta=50, activation=\"relu\", maxlen=MAX_SEQUENCE_LENGTH, seed=None):\n",
    "    e = Input(name='essay',shape=(maxlen,))\n",
    "    print(\"e\", e)\n",
    "#     trad_feats=Input(shape=(7,))\n",
    "#     print(\"trad_feats\", trad_feats)\n",
    "    embed = embedding_layer(e)\n",
    "    print(embed.shape)\n",
    "    lstm_layer=LSTM(lstm_dim,return_sequences=True)\n",
    "    print(lstm_layer)\n",
    "    hidden_states=lstm_layer(embed)\n",
    "    htm=Temporal_Mean_Pooling()(hidden_states)    \n",
    "    side_embed = side_embedding_layer(e)\n",
    "    side_hidden_states=lstm_layer(side_embed)    \n",
    "    tensor_layer=Neural_Tensor_layer(output_dim=k,input_dim=lstm_dim)\n",
    "#     print(input_dim, output_dim)\n",
    "    pairs = [((eta + i * delta) % maxlen, (eta + i * delta + delta) % maxlen) for i in range(maxlen // delta)]\n",
    "    hidden_pairs = [ (Lambda(lambda t: t[:, p[0], :])(side_hidden_states), Lambda(lambda t: t[:, p[1], :])(side_hidden_states)) for p in pairs]\n",
    "    sigmoid = Dense(1, activation=\"sigmoid\", kernel_initializer=initializers.glorot_normal(seed=seed))\n",
    "    coherence = [sigmoid(tensor_layer([hp[0], hp[1]])) for hp in hidden_pairs]\n",
    "    co_tm=Concatenate()(coherence[:]+[htm])\n",
    "    dense = Dense(256, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(co_tm)\n",
    "    dense = Dense(128, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    dense = Dense(64, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    out = Dense(1, activation=\"sigmoid\")(dense)\n",
    "    model = Model(inputs=[e], outputs=[out])\n",
    "    print(\"input\", [e])\n",
    "    print(\"outputs\", out)\n",
    "    adam = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"MSE\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e Tensor(\"essay_1:0\", shape=(?, 500), dtype=float32)\n",
      "(?, 500, 300)\n",
      "<keras.layers.recurrent.LSTM object at 0x7f68ee100780>\n",
      "input [<tf.Tensor 'essay_1:0' shape=(?, 500) dtype=float32>]\n",
      "outputs Tensor(\"dense_10/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "essay (InputLayer)              (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 500, 300)     4405500     essay[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 500, 500)     1602000     embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 500)          0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 500, 300)     4405500     essay[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "neural__tensor_layer_2 (Neural_ (None, 4)            1004500     lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 lambda_32[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 lambda_35[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "                                                                 lambda_38[0][0]                  \n",
      "                                                                 lambda_39[0][0]                  \n",
      "                                                                 lambda_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            5           neural__tensor_layer_2[0][0]     \n",
      "                                                                 neural__tensor_layer_2[1][0]     \n",
      "                                                                 neural__tensor_layer_2[2][0]     \n",
      "                                                                 neural__tensor_layer_2[3][0]     \n",
      "                                                                 neural__tensor_layer_2[4][0]     \n",
      "                                                                 neural__tensor_layer_2[5][0]     \n",
      "                                                                 neural__tensor_layer_2[6][0]     \n",
      "                                                                 neural__tensor_layer_2[7][0]     \n",
      "                                                                 neural__tensor_layer_2[8][0]     \n",
      "                                                                 neural__tensor_layer_2[9][0]     \n",
      "__________________________________________________________________________________________________\n",
      "temporal__mean__pooling_2 (Temp (None, 500)          0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 510)          0           dense_6[0][0]                    \n",
      "                                                                 dense_6[1][0]                    \n",
      "                                                                 dense_6[2][0]                    \n",
      "                                                                 dense_6[3][0]                    \n",
      "                                                                 dense_6[4][0]                    \n",
      "                                                                 dense_6[5][0]                    \n",
      "                                                                 dense_6[6][0]                    \n",
      "                                                                 dense_6[7][0]                    \n",
      "                                                                 dense_6[8][0]                    \n",
      "                                                                 dense_6[9][0]                    \n",
      "                                                                 temporal__mean__pooling_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          130816      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          32896       dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 64)           8256        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            65          dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,589,538\n",
      "Trainable params: 2,778,538\n",
      "Non-trainable params: 8,811,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.utils.vis_utils import plot_model\n",
    "earlystopping = EarlyStopping(monitor=\"val_mean_squared_error\", patience=5)\n",
    "sf_2 = SKIPFLOW(lstm_dim=500, lr=2e-4, lr_decay=2e-6, k=4, eta=13, delta=50, activation=\"relu\", seed=None)\n",
    "sf_2.summary()\n",
    "# plot_model(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Train on 1440 samples, validate on 360 samples\n",
      "Epoch 1/100\n",
      "1440/1440 [==============================] - 87s 61ms/step - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0142 - val_mean_squared_error: 0.0142\n",
      "Epoch 2/100\n",
      "1440/1440 [==============================] - 81s 56ms/step - loss: 0.0143 - mean_squared_error: 0.0143 - val_loss: 0.0147 - val_mean_squared_error: 0.0147\n",
      "Epoch 3/100\n",
      "1440/1440 [==============================] - 83s 58ms/step - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 4/100\n",
      "1440/1440 [==============================] - 83s 57ms/step - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
      "Epoch 5/100\n",
      "1440/1440 [==============================] - 82s 57ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0124 - val_mean_squared_error: 0.0124\n",
      "Epoch 6/100\n",
      "1440/1440 [==============================] - 82s 57ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
      "Epoch 7/100\n",
      "1440/1440 [==============================] - 83s 58ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
      "Epoch 8/100\n",
      "1440/1440 [==============================] - 82s 57ms/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
      "Epoch 9/100\n",
      "1440/1440 [==============================] - 83s 57ms/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
      "Epoch 10/100\n",
      "1440/1440 [==============================] - 83s 57ms/step - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
      "Epoch 11/100\n",
      "1440/1440 [==============================] - 83s 58ms/step - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.0113 - val_mean_squared_error: 0.0113\n",
      "Epoch 12/100\n",
      "1440/1440 [==============================] - 82s 57ms/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0119 - val_mean_squared_error: 0.0119\n",
      "Epoch 13/100\n",
      "1440/1440 [==============================] - 85s 59ms/step - loss: 0.0098 - mean_squared_error: 0.0098 - val_loss: 0.0113 - val_mean_squared_error: 0.0113\n",
      "Epoch 14/100\n",
      "1440/1440 [==============================] - 84s 58ms/step - loss: 0.0100 - mean_squared_error: 0.0100 - val_loss: 0.0115 - val_mean_squared_error: 0.0115\n",
      "Epoch 15/100\n",
      "1440/1440 [==============================] - 82s 57ms/step - loss: 0.0098 - mean_squared_error: 0.0098 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
      "Epoch 16/100\n",
      "1440/1440 [==============================] - 83s 58ms/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0125 - val_mean_squared_error: 0.0125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f67ec7ee518>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(sf_1)\n",
    "epochs = 100\n",
    "# epochs = 1000\n",
    "print(type(x_train))\n",
    "# y_train = np.asarray(y_train)\n",
    "print(type(y_train))\n",
    "\n",
    "sf_2.fit(x_train, y_train, batch_size=32, epochs=epochs, validation_data=([x_val], y_val), callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46089828],\n",
       "       [0.576725  ],\n",
       "       [0.64377844],\n",
       "       [0.4184278 ],\n",
       "       [0.5835984 ],\n",
       "       [0.42685193],\n",
       "       [0.41215548],\n",
       "       [0.12620825],\n",
       "       [0.6493654 ],\n",
       "       [0.4587961 ],\n",
       "       [0.52799934],\n",
       "       [0.5310778 ],\n",
       "       [0.5549681 ],\n",
       "       [0.31933445],\n",
       "       [0.3062078 ],\n",
       "       [0.51277786],\n",
       "       [0.42064697],\n",
       "       [0.38145876],\n",
       "       [0.556369  ],\n",
       "       [0.53551775],\n",
       "       [0.41446975],\n",
       "       [0.4121983 ],\n",
       "       [0.49808177],\n",
       "       [0.4043721 ],\n",
       "       [0.37808728],\n",
       "       [0.32674593],\n",
       "       [0.48663443],\n",
       "       [0.4724245 ],\n",
       "       [0.52522165],\n",
       "       [0.58121437],\n",
       "       [0.3770579 ],\n",
       "       [0.3601788 ],\n",
       "       [0.5417104 ],\n",
       "       [0.52304775],\n",
       "       [0.62351644],\n",
       "       [0.33476925],\n",
       "       [0.45366955],\n",
       "       [0.44276595],\n",
       "       [0.507017  ],\n",
       "       [0.50749516],\n",
       "       [0.52941364],\n",
       "       [0.5980596 ],\n",
       "       [0.48773807],\n",
       "       [0.39015317],\n",
       "       [0.5122498 ],\n",
       "       [0.23577136],\n",
       "       [0.5257977 ],\n",
       "       [0.38407302],\n",
       "       [0.33987254],\n",
       "       [0.48074976],\n",
       "       [0.37628284],\n",
       "       [0.3636062 ],\n",
       "       [0.41405442],\n",
       "       [0.49931678],\n",
       "       [0.47229332],\n",
       "       [0.4685246 ],\n",
       "       [0.62062824],\n",
       "       [0.55879533],\n",
       "       [0.46873096],\n",
       "       [0.50349855],\n",
       "       [0.4441164 ],\n",
       "       [0.55615866],\n",
       "       [0.5328404 ],\n",
       "       [0.50832766],\n",
       "       [0.45308372],\n",
       "       [0.31704932],\n",
       "       [0.38278162],\n",
       "       [0.43859598],\n",
       "       [0.6149786 ],\n",
       "       [0.5273418 ],\n",
       "       [0.311051  ],\n",
       "       [0.46121967],\n",
       "       [0.51127523],\n",
       "       [0.5722139 ],\n",
       "       [0.63673884],\n",
       "       [0.43050432],\n",
       "       [0.13097486],\n",
       "       [0.34273237],\n",
       "       [0.4746237 ],\n",
       "       [0.48192826],\n",
       "       [0.48909804],\n",
       "       [0.5280021 ],\n",
       "       [0.5731134 ],\n",
       "       [0.29647368],\n",
       "       [0.3684681 ],\n",
       "       [0.420033  ],\n",
       "       [0.4693995 ],\n",
       "       [0.52275085],\n",
       "       [0.31945997],\n",
       "       [0.34368747],\n",
       "       [0.61118543],\n",
       "       [0.56001097],\n",
       "       [0.67720574],\n",
       "       [0.42162278],\n",
       "       [0.3024121 ],\n",
       "       [0.56711423],\n",
       "       [0.4087761 ],\n",
       "       [0.5546241 ],\n",
       "       [0.48475063],\n",
       "       [0.41953117],\n",
       "       [0.4078771 ],\n",
       "       [0.61247367],\n",
       "       [0.4750656 ],\n",
       "       [0.45942307],\n",
       "       [0.40456012],\n",
       "       [0.3873532 ],\n",
       "       [0.358351  ],\n",
       "       [0.1382365 ],\n",
       "       [0.21931875],\n",
       "       [0.6205802 ],\n",
       "       [0.29998344],\n",
       "       [0.23747239],\n",
       "       [0.40638092],\n",
       "       [0.31429467],\n",
       "       [0.47011125],\n",
       "       [0.4683661 ],\n",
       "       [0.37840283],\n",
       "       [0.5891175 ],\n",
       "       [0.36391413],\n",
       "       [0.5977483 ],\n",
       "       [0.4350585 ],\n",
       "       [0.4210241 ],\n",
       "       [0.59007186],\n",
       "       [0.20004827],\n",
       "       [0.40587664],\n",
       "       [0.3781572 ],\n",
       "       [0.36570895],\n",
       "       [0.4761724 ],\n",
       "       [0.3515224 ],\n",
       "       [0.42739868],\n",
       "       [0.47741485],\n",
       "       [0.62918603],\n",
       "       [0.4787494 ],\n",
       "       [0.5256512 ],\n",
       "       [0.65892947],\n",
       "       [0.25639552],\n",
       "       [0.3864454 ],\n",
       "       [0.55952144],\n",
       "       [0.40348232],\n",
       "       [0.54075557],\n",
       "       [0.3584144 ],\n",
       "       [0.47776315],\n",
       "       [0.28324223],\n",
       "       [0.2585472 ],\n",
       "       [0.2567484 ],\n",
       "       [0.45067048],\n",
       "       [0.6049095 ],\n",
       "       [0.43628228],\n",
       "       [0.37274384],\n",
       "       [0.5058345 ],\n",
       "       [0.41064042],\n",
       "       [0.43056887],\n",
       "       [0.44005266],\n",
       "       [0.5818527 ],\n",
       "       [0.3488707 ],\n",
       "       [0.53707117],\n",
       "       [0.42449775],\n",
       "       [0.46555892],\n",
       "       [0.49005994],\n",
       "       [0.5073819 ],\n",
       "       [0.33381104],\n",
       "       [0.5628326 ],\n",
       "       [0.52543503],\n",
       "       [0.13788593],\n",
       "       [0.41898465],\n",
       "       [0.45790732],\n",
       "       [0.4665225 ],\n",
       "       [0.5250248 ],\n",
       "       [0.22593495],\n",
       "       [0.4393029 ],\n",
       "       [0.52988034],\n",
       "       [0.54333043],\n",
       "       [0.55121994],\n",
       "       [0.16585258],\n",
       "       [0.55207384],\n",
       "       [0.3687782 ],\n",
       "       [0.4077921 ],\n",
       "       [0.46597594],\n",
       "       [0.5019462 ],\n",
       "       [0.5509201 ],\n",
       "       [0.48716047],\n",
       "       [0.4586424 ],\n",
       "       [0.5823478 ],\n",
       "       [0.5955871 ],\n",
       "       [0.37316084],\n",
       "       [0.6159671 ],\n",
       "       [0.4187313 ],\n",
       "       [0.50909644],\n",
       "       [0.6087161 ],\n",
       "       [0.43107218],\n",
       "       [0.41218895],\n",
       "       [0.23911306],\n",
       "       [0.47599083],\n",
       "       [0.3916805 ],\n",
       "       [0.59250677],\n",
       "       [0.6561904 ],\n",
       "       [0.5976639 ],\n",
       "       [0.4244491 ],\n",
       "       [0.29353076],\n",
       "       [0.5114347 ],\n",
       "       [0.41858804],\n",
       "       [0.5549158 ],\n",
       "       [0.11064506],\n",
       "       [0.50325614],\n",
       "       [0.417366  ],\n",
       "       [0.15128848],\n",
       "       [0.51002604],\n",
       "       [0.4526124 ],\n",
       "       [0.43463343],\n",
       "       [0.3881104 ],\n",
       "       [0.6212375 ],\n",
       "       [0.5667988 ],\n",
       "       [0.40900952],\n",
       "       [0.3726704 ],\n",
       "       [0.13837323],\n",
       "       [0.394882  ],\n",
       "       [0.61231554],\n",
       "       [0.37807435],\n",
       "       [0.4033683 ],\n",
       "       [0.44728276],\n",
       "       [0.2904806 ],\n",
       "       [0.14643115],\n",
       "       [0.28773725],\n",
       "       [0.47499967],\n",
       "       [0.60749596],\n",
       "       [0.5111582 ],\n",
       "       [0.55327237],\n",
       "       [0.40748936],\n",
       "       [0.55379945],\n",
       "       [0.54532987],\n",
       "       [0.5125357 ],\n",
       "       [0.47928315],\n",
       "       [0.56056935],\n",
       "       [0.54357266],\n",
       "       [0.556246  ],\n",
       "       [0.5171827 ],\n",
       "       [0.46397093],\n",
       "       [0.31423903],\n",
       "       [0.33182025],\n",
       "       [0.40284345],\n",
       "       [0.38560885],\n",
       "       [0.46040744],\n",
       "       [0.33713573],\n",
       "       [0.5876806 ],\n",
       "       [0.6172359 ],\n",
       "       [0.50586766],\n",
       "       [0.560369  ],\n",
       "       [0.4333914 ],\n",
       "       [0.35336456],\n",
       "       [0.49385712],\n",
       "       [0.42692786],\n",
       "       [0.14563346],\n",
       "       [0.47262278],\n",
       "       [0.3420584 ],\n",
       "       [0.381504  ],\n",
       "       [0.5218421 ],\n",
       "       [0.41805735],\n",
       "       [0.31055975],\n",
       "       [0.4190905 ],\n",
       "       [0.38880086],\n",
       "       [0.5970796 ],\n",
       "       [0.4515251 ],\n",
       "       [0.56000006],\n",
       "       [0.5187379 ],\n",
       "       [0.42429742],\n",
       "       [0.32666975],\n",
       "       [0.49088684],\n",
       "       [0.16624752],\n",
       "       [0.5983343 ],\n",
       "       [0.4719973 ],\n",
       "       [0.48047563],\n",
       "       [0.41450635],\n",
       "       [0.46858087],\n",
       "       [0.19310695],\n",
       "       [0.56569946],\n",
       "       [0.58939   ],\n",
       "       [0.4448334 ],\n",
       "       [0.32762897],\n",
       "       [0.53004235],\n",
       "       [0.4336725 ],\n",
       "       [0.5380297 ],\n",
       "       [0.484089  ],\n",
       "       [0.43647206],\n",
       "       [0.3733157 ],\n",
       "       [0.5371107 ],\n",
       "       [0.47427627],\n",
       "       [0.3542646 ],\n",
       "       [0.36701435],\n",
       "       [0.37258148],\n",
       "       [0.55915725],\n",
       "       [0.59992754],\n",
       "       [0.4841915 ],\n",
       "       [0.4893863 ],\n",
       "       [0.50526386],\n",
       "       [0.38022265],\n",
       "       [0.4772335 ],\n",
       "       [0.29662025],\n",
       "       [0.6543177 ],\n",
       "       [0.5094664 ],\n",
       "       [0.5824739 ],\n",
       "       [0.45812294],\n",
       "       [0.6221794 ],\n",
       "       [0.39392322],\n",
       "       [0.33843213],\n",
       "       [0.48545426],\n",
       "       [0.4254183 ],\n",
       "       [0.43064472],\n",
       "       [0.3047376 ],\n",
       "       [0.4738557 ],\n",
       "       [0.6333259 ],\n",
       "       [0.44266164],\n",
       "       [0.36649445],\n",
       "       [0.58404773],\n",
       "       [0.37897307],\n",
       "       [0.36584377],\n",
       "       [0.55428183],\n",
       "       [0.56620276],\n",
       "       [0.12887004],\n",
       "       [0.5283987 ],\n",
       "       [0.47174618],\n",
       "       [0.42648226],\n",
       "       [0.4988544 ],\n",
       "       [0.6705772 ],\n",
       "       [0.449035  ],\n",
       "       [0.412919  ],\n",
       "       [0.5166625 ],\n",
       "       [0.53586364],\n",
       "       [0.46238482],\n",
       "       [0.45805717],\n",
       "       [0.5079654 ],\n",
       "       [0.59663844],\n",
       "       [0.5342131 ],\n",
       "       [0.42839167],\n",
       "       [0.42928523],\n",
       "       [0.538024  ],\n",
       "       [0.5217731 ],\n",
       "       [0.46343866],\n",
       "       [0.61231405],\n",
       "       [0.50866485],\n",
       "       [0.4537782 ],\n",
       "       [0.6184502 ],\n",
       "       [0.3579567 ],\n",
       "       [0.60111517],\n",
       "       [0.6134786 ],\n",
       "       [0.37053397],\n",
       "       [0.50085205],\n",
       "       [0.3965441 ],\n",
       "       [0.18521571],\n",
       "       [0.13200727],\n",
       "       [0.5348461 ],\n",
       "       [0.62336946],\n",
       "       [0.5696161 ],\n",
       "       [0.53165674],\n",
       "       [0.41765326],\n",
       "       [0.40297258],\n",
       "       [0.34346393],\n",
       "       [0.5854445 ],\n",
       "       [0.5285808 ],\n",
       "       [0.57069343],\n",
       "       [0.45949993]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=sf_2.predict([x_val])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 3, 4, 3, 3, 2, 4, 4, 4, 3, 3, 3, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 4, 3, 3, 3, 3, 3, 4, 3, 4, 3, 2, 4, 4, 2, 3, 3, 4, 3, 5, 4, 4, 3, 2, 5, 3, 5, 4, 3, 3, 3, 4, 4, 3, 3, 3, 4, 5, 3, 2, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 4, 5, 3, 3, 5, 3, 4, 3, 3, 3, 4, 4, 4, 4, 4, 4, 1, 2, 4, 3, 2, 3, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 1, 3, 3, 2, 4, 4, 4, 4, 4, 4, 4, 5, 3, 3, 4, 3, 4, 3, 4, 2, 3, 3, 3, 4, 4, 3, 4, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 5, 5, 2, 3, 4, 4, 4, 3, 3, 4, 4, 4, 3, 4, 3, 3, 2, 3, 4, 4, 4, 4, 5, 2, 4, 3, 4, 4, 3, 3, 3, 3, 2, 5, 4, 4, 4, 3, 3, 3, 3, 1, 4, 3, 2, 3, 3, 3, 3, 4, 3, 3, 3, 2, 3, 4, 3, 3, 4, 3, 1, 3, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 3, 4, 2, 4, 5, 4, 4, 4, 3, 4, 3, 2, 4, 3, 3, 3, 4, 3, 3, 4, 6, 4, 4, 4, 3, 3, 4, 2, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 4, 4, 4, 4, 3, 4, 4, 2, 4, 3, 4, 4, 3, 4, 4, 3, 4, 2, 3, 4, 4, 3, 5, 3, 3, 4, 3, 3, 3, 3, 4, 3, 4, 4, 3, 3, 4, 4, 2, 4, 3, 3, 4, 5, 3, 3, 3, 4, 4, 2, 3, 4, 4, 4, 4, 4, 3, 3, 5, 3, 4, 4, 2, 4, 3, 2, 4, 3, 3, 2, 4, 5, 4, 4, 3, 3, 3, 5, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "y_val_fin = [int(round(a*(range_max-range_min)+range_min)) for a in y_val]\n",
    "print(y_val_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 4, 3, 4, 3, 3, 2, 4, 3, 4, 4, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4, 4, 4, 3, 3, 4, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 3, 4, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3, 4, 4, 4, 3, 2, 3, 3, 3, 3, 4, 4, 2, 3, 3, 3, 4, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 2, 2, 4, 2, 2, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 4, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 2, 3, 4, 3, 4, 3, 3, 2, 2, 2, 3, 4, 3, 3, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 2, 3, 3, 3, 4, 2, 3, 4, 4, 4, 2, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 4, 3, 4, 4, 3, 3, 2, 3, 3, 4, 4, 4, 3, 2, 4, 3, 4, 2, 4, 3, 2, 4, 3, 3, 3, 4, 4, 3, 3, 2, 3, 4, 3, 3, 3, 2, 2, 2, 3, 4, 4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 2, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 4, 4, 3, 3, 3, 2, 4, 3, 3, 3, 3, 2, 4, 4, 3, 3, 4, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3, 2, 4, 4, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 2, 4, 3, 3, 3, 4, 3, 3, 4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 4, 4, 3, 4, 3, 2, 2, 4, 4, 4, 4, 3, 3, 3, 4, 4, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "y_pred_fin =[int(round(a*(range_max-range_min)+range_min)) for a in y_pred.reshape(360).tolist()]\n",
    "print(y_pred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5930260329591592\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_val_fin,y_pred_fin,weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def QWK_new(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = Cmatrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5930260329591592"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QWK_new(y_val_fin, y_pred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_2.save('model_final/2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3044915],\n",
       "       [3.883625 ],\n",
       "       [4.218892 ],\n",
       "       [3.092139 ],\n",
       "       [3.9179919],\n",
       "       [3.1342597],\n",
       "       [3.0607774],\n",
       "       [1.6310413],\n",
       "       [4.246827 ],\n",
       "       [3.2939806],\n",
       "       [3.6399968],\n",
       "       [3.655389 ],\n",
       "       [3.7748406],\n",
       "       [2.5966723],\n",
       "       [2.531039 ],\n",
       "       [3.5638893],\n",
       "       [3.1032348],\n",
       "       [2.9072938],\n",
       "       [3.781845 ],\n",
       "       [3.6775887],\n",
       "       [3.0723488],\n",
       "       [3.0609915],\n",
       "       [3.490409 ],\n",
       "       [3.0218606],\n",
       "       [2.8904364],\n",
       "       [2.6337297],\n",
       "       [3.4331722],\n",
       "       [3.3621225],\n",
       "       [3.6261082],\n",
       "       [3.906072 ],\n",
       "       [2.8852897],\n",
       "       [2.800894 ],\n",
       "       [3.708552 ],\n",
       "       [3.6152387],\n",
       "       [4.1175823],\n",
       "       [2.6738462],\n",
       "       [3.2683477],\n",
       "       [3.2138298],\n",
       "       [3.5350852],\n",
       "       [3.5374758],\n",
       "       [3.6470683],\n",
       "       [3.990298 ],\n",
       "       [3.4386904],\n",
       "       [2.9507658],\n",
       "       [3.5612493],\n",
       "       [2.1788568],\n",
       "       [3.6289887],\n",
       "       [2.920365 ],\n",
       "       [2.6993628],\n",
       "       [3.4037488],\n",
       "       [2.8814142],\n",
       "       [2.818031 ],\n",
       "       [3.0702722],\n",
       "       [3.496584 ],\n",
       "       [3.3614666],\n",
       "       [3.342623 ],\n",
       "       [4.1031413],\n",
       "       [3.7939768],\n",
       "       [3.3436549],\n",
       "       [3.5174928],\n",
       "       [3.220582 ],\n",
       "       [3.7807932],\n",
       "       [3.6642017],\n",
       "       [3.5416384],\n",
       "       [3.2654185],\n",
       "       [2.5852466],\n",
       "       [2.913908 ],\n",
       "       [3.1929798],\n",
       "       [4.074893 ],\n",
       "       [3.636709 ],\n",
       "       [2.555255 ],\n",
       "       [3.3060985],\n",
       "       [3.5563762],\n",
       "       [3.8610694],\n",
       "       [4.183694 ],\n",
       "       [3.1525216],\n",
       "       [1.6548743],\n",
       "       [2.713662 ],\n",
       "       [3.3731186],\n",
       "       [3.4096413],\n",
       "       [3.4454901],\n",
       "       [3.6400104],\n",
       "       [3.865567 ],\n",
       "       [2.4823685],\n",
       "       [2.8423405],\n",
       "       [3.1001651],\n",
       "       [3.3469975],\n",
       "       [3.6137543],\n",
       "       [2.5972998],\n",
       "       [2.7184374],\n",
       "       [4.0559273],\n",
       "       [3.8000548],\n",
       "       [4.386029 ],\n",
       "       [3.108114 ],\n",
       "       [2.5120604],\n",
       "       [3.8355713],\n",
       "       [3.0438805],\n",
       "       [3.7731204],\n",
       "       [3.4237533],\n",
       "       [3.0976558],\n",
       "       [3.0393853],\n",
       "       [4.0623684],\n",
       "       [3.375328 ],\n",
       "       [3.2971153],\n",
       "       [3.0228007],\n",
       "       [2.9367661],\n",
       "       [2.791755 ],\n",
       "       [1.6911825],\n",
       "       [2.0965939],\n",
       "       [4.102901 ],\n",
       "       [2.4999173],\n",
       "       [2.187362 ],\n",
       "       [3.0319047],\n",
       "       [2.5714734],\n",
       "       [3.3505564],\n",
       "       [3.3418305],\n",
       "       [2.892014 ],\n",
       "       [3.9455876],\n",
       "       [2.8195705],\n",
       "       [3.9887414],\n",
       "       [3.1752925],\n",
       "       [3.1051207],\n",
       "       [3.9503593],\n",
       "       [2.0002413],\n",
       "       [3.0293832],\n",
       "       [2.890786 ],\n",
       "       [2.8285446],\n",
       "       [3.380862 ],\n",
       "       [2.757612 ],\n",
       "       [3.1369934],\n",
       "       [3.3870742],\n",
       "       [4.1459303],\n",
       "       [3.3937469],\n",
       "       [3.628256 ],\n",
       "       [4.294647 ],\n",
       "       [2.2819777],\n",
       "       [2.9322271],\n",
       "       [3.7976072],\n",
       "       [3.0174117],\n",
       "       [3.7037778],\n",
       "       [2.792072 ],\n",
       "       [3.3888156],\n",
       "       [2.4162111],\n",
       "       [2.292736 ],\n",
       "       [2.283742 ],\n",
       "       [3.2533524],\n",
       "       [4.0245476],\n",
       "       [3.1814113],\n",
       "       [2.8637192],\n",
       "       [3.5291727],\n",
       "       [3.0532022],\n",
       "       [3.1528444],\n",
       "       [3.2002633],\n",
       "       [3.9092634],\n",
       "       [2.7443535],\n",
       "       [3.685356 ],\n",
       "       [3.1224887],\n",
       "       [3.3277946],\n",
       "       [3.4502997],\n",
       "       [3.5369096],\n",
       "       [2.6690552],\n",
       "       [3.814163 ],\n",
       "       [3.627175 ],\n",
       "       [1.6894296],\n",
       "       [3.0949233],\n",
       "       [3.2895365],\n",
       "       [3.3326125],\n",
       "       [3.625124 ],\n",
       "       [2.129675 ],\n",
       "       [3.1965144],\n",
       "       [3.6494017],\n",
       "       [3.7166522],\n",
       "       [3.7560997],\n",
       "       [1.8292629],\n",
       "       [3.7603693],\n",
       "       [2.8438911],\n",
       "       [3.0389605],\n",
       "       [3.3298798],\n",
       "       [3.509731 ],\n",
       "       [3.7546005],\n",
       "       [3.4358025],\n",
       "       [3.293212 ],\n",
       "       [3.911739 ],\n",
       "       [3.9779353],\n",
       "       [2.8658042],\n",
       "       [4.0798354],\n",
       "       [3.0936565],\n",
       "       [3.5454822],\n",
       "       [4.04358  ],\n",
       "       [3.155361 ],\n",
       "       [3.0609448],\n",
       "       [2.1955652],\n",
       "       [3.379954 ],\n",
       "       [2.9584026],\n",
       "       [3.962534 ],\n",
       "       [4.280952 ],\n",
       "       [3.9883194],\n",
       "       [3.1222453],\n",
       "       [2.4676538],\n",
       "       [3.5571733],\n",
       "       [3.0929403],\n",
       "       [3.774579 ],\n",
       "       [1.5532253],\n",
       "       [3.5162807],\n",
       "       [3.08683  ],\n",
       "       [1.7564424],\n",
       "       [3.5501301],\n",
       "       [3.263062 ],\n",
       "       [3.1731672],\n",
       "       [2.940552 ],\n",
       "       [4.106188 ],\n",
       "       [3.833994 ],\n",
       "       [3.0450475],\n",
       "       [2.863352 ],\n",
       "       [1.6918662],\n",
       "       [2.97441  ],\n",
       "       [4.061578 ],\n",
       "       [2.8903718],\n",
       "       [3.0168414],\n",
       "       [3.2364137],\n",
       "       [2.452403 ],\n",
       "       [1.7321558],\n",
       "       [2.4386864],\n",
       "       [3.3749983],\n",
       "       [4.03748  ],\n",
       "       [3.5557911],\n",
       "       [3.7663617],\n",
       "       [3.0374467],\n",
       "       [3.7689972],\n",
       "       [3.7266493],\n",
       "       [3.5626783],\n",
       "       [3.3964157],\n",
       "       [3.8028467],\n",
       "       [3.7178633],\n",
       "       [3.78123  ],\n",
       "       [3.5859137],\n",
       "       [3.3198547],\n",
       "       [2.5711951],\n",
       "       [2.6591012],\n",
       "       [3.0142171],\n",
       "       [2.9280443],\n",
       "       [3.3020372],\n",
       "       [2.6856787],\n",
       "       [3.938403 ],\n",
       "       [4.0861797],\n",
       "       [3.5293384],\n",
       "       [3.801845 ],\n",
       "       [3.166957 ],\n",
       "       [2.7668228],\n",
       "       [3.4692855],\n",
       "       [3.1346393],\n",
       "       [1.7281673],\n",
       "       [3.3631139],\n",
       "       [2.7102919],\n",
       "       [2.90752  ],\n",
       "       [3.6092105],\n",
       "       [3.0902867],\n",
       "       [2.5527987],\n",
       "       [3.0954525],\n",
       "       [2.9440043],\n",
       "       [3.9853978],\n",
       "       [3.2576256],\n",
       "       [3.8000002],\n",
       "       [3.5936894],\n",
       "       [3.1214871],\n",
       "       [2.6333487],\n",
       "       [3.4544342],\n",
       "       [1.8312376],\n",
       "       [3.9916716],\n",
       "       [3.3599865],\n",
       "       [3.402378 ],\n",
       "       [3.0725317],\n",
       "       [3.3429043],\n",
       "       [1.9655347],\n",
       "       [3.8284974],\n",
       "       [3.94695  ],\n",
       "       [3.2241669],\n",
       "       [2.638145 ],\n",
       "       [3.6502118],\n",
       "       [3.1683624],\n",
       "       [3.6901484],\n",
       "       [3.420445 ],\n",
       "       [3.1823602],\n",
       "       [2.8665786],\n",
       "       [3.6855536],\n",
       "       [3.3713813],\n",
       "       [2.771323 ],\n",
       "       [2.8350718],\n",
       "       [2.8629074],\n",
       "       [3.7957864],\n",
       "       [3.9996376],\n",
       "       [3.4209576],\n",
       "       [3.4469314],\n",
       "       [3.5263193],\n",
       "       [2.9011133],\n",
       "       [3.3861675],\n",
       "       [2.4831014],\n",
       "       [4.2715883],\n",
       "       [3.547332 ],\n",
       "       [3.9123693],\n",
       "       [3.2906146],\n",
       "       [4.110897 ],\n",
       "       [2.9696162],\n",
       "       [2.6921606],\n",
       "       [3.4272714],\n",
       "       [3.1270914],\n",
       "       [3.1532235],\n",
       "       [2.5236878],\n",
       "       [3.3692784],\n",
       "       [4.1666293],\n",
       "       [3.2133083],\n",
       "       [2.8324723],\n",
       "       [3.9202387],\n",
       "       [2.8948653],\n",
       "       [2.8292189],\n",
       "       [3.771409 ],\n",
       "       [3.8310137],\n",
       "       [1.6443502],\n",
       "       [3.6419935],\n",
       "       [3.3587308],\n",
       "       [3.1324112],\n",
       "       [3.494272 ],\n",
       "       [4.352886 ],\n",
       "       [3.245175 ],\n",
       "       [3.064595 ],\n",
       "       [3.5833125],\n",
       "       [3.6793182],\n",
       "       [3.311924 ],\n",
       "       [3.2902858],\n",
       "       [3.5398269],\n",
       "       [3.9831922],\n",
       "       [3.6710656],\n",
       "       [3.1419582],\n",
       "       [3.1464262],\n",
       "       [3.69012  ],\n",
       "       [3.6088655],\n",
       "       [3.3171933],\n",
       "       [4.06157  ],\n",
       "       [3.5433242],\n",
       "       [3.268891 ],\n",
       "       [4.092251 ],\n",
       "       [2.7897835],\n",
       "       [4.005576 ],\n",
       "       [4.0673933],\n",
       "       [2.8526697],\n",
       "       [3.5042603],\n",
       "       [2.9827204],\n",
       "       [1.9260786],\n",
       "       [1.6600363],\n",
       "       [3.6742306],\n",
       "       [4.116847 ],\n",
       "       [3.8480804],\n",
       "       [3.6582837],\n",
       "       [3.0882664],\n",
       "       [3.014863 ],\n",
       "       [2.7173195],\n",
       "       [3.9272225],\n",
       "       [3.6429038],\n",
       "       [3.8534672],\n",
       "       [3.2974997]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred*(range_max-range_min)+range_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_2.save_weights('weights_final/2_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot\n",
    "# !pip install graphviz\n",
    "from keras.utils import plot_model\n",
    "plot_model(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done\n"
     ]
    }
   ],
   "source": [
    "import  keras.layers  as  klayers \n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from quadratic_weighted_kappa import QWK\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "MAX_NB_WORDS=4000\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "DELTA=20\n",
    "\n",
    "fp1=open(\"glove.6B.300d.txt\",\"r\", encoding=\"utf-8\")\n",
    "glove_emb={}\n",
    "for line in fp1:\n",
    "\ttemp=line.split(\" \")\n",
    "\tglove_emb[temp[0]]=np.asarray([float(i) for i in temp[1:]])\n",
    "\n",
    "print(\"Embedding done\")\n",
    "\n",
    "\n",
    "class Neural_Tensor_layer(Layer):\n",
    "\tdef __init__(self,output_dim,input_dim=None, **kwargs):\n",
    "\t\tself.output_dim=output_dim\n",
    "\t\tself.input_dim=input_dim\n",
    "\t\tif self.input_dim:\n",
    "\t\t\tkwargs['input_shape']=(self.input_dim,)\n",
    "\t\tsuper(Neural_Tensor_layer,self).__init__(**kwargs)\n",
    "\n",
    "\tdef call(self,inputs,mask=None):\n",
    "\t\te1=inputs[0]\n",
    "\t\te2=inputs[1]\n",
    "\t\tbatch_size=K.shape(e1)[0]\n",
    "\t\tk=self.output_dim\n",
    "\t\t\n",
    "\n",
    "\t\tfeed_forward=K.dot(K.concatenate([e1,e2]),self.V)\n",
    "\n",
    "\t\tbilinear_tensor_products = [ K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1) ]\n",
    "\n",
    "\t\tfor i in range(k)[1:]:\t\n",
    "\t\t\tbtp=K.sum((e2*K.dot(e1,self.W[i]))+self.b,axis=1)\n",
    "\t\t\tbilinear_tensor_products.append(btp)\n",
    "\n",
    "\t\tresult=K.tanh(K.reshape(K.concatenate(bilinear_tensor_products,axis=0),(batch_size,k))+feed_forward)\n",
    "\n",
    "\t\treturn result\n",
    "    \n",
    "\tdef build(self,input_shape):\n",
    "\t\tmean=0.0\n",
    "\t\tstd=1.0\n",
    "\t\tk=self.output_dim\n",
    "\t\td=self.input_dim\n",
    "\t\tW_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k,d,d))\n",
    "\t\tV_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2*d,k))\n",
    "\t\tself.W=K.variable(W_val)\n",
    "\t\tself.V=K.variable(V_val)\n",
    "\t\tself.b=K.zeros((self.input_dim,))\n",
    "\t\tself.trainable_weights=[self.W,self.V,self.b]    \n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\tbatch_size=input_shape[0][0]\n",
    "\t\treturn(batch_size,self.output_dim)\n",
    "\n",
    "class Temporal_Mean_Pooling(Layer):\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Temporal_Mean_Pooling,self).__init__(**kwargs)\n",
    "\t\tself.supports_masking=True\n",
    "\t\tself.input_spec=InputSpec(ndim=3)\n",
    "        \n",
    "\tdef call(self,x,mask=None):\n",
    "\t\tif mask is None:\n",
    "\t\t\tmask=K.mean(K.ones_like(x),axis=-1)\n",
    "\n",
    "\t\tmask=K.cast(mask,K.floatx())\n",
    "\t\treturn K.sum(x,axis=-2)/K.sum(mask,axis=-1,keepdims=True)        \n",
    "\n",
    "\tdef compute_mask(self,input,mask):\n",
    "\t\treturn None\n",
    "    \n",
    "    \n",
    "\tdef compute_output_shape(self,input_shape):\n",
    "\t\treturn (input_shape[0],input_shape[2])\n",
    "\n",
    "def SKIPFLOW(embedding_matrix, vocab_size, lstm_dim=50, lr=1e-4, lr_decay=1e-6, k=5, eta=3, delta=50, activation=\"relu\", maxlen=MAX_SEQUENCE_LENGTH, seed=None):\n",
    "                                \n",
    "\tembedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\t\tmask_zero=True,\n",
    "\t\t\t\t\t\t\t\ttrainable=False)\n",
    "\tside_embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\t\tmask_zero=False,\n",
    "\t\t\t\t\t\t\t\ttrainable=False)\n",
    "\te = Input(name='essay',shape=(maxlen,))\n",
    "\t# trad_feats=Input(shape=(7,))\n",
    "\tembed = embedding_layer(e)\n",
    "\tlstm_layer=LSTM(lstm_dim,return_sequences=True)\n",
    "\thidden_states=lstm_layer(embed)\n",
    "\thtm=Temporal_Mean_Pooling()(hidden_states)\n",
    "\tside_embed = side_embedding_layer(e)\n",
    "\tside_hidden_states=lstm_layer(side_embed)\t\n",
    "\t# lstm_layer=LSTM(lstm_dim,return_sequences=True)\n",
    "\ttensor_layer=Neural_Tensor_layer(output_dim=k,input_dim=lstm_dim)\n",
    "\tpairs = [((eta + i * delta) % maxlen, (eta + i * delta + delta) % maxlen) for i in range(maxlen // delta)]\n",
    "\thidden_pairs = [ (Lambda(lambda t: t[:, p[0], :])(side_hidden_states), Lambda(lambda t: t[:, p[1], :])(side_hidden_states)) for p in pairs]\n",
    "\tsigmoid = Dense(1, activation=\"sigmoid\", kernel_initializer=initializers.glorot_normal(seed=seed))\n",
    "\tcoherence = [sigmoid(tensor_layer([hp[0], hp[1]])) for hp in hidden_pairs]\n",
    "\tco_tm=Concatenate()(coherence[:]+[htm])\n",
    "\tdense = Dense(256, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(co_tm)\n",
    "\tdense = Dense(128, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "\tdense = Dense(64, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "\tout = Dense(1, activation=\"sigmoid\")(dense)\n",
    "\tmodel = Model(inputs=[e], outputs=[out])\n",
    "\tadam = Adam(lr=lr, decay=lr_decay)\n",
    "\tmodel.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"MSE\"])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def init(essay_type = '4'): \n",
    "\n",
    "\tEMBEDDING_DIM=300\n",
    "\tMAX_NB_WORDS=4000\n",
    "\tMAX_SEQUENCE_LENGTH=500\n",
    "\tDELTA=20\n",
    "\n",
    "\ttexts=[]\n",
    "\toriginals = []\n",
    "\n",
    "\tfp=open(\"C:/Users/mehar/Desktop/MIDAS_CallingBluff/skipflow/data/training_set_rel3.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "\tfp.readline()\n",
    "\tsentences=[]\n",
    "\tdoctovec=[]\n",
    "\tfor line in fp:\n",
    "\t    temp=line.split(\"\\t\")\n",
    "\t    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "\t        texts.append(temp[2])\n",
    "\t        originals.append(float(temp[6]))\n",
    "\t        line=temp[2].strip()\n",
    "\tfp.close()\n",
    "\n",
    "\trange_min = min(originals)\n",
    "\trange_max = max(originals)\n",
    "\tprint(\"range min - \", min(originals) , \" ; range max - \", max(originals))\n",
    "\n",
    "\ttokenizer=Tokenizer() #num_words=MAX_NB_WORDS) #limits vocabulory size\n",
    "\ttokenizer.fit_on_texts(texts)\n",
    "\tsequences=tokenizer.texts_to_sequences(texts) \n",
    "\tword_index=tokenizer.word_index #dictionary mapping\n",
    "\n",
    "\t# print('Found %s unique tokens.' % len(word_index)) \n",
    "\tvocab_size = len(word_index)\n",
    "\tembedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "\tfor word,i in word_index.items():\n",
    "\t\tif(i>=len(word_index)):\n",
    "\t\t\tcontinue\n",
    "\t\tif word in glove_emb:\n",
    "\t\t\t\tembedding_matrix[i]=glove_emb[word]     \t\n",
    "\n",
    "\tearlystopping = EarlyStopping(monitor=\"val_mean_squared_error\", patience=5)\n",
    "\tsf_1 = SKIPFLOW(embedding_matrix, vocab_size ,lstm_dim=50, lr=2e-4, lr_decay=2e-6, k=4, eta=13, delta=50, activation=\"relu\", seed=None)\n",
    "\n",
    "\tsf_1.load_weights(\"C:/Users/mehar/Desktop/MIDAS_CallingBluff/skipflow/Automatic-Text-Scoring/webapp/models/weights_final/\" + str(essay_type) + \"_weights.h5\")\n",
    "\tprint(\"Loaded Model from disk\")\n",
    "\n",
    "\n",
    "\tgraph = tf.get_default_graph()\n",
    "\n",
    "\tprint(\"Essay type done loading ::: \", essay_type)\n",
    "\n",
    "\treturn sf_1,graph,glove_emb, tokenizer, range_min, range_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
