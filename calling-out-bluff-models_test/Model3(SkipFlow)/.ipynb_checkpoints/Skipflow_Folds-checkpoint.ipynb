{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/Saiteja-Reddy/Automatic-Text-Scoring.git\n",
    "import  keras.layers  as  klayers \n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from quadratic_weighted_kappa import QWK\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Tensor_layer(Layer):\n",
    "\tdef __init__(self,output_dim,input_dim=None, **kwargs):\n",
    "\t\tself.output_dim=output_dim\n",
    "\t\tself.input_dim=input_dim\n",
    "\t\tif self.input_dim:\n",
    "\t\t\tkwargs['input_shape']=(self.input_dim,)\n",
    "# \t\tprint(\"YAYY\", input_dim, output_dim)\n",
    "\t\tsuper(Neural_Tensor_layer,self).__init__(**kwargs)\n",
    "\n",
    "\tdef call(self,inputs,mask=None):\n",
    "\t\te1=inputs[0]\n",
    "\t\te2=inputs[1]\n",
    "\t\tbatch_size=K.shape(e1)[0]\n",
    "\t\tk=self.output_dim\n",
    "\t\t\n",
    "\n",
    "\t\tfeed_forward=K.dot(K.concatenate([e1,e2]),self.V)\n",
    "\n",
    "\t\tbilinear_tensor_products = [ K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1) ]\n",
    "\n",
    "\t\tfor i in range(k)[1:]:\t\n",
    "\t\t\tbtp=K.sum((e2*K.dot(e1,self.W[i]))+self.b,axis=1)\n",
    "\t\t\tbilinear_tensor_products.append(btp)\n",
    "\n",
    "\t\tresult=K.tanh(K.reshape(K.concatenate(bilinear_tensor_products,axis=0),(batch_size,k))+feed_forward)\n",
    "\n",
    "\t\treturn result\n",
    "    \n",
    "\tdef build(self,input_shape):\n",
    "\t\tmean=0.0\n",
    "\t\tstd=1.0\n",
    "\t\tk=self.output_dim\n",
    "\t\td=self.input_dim\n",
    "\t\t##truncnorm generate continuous random numbers in given range\n",
    "\t\tW_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k,d,d))\n",
    "\t\tV_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2*d,k))\n",
    "\t\tself.W=K.variable(W_val)\n",
    "\t\tself.V=K.variable(V_val)\n",
    "\t\tself.b=K.zeros((self.input_dim,))\n",
    "\t\tself.trainable_weights=[self.W,self.V,self.b]    \n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\tbatch_size=input_shape[0][0]\n",
    "\t\treturn(batch_size,self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Mean_Pooling(Layer): # conversion from (samples,timesteps,features) to (samples,features)\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Temporal_Mean_Pooling,self).__init__(**kwargs)\n",
    "\t\t# masked values in x (number_of_samples,time)\n",
    "\t\tself.supports_masking=True\n",
    "\t\t# Specifies number of dimensions to each layer\n",
    "\t\tself.input_spec=InputSpec(ndim=3)\n",
    "        \n",
    "\tdef call(self,x,mask=None):\n",
    "\t\tif mask is None:\n",
    "\t\t\tmask=K.mean(K.ones_like(x),axis=-1)\n",
    "\n",
    "\t\tmask=K.cast(mask,K.floatx())\n",
    "\t\t\t\t#dimension size single vec/number of samples\n",
    "\t\treturn K.sum(x,axis=-2)/K.sum(mask,axis=-1,keepdims=True)        \n",
    "\n",
    "\tdef compute_mask(self,input,mask):\n",
    "\t\treturn None\n",
    "    \n",
    "    \n",
    "\tdef compute_output_shape(self,input_shape):\n",
    "\t\treturn (input_shape[0],input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "MAX_NB_WORDS=4000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "VALIDATION_SPLIT=0.20\n",
    "DELTA=20\n",
    "\n",
    "texts=[]\n",
    "labels=[]\n",
    "sentences=[]\n",
    "\n",
    "originals = []\n",
    "\n",
    "fp1=open(\"glove.6B.300d.txt\",\"r\", encoding=\"utf-8\")\n",
    "glove_emb={}\n",
    "for line in fp1:\n",
    "\ttemp=line.split(\" \")\n",
    "\tglove_emb[temp[0]]=np.asarray([float(i) for i in temp[1:]])\n",
    "\n",
    "print(\"Embedding done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range min -  2.0  ; range max -  12.0\n"
     ]
    }
   ],
   "source": [
    "essay_type = '1'\n",
    "\n",
    "fp=open(\"data/training_set_rel3.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "originals = []\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        originals.append(float(temp[6]))\n",
    "# print(originals)\n",
    "fp.close()\n",
    "# print(originals)\n",
    "print(\"range min - \", min(originals) , \" ; range max - \", max(originals))\n",
    "\n",
    "range_min = min(originals)\n",
    "range_max = max(originals)\n",
    "\n",
    "fp=open(\"data/training_set_rel3.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "sentences=[]\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        texts.append(temp[2])\n",
    "        labels.append((float(temp[6])-range_min)/(range_max-range_min)) ## why ??  - normalize to range [0-1]\n",
    "        line=temp[2].strip()\n",
    "        sentences.append(nltk.tokenize.word_tokenize(line))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1783\n"
     ]
    }
   ],
   "source": [
    "labels\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Dear @CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many people have myspace, facebooks, aim, these all benefit us by having conversations with one another. Many people believe computers are bad but how can you make friends if you can never talk to them? I am very fortunate for having a computer that can help with not only school work but my social life and how I make friends. Computers help us with finding our locations, coordibates and millions of information online. If we didn\\'t go on the internet a lot we wouldn\\'t know how to go onto websites that @MONTH1 help us with locations and coordinates like @LOCATION1. Would you rather use a computer or be in @LOCATION3. When your supposed to be vacationing in @LOCATION2. Million of information is found on the internet. You can as almost every question and a computer will have it. Would you rather easily draw up a house plan on the computers or take @NUM1 hours doing one by hand with ugly erazer marks all over it, you are garrenteed that to find a job with a drawing like that. Also when appling for a job many workers must write very long papers like a @NUM3 word essay on why this job fits you the most, and many people I know don\\'t like writing @NUM3 words non-stopp for hours when it could take them I hav an a computer. That is why computers we needed a lot now adays. I hope this essay has impacted your descion on computers because they are great machines to work with. The other day I showed my mom how to use a computer and she said it was the greatest invention sense sliced bread! Now go out and buy a computer to help you chat online with friends, find locations and millions of information on one click of the button and help your self with getting a job with neat, prepared, printed work that your boss will love.\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text labels appended 1783\n",
      "[0.6 0.7 0.5 ... 0.6 0.  0.5]\n",
      "1783\n"
     ]
    }
   ],
   "source": [
    "print(\"text labels appended %s\" %len(texts))\n",
    "\n",
    "labels=np.asarray(labels)\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "\ttemp1=np.zeros((1, EMBEDDING_DIM))\n",
    "\tfor w in i:\n",
    "\t\tif(w in glove_emb):\n",
    "\t\t\ttemp1+=glove_emb[w]\n",
    "\ttemp1/=len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16200 unique tokens.\n",
      "Shape of data tensor: (1783, 500)\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(num_words = MAX_NB_WORDS) #num_words=MAX_NB_WORDS) #limits vocabulory size\n",
    "tokenizer.fit_on_texts(texts) #encoding the text\n",
    "sequences=tokenizer.texts_to_sequences(texts) #returns list of sequences\n",
    "word_index=tokenizer.word_index #dictionary mapping, word and specific token for that word...\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #padding to max_length\n",
    "\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783, 500)\n",
      "(1783, 500)\n",
      "(1783,)\n",
      "356\n"
     ]
    }
   ],
   "source": [
    "indices=np.arange(data.shape[0]) #with one argument, start=0, step =1\n",
    "print(data.shape)\n",
    "np.random.shuffle(indices)\n",
    "data=data[indices]\n",
    "print(data.shape)\n",
    "labels=labels[indices]\n",
    "# np.reshape(labels, ())\n",
    "print(labels.shape)\n",
    "validation_size=int(VALIDATION_SPLIT*data.shape[0])\n",
    "print(validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1427, 500)\n",
      "(1427,)\n",
      "(356, 500)\n"
     ]
    }
   ],
   "source": [
    "x_train=data[:-validation_size] #data-validation data\n",
    "print(x_train.shape)\n",
    "# print(x_train)\n",
    "# print(labels)\n",
    "y_train=labels[:-validation_size]\n",
    "# print(y_train.transpose)\n",
    "print(y_train.shape)\n",
    "# y_train = np.reshape(y_train, (1427, 1))\n",
    "# print(y_train_new)\n",
    "# print(y_train)\n",
    "x_val=data[-validation_size:]\n",
    "print(x_val.shape)\n",
    "y_val=labels[-validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16200, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200\n"
     ]
    }
   ],
   "source": [
    "for word,i in word_index.items():\n",
    "\tif(i>=len(word_index)):\n",
    "\t\tcontinue\n",
    "\tif word in glove_emb:\n",
    "\t\t\tembedding_matrix[i]=glove_emb[word]\n",
    "vocab_size=len(word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0213 00:57:02.740021 140253389707072 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=True,\n",
    "\t\t\t\t\t\t\ttrainable=False)\n",
    "# print(embedding_layer.shape)\n",
    "side_embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=False,\n",
    "\t\t\t\t\t\t\ttrainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SKIPFLOW(lstm_dim=50, lr=1e-4, lr_decay=1e-6, k=4, eta=3, delta=50, activation=\"relu\", maxlen=MAX_SEQUENCE_LENGTH, seed=None):\n",
    "    e = Input(name='essay',shape=(maxlen,))\n",
    "    print(\"e\", e)\n",
    "#     trad_feats=Input(shape=(7,))\n",
    "#     print(\"trad_feats\", trad_feats)\n",
    "    embed = embedding_layer(e)\n",
    "    print(embed.shape)\n",
    "    lstm_layer=LSTM(lstm_dim,return_sequences=True)\n",
    "    print(lstm_layer)\n",
    "    hidden_states=lstm_layer(embed)\n",
    "    htm=Temporal_Mean_Pooling()(hidden_states)    \n",
    "    side_embed = side_embedding_layer(e)\n",
    "    side_hidden_states=lstm_layer(side_embed)    \n",
    "    tensor_layer=Neural_Tensor_layer(output_dim=k,input_dim=500)\n",
    "#     print(input_dim, output_dim)\n",
    "    pairs = [((eta + i * delta) % maxlen, (eta + i * delta + delta) % maxlen) for i in range(maxlen // delta)]\n",
    "    hidden_pairs = [ (Lambda(lambda t: t[:, p[0], :])(side_hidden_states), Lambda(lambda t: t[:, p[1], :])(side_hidden_states)) for p in pairs]\n",
    "    sigmoid = Dense(1, activation=\"sigmoid\", kernel_initializer=initializers.glorot_normal(seed=seed))\n",
    "    coherence = [sigmoid(tensor_layer([hp[0], hp[1]])) for hp in hidden_pairs]\n",
    "    co_tm=Concatenate()(coherence[:]+[htm])\n",
    "    dense = Dense(256, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(co_tm)\n",
    "    dense = Dense(128, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    dense = Dense(64, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    out = Dense(1, activation=\"sigmoid\")(dense)\n",
    "    model = Model(inputs=[e], outputs=[out])\n",
    "    print(\"input\", [e])\n",
    "    print(\"outputs\", out)\n",
    "    adam = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"MSE\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0213 00:57:09.547019 140253389707072 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0213 00:57:09.562562 140253389707072 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0213 00:57:09.584090 140253389707072 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0213 00:57:09.585324 140253389707072 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e Tensor(\"essay:0\", shape=(?, 500), dtype=float32)\n",
      "(?, 500, 300)\n",
      "<keras.layers.recurrent.LSTM object at 0x7f8f28024b00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0213 00:57:17.741962 140253389707072 deprecation.py:323] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0213 00:57:19.041826 140253389707072 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [<tf.Tensor 'essay:0' shape=(?, 500) dtype=float32>]\n",
      "outputs Tensor(\"dense_5/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "earlystopping = EarlyStopping(monitor=\"val_mean_squared_error\", patience=5)\n",
    "sf_1 = SKIPFLOW(lstm_dim=500, lr=2e-4, lr_decay=2e-6, k=4, eta=13, delta=50, activation=\"relu\", seed=None)\n",
    "# sf_1.summary()\n",
    "# plot_model(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Train on 1427 samples, validate on 356 samples\n",
      "Epoch 1/100\n",
      "1427/1427 [==============================] - 91s 64ms/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0117 - val_mean_squared_error: 0.0117\n",
      "Epoch 2/100\n",
      "1427/1427 [==============================] - 85s 60ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "Epoch 3/100\n",
      "1427/1427 [==============================] - 85s 60ms/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 4/100\n",
      "1427/1427 [==============================] - 85s 59ms/step - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 5/100\n",
      "1427/1427 [==============================] - 85s 60ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 6/100\n",
      "1427/1427 [==============================] - 85s 60ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "Epoch 7/100\n",
      "1427/1427 [==============================] - 84s 59ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 8/100\n",
      "1427/1427 [==============================] - 85s 60ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 9/100\n",
      "1427/1427 [==============================] - 86s 60ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 10/100\n",
      "1427/1427 [==============================] - 86s 60ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 11/100\n",
      "1427/1427 [==============================] - 86s 60ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 12/100\n",
      "1427/1427 [==============================] - 85s 60ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 13/100\n",
      "1427/1427 [==============================] - 86s 60ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 14/100\n",
      "1427/1427 [==============================] - 86s 60ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n"
     ]
    }
   ],
   "source": [
    "# print(sf_1)\n",
    "epochs = 100\n",
    "# epochs = 1000\n",
    "print(type(x_train))\n",
    "# y_train = np.asarray(y_train)\n",
    "print(type(y_train))\n",
    "\n",
    "hist = sf_1.fit(x_train, y_train, batch_size=32, epochs=epochs, validation_data=([x_val], y_val), callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70774025],\n",
       "       [0.5912255 ],\n",
       "       [0.5734383 ],\n",
       "       [0.7691392 ],\n",
       "       [0.51335275],\n",
       "       [0.66869515],\n",
       "       [0.8531337 ],\n",
       "       [0.7339651 ],\n",
       "       [0.7860868 ],\n",
       "       [0.63804895],\n",
       "       [0.74698055],\n",
       "       [0.77318627],\n",
       "       [0.6236764 ],\n",
       "       [0.6833696 ],\n",
       "       [0.6161226 ],\n",
       "       [0.7201469 ],\n",
       "       [0.639722  ],\n",
       "       [0.6848018 ],\n",
       "       [0.7321454 ],\n",
       "       [0.6117475 ],\n",
       "       [0.632741  ],\n",
       "       [0.56823814],\n",
       "       [0.5354194 ],\n",
       "       [0.6803205 ],\n",
       "       [0.5743875 ],\n",
       "       [0.60661364],\n",
       "       [0.57435834],\n",
       "       [0.8297057 ],\n",
       "       [0.5532635 ],\n",
       "       [0.61760247],\n",
       "       [0.74363005],\n",
       "       [0.80698466],\n",
       "       [0.62084246],\n",
       "       [0.6402535 ],\n",
       "       [0.72480124],\n",
       "       [0.638856  ],\n",
       "       [0.7170398 ],\n",
       "       [0.6650792 ],\n",
       "       [0.7601019 ],\n",
       "       [0.8586998 ],\n",
       "       [0.6031967 ],\n",
       "       [0.51991963],\n",
       "       [0.80932164],\n",
       "       [0.6512126 ],\n",
       "       [0.7358947 ],\n",
       "       [0.597873  ],\n",
       "       [0.5933788 ],\n",
       "       [0.6266549 ],\n",
       "       [0.7908952 ],\n",
       "       [0.6351931 ],\n",
       "       [0.724663  ],\n",
       "       [0.8492104 ],\n",
       "       [0.47706342],\n",
       "       [0.6787615 ],\n",
       "       [0.7109518 ],\n",
       "       [0.7118258 ],\n",
       "       [0.81254965],\n",
       "       [0.810331  ],\n",
       "       [0.6233382 ],\n",
       "       [0.7374973 ],\n",
       "       [0.7505795 ],\n",
       "       [0.5496211 ],\n",
       "       [0.47031885],\n",
       "       [0.7154398 ],\n",
       "       [0.7816329 ],\n",
       "       [0.48201448],\n",
       "       [0.67924064],\n",
       "       [0.29305178],\n",
       "       [0.8056349 ],\n",
       "       [0.45471686],\n",
       "       [0.7310152 ],\n",
       "       [0.63849115],\n",
       "       [0.66257685],\n",
       "       [0.79324955],\n",
       "       [0.59181607],\n",
       "       [0.7888633 ],\n",
       "       [0.58952147],\n",
       "       [0.65034413],\n",
       "       [0.2979511 ],\n",
       "       [0.44169158],\n",
       "       [0.715028  ],\n",
       "       [0.519028  ],\n",
       "       [0.80200243],\n",
       "       [0.8170067 ],\n",
       "       [0.61053145],\n",
       "       [0.3829087 ],\n",
       "       [0.6532607 ],\n",
       "       [0.61244917],\n",
       "       [0.7338997 ],\n",
       "       [0.64665604],\n",
       "       [0.6379212 ],\n",
       "       [0.72909176],\n",
       "       [0.53782046],\n",
       "       [0.6321394 ],\n",
       "       [0.67451495],\n",
       "       [0.73546875],\n",
       "       [0.6658572 ],\n",
       "       [0.86087644],\n",
       "       [0.6853391 ],\n",
       "       [0.6907572 ],\n",
       "       [0.5768181 ],\n",
       "       [0.75492156],\n",
       "       [0.65290415],\n",
       "       [0.7758089 ],\n",
       "       [0.672021  ],\n",
       "       [0.70481026],\n",
       "       [0.8380974 ],\n",
       "       [0.52526134],\n",
       "       [0.7087971 ],\n",
       "       [0.8064995 ],\n",
       "       [0.77394927],\n",
       "       [0.84150034],\n",
       "       [0.6626419 ],\n",
       "       [0.5837505 ],\n",
       "       [0.7389959 ],\n",
       "       [0.63449264],\n",
       "       [0.64001036],\n",
       "       [0.69446146],\n",
       "       [0.8421464 ],\n",
       "       [0.6419434 ],\n",
       "       [0.64838284],\n",
       "       [0.6429087 ],\n",
       "       [0.5924487 ],\n",
       "       [0.4365404 ],\n",
       "       [0.6006229 ],\n",
       "       [0.5281894 ],\n",
       "       [0.5399295 ],\n",
       "       [0.64916176],\n",
       "       [0.5551766 ],\n",
       "       [0.6999006 ],\n",
       "       [0.57902473],\n",
       "       [0.61029875],\n",
       "       [0.46481907],\n",
       "       [0.6694112 ],\n",
       "       [0.7073766 ],\n",
       "       [0.33853698],\n",
       "       [0.7685251 ],\n",
       "       [0.62559265],\n",
       "       [0.8160598 ],\n",
       "       [0.5292991 ],\n",
       "       [0.63749087],\n",
       "       [0.84823763],\n",
       "       [0.5890647 ],\n",
       "       [0.7085606 ],\n",
       "       [0.8126986 ],\n",
       "       [0.6188629 ],\n",
       "       [0.8020303 ],\n",
       "       [0.673711  ],\n",
       "       [0.82388353],\n",
       "       [0.50490797],\n",
       "       [0.5178122 ],\n",
       "       [0.67205894],\n",
       "       [0.65378726],\n",
       "       [0.6304801 ],\n",
       "       [0.5600847 ],\n",
       "       [0.8637737 ],\n",
       "       [0.49938336],\n",
       "       [0.5080223 ],\n",
       "       [0.7457325 ],\n",
       "       [0.78479135],\n",
       "       [0.77074695],\n",
       "       [0.6757159 ],\n",
       "       [0.63577765],\n",
       "       [0.34807503],\n",
       "       [0.646255  ],\n",
       "       [0.79354376],\n",
       "       [0.79834425],\n",
       "       [0.27245796],\n",
       "       [0.67481345],\n",
       "       [0.8180211 ],\n",
       "       [0.62526566],\n",
       "       [0.8394155 ],\n",
       "       [0.41704702],\n",
       "       [0.47297987],\n",
       "       [0.6063445 ],\n",
       "       [0.6457604 ],\n",
       "       [0.83262825],\n",
       "       [0.8304259 ],\n",
       "       [0.66008055],\n",
       "       [0.55006075],\n",
       "       [0.6631786 ],\n",
       "       [0.50659364],\n",
       "       [0.6431751 ],\n",
       "       [0.6054566 ],\n",
       "       [0.783979  ],\n",
       "       [0.63671905],\n",
       "       [0.8031017 ],\n",
       "       [0.78216445],\n",
       "       [0.6560116 ],\n",
       "       [0.80205214],\n",
       "       [0.44710997],\n",
       "       [0.65158796],\n",
       "       [0.60482824],\n",
       "       [0.45423833],\n",
       "       [0.76092553],\n",
       "       [0.8108294 ],\n",
       "       [0.6244682 ],\n",
       "       [0.6249423 ],\n",
       "       [0.79110384],\n",
       "       [0.6237346 ],\n",
       "       [0.64895034],\n",
       "       [0.71893877],\n",
       "       [0.6700795 ],\n",
       "       [0.64195806],\n",
       "       [0.6068232 ],\n",
       "       [0.57713556],\n",
       "       [0.6421869 ],\n",
       "       [0.582592  ],\n",
       "       [0.6703974 ],\n",
       "       [0.66865593],\n",
       "       [0.56573   ],\n",
       "       [0.672964  ],\n",
       "       [0.75062954],\n",
       "       [0.80429304],\n",
       "       [0.65365803],\n",
       "       [0.73454523],\n",
       "       [0.59401464],\n",
       "       [0.2864729 ],\n",
       "       [0.7091875 ],\n",
       "       [0.58422583],\n",
       "       [0.77035046],\n",
       "       [0.70710576],\n",
       "       [0.6058699 ],\n",
       "       [0.75805485],\n",
       "       [0.70295316],\n",
       "       [0.7132512 ],\n",
       "       [0.78919744],\n",
       "       [0.78634775],\n",
       "       [0.6613953 ],\n",
       "       [0.5938059 ],\n",
       "       [0.78251207],\n",
       "       [0.5970119 ],\n",
       "       [0.62319505],\n",
       "       [0.5947225 ],\n",
       "       [0.66745436],\n",
       "       [0.8008651 ],\n",
       "       [0.8399528 ],\n",
       "       [0.67176545],\n",
       "       [0.6421543 ],\n",
       "       [0.50630575],\n",
       "       [0.7802402 ],\n",
       "       [0.66211516],\n",
       "       [0.7512411 ],\n",
       "       [0.8306797 ],\n",
       "       [0.66680145],\n",
       "       [0.7704646 ],\n",
       "       [0.7142165 ],\n",
       "       [0.59617954],\n",
       "       [0.61384505],\n",
       "       [0.6327225 ],\n",
       "       [0.850783  ],\n",
       "       [0.5684355 ],\n",
       "       [0.6164542 ],\n",
       "       [0.6140018 ],\n",
       "       [0.7822983 ],\n",
       "       [0.53933907],\n",
       "       [0.6837049 ],\n",
       "       [0.85969335],\n",
       "       [0.27420488],\n",
       "       [0.85745513],\n",
       "       [0.6618522 ],\n",
       "       [0.6239493 ],\n",
       "       [0.6398913 ],\n",
       "       [0.81014955],\n",
       "       [0.43087038],\n",
       "       [0.6909537 ],\n",
       "       [0.6486353 ],\n",
       "       [0.6646996 ],\n",
       "       [0.4453662 ],\n",
       "       [0.80537856],\n",
       "       [0.5895219 ],\n",
       "       [0.8190446 ],\n",
       "       [0.7199455 ],\n",
       "       [0.66021055],\n",
       "       [0.8128358 ],\n",
       "       [0.59420305],\n",
       "       [0.68597424],\n",
       "       [0.6365891 ],\n",
       "       [0.65621   ],\n",
       "       [0.59907794],\n",
       "       [0.83219516],\n",
       "       [0.65995884],\n",
       "       [0.65607786],\n",
       "       [0.46213055],\n",
       "       [0.68716645],\n",
       "       [0.6555413 ],\n",
       "       [0.6864789 ],\n",
       "       [0.6669564 ],\n",
       "       [0.62445337],\n",
       "       [0.5897432 ],\n",
       "       [0.31599078],\n",
       "       [0.5113278 ],\n",
       "       [0.6241156 ],\n",
       "       [0.44864962],\n",
       "       [0.3134101 ],\n",
       "       [0.7179871 ],\n",
       "       [0.6875603 ],\n",
       "       [0.7188819 ],\n",
       "       [0.6002503 ],\n",
       "       [0.611519  ],\n",
       "       [0.58727187],\n",
       "       [0.5190326 ],\n",
       "       [0.74603474],\n",
       "       [0.69689703],\n",
       "       [0.8221568 ],\n",
       "       [0.61869216],\n",
       "       [0.69912475],\n",
       "       [0.7379968 ],\n",
       "       [0.6312628 ],\n",
       "       [0.6994602 ],\n",
       "       [0.25675744],\n",
       "       [0.75199753],\n",
       "       [0.65695727],\n",
       "       [0.6167374 ],\n",
       "       [0.6211298 ],\n",
       "       [0.6884252 ],\n",
       "       [0.6455033 ],\n",
       "       [0.5817186 ],\n",
       "       [0.63541365],\n",
       "       [0.6368809 ],\n",
       "       [0.45781097],\n",
       "       [0.6044454 ],\n",
       "       [0.43305817],\n",
       "       [0.7715328 ],\n",
       "       [0.62317604],\n",
       "       [0.6577062 ],\n",
       "       [0.82557476],\n",
       "       [0.7271384 ],\n",
       "       [0.58598995],\n",
       "       [0.8087537 ],\n",
       "       [0.6937407 ],\n",
       "       [0.4290553 ],\n",
       "       [0.6906066 ],\n",
       "       [0.6602688 ],\n",
       "       [0.7888319 ],\n",
       "       [0.7524743 ],\n",
       "       [0.42982036],\n",
       "       [0.5230398 ],\n",
       "       [0.61900455],\n",
       "       [0.5931449 ],\n",
       "       [0.7629963 ],\n",
       "       [0.62987345],\n",
       "       [0.6020423 ],\n",
       "       [0.67707527],\n",
       "       [0.83831275],\n",
       "       [0.71726143],\n",
       "       [0.67554355],\n",
       "       [0.50984627],\n",
       "       [0.66756797],\n",
       "       [0.7977454 ],\n",
       "       [0.6347451 ],\n",
       "       [0.60690576],\n",
       "       [0.62584865],\n",
       "       [0.65510637],\n",
       "       [0.5916741 ],\n",
       "       [0.62086356]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=sf_1.predict([x_val])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 8, 9, 8, 6, 9, 10, 9, 10, 10, 9, 10, 8, 8, 8, 9, 8, 9, 10, 8, 8, 8, 7, 8, 8, 8, 8, 10, 8, 8, 9, 12, 8, 8, 10, 8, 9, 9, 10, 11, 8, 8, 11, 10, 8, 9, 8, 8, 9, 10, 9, 12, 7, 10, 8, 8, 12, 11, 8, 10, 8, 9, 7, 9, 10, 7, 11, 6, 9, 7, 11, 8, 9, 8, 7, 11, 8, 8, 4, 6, 10, 8, 10, 9, 9, 8, 8, 7, 10, 8, 9, 8, 6, 9, 10, 10, 8, 12, 8, 10, 8, 10, 8, 11, 7, 8, 10, 7, 8, 10, 10, 10, 8, 8, 9, 8, 8, 9, 10, 9, 8, 8, 8, 6, 6, 6, 8, 8, 8, 9, 8, 8, 7, 8, 8, 6, 10, 8, 9, 6, 9, 12, 9, 8, 10, 9, 9, 7, 10, 7, 6, 8, 9, 10, 7, 10, 7, 6, 8, 10, 9, 8, 8, 6, 8, 9, 10, 5, 9, 10, 8, 12, 5, 6, 8, 8, 11, 10, 8, 6, 9, 7, 9, 8, 11, 8, 10, 10, 8, 10, 4, 9, 8, 8, 9, 10, 8, 8, 8, 8, 8, 8, 9, 8, 8, 7, 9, 8, 9, 8, 8, 8, 8, 11, 9, 10, 7, 2, 10, 8, 10, 8, 9, 8, 9, 8, 10, 10, 8, 8, 10, 7, 8, 8, 8, 10, 10, 8, 8, 7, 10, 9, 9, 11, 8, 10, 8, 8, 8, 10, 12, 6, 9, 8, 10, 6, 8, 12, 4, 11, 8, 8, 8, 11, 6, 10, 9, 8, 7, 8, 8, 11, 9, 8, 10, 8, 11, 7, 8, 8, 12, 8, 8, 6, 9, 10, 9, 9, 8, 8, 6, 7, 8, 6, 5, 8, 10, 10, 8, 8, 8, 6, 10, 10, 10, 8, 9, 10, 8, 8, 5, 10, 11, 8, 8, 8, 8, 7, 8, 7, 6, 8, 6, 10, 8, 8, 11, 11, 8, 10, 8, 5, 9, 8, 11, 9, 6, 7, 7, 8, 10, 9, 8, 9, 10, 8, 9, 7, 10, 10, 8, 8, 8, 8, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "y_val_fin = [int(round(a*(range_max-range_min)+range_min)) for a in y_val]\n",
    "print(y_val_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, 8, 10, 7, 9, 11, 9, 10, 8, 9, 10, 8, 9, 8, 9, 8, 9, 9, 8, 8, 8, 7, 9, 8, 8, 8, 10, 8, 8, 9, 10, 8, 8, 9, 8, 9, 9, 10, 11, 8, 7, 10, 9, 9, 8, 8, 8, 10, 8, 9, 10, 7, 9, 9, 9, 10, 10, 8, 9, 10, 7, 7, 9, 10, 7, 9, 5, 10, 7, 9, 8, 9, 10, 8, 10, 8, 9, 5, 6, 9, 7, 10, 10, 8, 6, 9, 8, 9, 8, 8, 9, 7, 8, 9, 9, 9, 11, 9, 9, 8, 10, 9, 10, 9, 9, 10, 7, 9, 10, 10, 10, 9, 8, 9, 8, 8, 9, 10, 8, 8, 8, 8, 6, 8, 7, 7, 8, 8, 9, 8, 8, 7, 9, 9, 5, 10, 8, 10, 7, 8, 10, 8, 9, 10, 8, 10, 9, 10, 7, 7, 9, 9, 8, 8, 11, 7, 7, 9, 10, 10, 9, 8, 5, 8, 10, 10, 5, 9, 10, 8, 10, 6, 7, 8, 8, 10, 10, 9, 8, 9, 7, 8, 8, 10, 8, 10, 10, 9, 10, 6, 9, 8, 7, 10, 10, 8, 8, 10, 8, 8, 9, 9, 8, 8, 8, 8, 8, 9, 9, 8, 9, 10, 10, 9, 9, 8, 5, 9, 8, 10, 9, 8, 10, 9, 9, 10, 10, 9, 8, 10, 8, 8, 8, 9, 10, 10, 9, 8, 7, 10, 9, 10, 10, 9, 10, 9, 8, 8, 8, 11, 8, 8, 8, 10, 7, 9, 11, 5, 11, 9, 8, 8, 10, 6, 9, 8, 9, 6, 10, 8, 10, 9, 9, 10, 8, 9, 8, 9, 8, 10, 9, 9, 7, 9, 9, 9, 9, 8, 8, 5, 7, 8, 6, 5, 9, 9, 9, 8, 8, 8, 7, 9, 9, 10, 8, 9, 9, 8, 9, 5, 10, 9, 8, 8, 9, 8, 8, 8, 8, 7, 8, 6, 10, 8, 9, 10, 9, 8, 10, 9, 6, 9, 9, 10, 10, 6, 7, 8, 8, 10, 8, 8, 9, 10, 9, 9, 7, 9, 10, 8, 8, 8, 9, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "y_pred_fin =[int(round(a*(range_max-range_min)+range_min)) for a in y_pred.reshape(356).tolist()]\n",
    "print(y_pred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7962121212121211\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_val_fin,y_pred_fin,weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def QWK_new(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = Cmatrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7943090425240287"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QWK_new(y_val_fin, y_pred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_1.save('model_final/1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.077402 ],\n",
       "       [ 7.9122553],\n",
       "       [ 7.7343826],\n",
       "       [ 9.691392 ],\n",
       "       [ 7.1335278],\n",
       "       [ 8.686952 ],\n",
       "       [10.531337 ],\n",
       "       [ 9.339651 ],\n",
       "       [ 9.860868 ],\n",
       "       [ 8.380489 ],\n",
       "       [ 9.469806 ],\n",
       "       [ 9.731863 ],\n",
       "       [ 8.236764 ],\n",
       "       [ 8.833696 ],\n",
       "       [ 8.161226 ],\n",
       "       [ 9.201469 ],\n",
       "       [ 8.39722  ],\n",
       "       [ 8.848019 ],\n",
       "       [ 9.321454 ],\n",
       "       [ 8.1174755],\n",
       "       [ 8.32741  ],\n",
       "       [ 7.6823816],\n",
       "       [ 7.354194 ],\n",
       "       [ 8.8032055],\n",
       "       [ 7.743875 ],\n",
       "       [ 8.066136 ],\n",
       "       [ 7.7435837],\n",
       "       [10.297057 ],\n",
       "       [ 7.5326347],\n",
       "       [ 8.176024 ],\n",
       "       [ 9.4363   ],\n",
       "       [10.069847 ],\n",
       "       [ 8.208425 ],\n",
       "       [ 8.4025345],\n",
       "       [ 9.248013 ],\n",
       "       [ 8.388559 ],\n",
       "       [ 9.170399 ],\n",
       "       [ 8.650791 ],\n",
       "       [ 9.601019 ],\n",
       "       [10.586998 ],\n",
       "       [ 8.031967 ],\n",
       "       [ 7.1991963],\n",
       "       [10.093216 ],\n",
       "       [ 8.512126 ],\n",
       "       [ 9.358947 ],\n",
       "       [ 7.9787297],\n",
       "       [ 7.933788 ],\n",
       "       [ 8.266549 ],\n",
       "       [ 9.908953 ],\n",
       "       [ 8.351931 ],\n",
       "       [ 9.24663  ],\n",
       "       [10.492104 ],\n",
       "       [ 6.770634 ],\n",
       "       [ 8.787615 ],\n",
       "       [ 9.109518 ],\n",
       "       [ 9.1182575],\n",
       "       [10.125497 ],\n",
       "       [10.10331  ],\n",
       "       [ 8.233382 ],\n",
       "       [ 9.374973 ],\n",
       "       [ 9.505795 ],\n",
       "       [ 7.496211 ],\n",
       "       [ 6.7031884],\n",
       "       [ 9.154398 ],\n",
       "       [ 9.816329 ],\n",
       "       [ 6.8201447],\n",
       "       [ 8.792406 ],\n",
       "       [ 4.9305177],\n",
       "       [10.056349 ],\n",
       "       [ 6.5471687],\n",
       "       [ 9.310152 ],\n",
       "       [ 8.384912 ],\n",
       "       [ 8.625769 ],\n",
       "       [ 9.932495 ],\n",
       "       [ 7.9181604],\n",
       "       [ 9.888633 ],\n",
       "       [ 7.8952146],\n",
       "       [ 8.503441 ],\n",
       "       [ 4.9795113],\n",
       "       [ 6.416916 ],\n",
       "       [ 9.15028  ],\n",
       "       [ 7.19028  ],\n",
       "       [10.020024 ],\n",
       "       [10.170067 ],\n",
       "       [ 8.105314 ],\n",
       "       [ 5.8290873],\n",
       "       [ 8.532607 ],\n",
       "       [ 8.124492 ],\n",
       "       [ 9.338997 ],\n",
       "       [ 8.46656  ],\n",
       "       [ 8.379212 ],\n",
       "       [ 9.290917 ],\n",
       "       [ 7.3782043],\n",
       "       [ 8.321394 ],\n",
       "       [ 8.74515  ],\n",
       "       [ 9.354688 ],\n",
       "       [ 8.658572 ],\n",
       "       [10.608765 ],\n",
       "       [ 8.853391 ],\n",
       "       [ 8.907572 ],\n",
       "       [ 7.768181 ],\n",
       "       [ 9.549215 ],\n",
       "       [ 8.529041 ],\n",
       "       [ 9.758089 ],\n",
       "       [ 8.720209 ],\n",
       "       [ 9.048102 ],\n",
       "       [10.380974 ],\n",
       "       [ 7.2526135],\n",
       "       [ 9.087971 ],\n",
       "       [10.064995 ],\n",
       "       [ 9.739492 ],\n",
       "       [10.415004 ],\n",
       "       [ 8.626419 ],\n",
       "       [ 7.837505 ],\n",
       "       [ 9.389959 ],\n",
       "       [ 8.344927 ],\n",
       "       [ 8.400104 ],\n",
       "       [ 8.944614 ],\n",
       "       [10.421464 ],\n",
       "       [ 8.419434 ],\n",
       "       [ 8.483829 ],\n",
       "       [ 8.429087 ],\n",
       "       [ 7.924487 ],\n",
       "       [ 6.365404 ],\n",
       "       [ 8.006229 ],\n",
       "       [ 7.281894 ],\n",
       "       [ 7.399295 ],\n",
       "       [ 8.491617 ],\n",
       "       [ 7.5517664],\n",
       "       [ 8.999006 ],\n",
       "       [ 7.7902474],\n",
       "       [ 8.102987 ],\n",
       "       [ 6.6481905],\n",
       "       [ 8.694112 ],\n",
       "       [ 9.073766 ],\n",
       "       [ 5.38537  ],\n",
       "       [ 9.685251 ],\n",
       "       [ 8.255926 ],\n",
       "       [10.160599 ],\n",
       "       [ 7.2929907],\n",
       "       [ 8.374908 ],\n",
       "       [10.482376 ],\n",
       "       [ 7.890647 ],\n",
       "       [ 9.085606 ],\n",
       "       [10.126986 ],\n",
       "       [ 8.188629 ],\n",
       "       [10.020304 ],\n",
       "       [ 8.73711  ],\n",
       "       [10.238835 ],\n",
       "       [ 7.04908  ],\n",
       "       [ 7.178122 ],\n",
       "       [ 8.72059  ],\n",
       "       [ 8.537872 ],\n",
       "       [ 8.304801 ],\n",
       "       [ 7.6008472],\n",
       "       [10.637737 ],\n",
       "       [ 6.9938335],\n",
       "       [ 7.080223 ],\n",
       "       [ 9.457325 ],\n",
       "       [ 9.847914 ],\n",
       "       [ 9.70747  ],\n",
       "       [ 8.757159 ],\n",
       "       [ 8.357777 ],\n",
       "       [ 5.48075  ],\n",
       "       [ 8.46255  ],\n",
       "       [ 9.935438 ],\n",
       "       [ 9.983442 ],\n",
       "       [ 4.72458  ],\n",
       "       [ 8.748135 ],\n",
       "       [10.180211 ],\n",
       "       [ 8.252657 ],\n",
       "       [10.394155 ],\n",
       "       [ 6.17047  ],\n",
       "       [ 6.729799 ],\n",
       "       [ 8.063445 ],\n",
       "       [ 8.457604 ],\n",
       "       [10.3262825],\n",
       "       [10.304259 ],\n",
       "       [ 8.600805 ],\n",
       "       [ 7.5006075],\n",
       "       [ 8.631786 ],\n",
       "       [ 7.0659366],\n",
       "       [ 8.431751 ],\n",
       "       [ 8.054565 ],\n",
       "       [ 9.83979  ],\n",
       "       [ 8.36719  ],\n",
       "       [10.031017 ],\n",
       "       [ 9.821645 ],\n",
       "       [ 8.560116 ],\n",
       "       [10.020521 ],\n",
       "       [ 6.4711   ],\n",
       "       [ 8.51588  ],\n",
       "       [ 8.048283 ],\n",
       "       [ 6.542383 ],\n",
       "       [ 9.609255 ],\n",
       "       [10.108294 ],\n",
       "       [ 8.244682 ],\n",
       "       [ 8.249423 ],\n",
       "       [ 9.911038 ],\n",
       "       [ 8.237346 ],\n",
       "       [ 8.489504 ],\n",
       "       [ 9.189388 ],\n",
       "       [ 8.700795 ],\n",
       "       [ 8.41958  ],\n",
       "       [ 8.068232 ],\n",
       "       [ 7.7713556],\n",
       "       [ 8.421869 ],\n",
       "       [ 7.82592  ],\n",
       "       [ 8.703974 ],\n",
       "       [ 8.68656  ],\n",
       "       [ 7.6573   ],\n",
       "       [ 8.72964  ],\n",
       "       [ 9.506295 ],\n",
       "       [10.042931 ],\n",
       "       [ 8.53658  ],\n",
       "       [ 9.345452 ],\n",
       "       [ 7.9401464],\n",
       "       [ 4.864729 ],\n",
       "       [ 9.091875 ],\n",
       "       [ 7.8422585],\n",
       "       [ 9.703505 ],\n",
       "       [ 9.071057 ],\n",
       "       [ 8.058699 ],\n",
       "       [ 9.580548 ],\n",
       "       [ 9.0295315],\n",
       "       [ 9.132511 ],\n",
       "       [ 9.891974 ],\n",
       "       [ 9.863478 ],\n",
       "       [ 8.613953 ],\n",
       "       [ 7.938059 ],\n",
       "       [ 9.825121 ],\n",
       "       [ 7.9701195],\n",
       "       [ 8.231951 ],\n",
       "       [ 7.947225 ],\n",
       "       [ 8.674543 ],\n",
       "       [10.008651 ],\n",
       "       [10.3995285],\n",
       "       [ 8.717654 ],\n",
       "       [ 8.421543 ],\n",
       "       [ 7.0630574],\n",
       "       [ 9.802402 ],\n",
       "       [ 8.621151 ],\n",
       "       [ 9.512411 ],\n",
       "       [10.306797 ],\n",
       "       [ 8.668015 ],\n",
       "       [ 9.704646 ],\n",
       "       [ 9.142164 ],\n",
       "       [ 7.9617953],\n",
       "       [ 8.138451 ],\n",
       "       [ 8.327225 ],\n",
       "       [10.50783  ],\n",
       "       [ 7.684355 ],\n",
       "       [ 8.164541 ],\n",
       "       [ 8.140018 ],\n",
       "       [ 9.822983 ],\n",
       "       [ 7.3933907],\n",
       "       [ 8.8370495],\n",
       "       [10.596933 ],\n",
       "       [ 4.7420487],\n",
       "       [10.574552 ],\n",
       "       [ 8.618522 ],\n",
       "       [ 8.239492 ],\n",
       "       [ 8.398913 ],\n",
       "       [10.101496 ],\n",
       "       [ 6.308704 ],\n",
       "       [ 8.909536 ],\n",
       "       [ 8.486353 ],\n",
       "       [ 8.646996 ],\n",
       "       [ 6.453662 ],\n",
       "       [10.053785 ],\n",
       "       [ 7.895219 ],\n",
       "       [10.190446 ],\n",
       "       [ 9.199455 ],\n",
       "       [ 8.602106 ],\n",
       "       [10.128358 ],\n",
       "       [ 7.9420304],\n",
       "       [ 8.859742 ],\n",
       "       [ 8.3658905],\n",
       "       [ 8.562099 ],\n",
       "       [ 7.9907794],\n",
       "       [10.321952 ],\n",
       "       [ 8.599588 ],\n",
       "       [ 8.560779 ],\n",
       "       [ 6.6213055],\n",
       "       [ 8.871664 ],\n",
       "       [ 8.555413 ],\n",
       "       [ 8.864789 ],\n",
       "       [ 8.669564 ],\n",
       "       [ 8.244534 ],\n",
       "       [ 7.897432 ],\n",
       "       [ 5.159908 ],\n",
       "       [ 7.113278 ],\n",
       "       [ 8.241156 ],\n",
       "       [ 6.486496 ],\n",
       "       [ 5.134101 ],\n",
       "       [ 9.179871 ],\n",
       "       [ 8.875603 ],\n",
       "       [ 9.188819 ],\n",
       "       [ 8.002502 ],\n",
       "       [ 8.11519  ],\n",
       "       [ 7.872719 ],\n",
       "       [ 7.1903257],\n",
       "       [ 9.460347 ],\n",
       "       [ 8.96897  ],\n",
       "       [10.221568 ],\n",
       "       [ 8.186922 ],\n",
       "       [ 8.991247 ],\n",
       "       [ 9.379969 ],\n",
       "       [ 8.312628 ],\n",
       "       [ 8.994602 ],\n",
       "       [ 4.5675745],\n",
       "       [ 9.519976 ],\n",
       "       [ 8.569572 ],\n",
       "       [ 8.167374 ],\n",
       "       [ 8.211298 ],\n",
       "       [ 8.884252 ],\n",
       "       [ 8.455032 ],\n",
       "       [ 7.8171864],\n",
       "       [ 8.354136 ],\n",
       "       [ 8.368809 ],\n",
       "       [ 6.5781097],\n",
       "       [ 8.044455 ],\n",
       "       [ 6.3305817],\n",
       "       [ 9.715328 ],\n",
       "       [ 8.23176  ],\n",
       "       [ 8.577063 ],\n",
       "       [10.255748 ],\n",
       "       [ 9.271384 ],\n",
       "       [ 7.8598995],\n",
       "       [10.087538 ],\n",
       "       [ 8.9374075],\n",
       "       [ 6.290553 ],\n",
       "       [ 8.906066 ],\n",
       "       [ 8.602688 ],\n",
       "       [ 9.888319 ],\n",
       "       [ 9.524743 ],\n",
       "       [ 6.2982035],\n",
       "       [ 7.230398 ],\n",
       "       [ 8.190045 ],\n",
       "       [ 7.931449 ],\n",
       "       [ 9.629963 ],\n",
       "       [ 8.298735 ],\n",
       "       [ 8.020423 ],\n",
       "       [ 8.770753 ],\n",
       "       [10.383127 ],\n",
       "       [ 9.172614 ],\n",
       "       [ 8.755436 ],\n",
       "       [ 7.0984626],\n",
       "       [ 8.67568  ],\n",
       "       [ 9.977454 ],\n",
       "       [ 8.347451 ],\n",
       "       [ 8.069057 ],\n",
       "       [ 8.258487 ],\n",
       "       [ 8.551064 ],\n",
       "       [ 7.916741 ],\n",
       "       [ 8.208635 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred*(range_max-range_min)+range_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 5,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 11,\n",
       " 5,\n",
       " 11,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 12,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 11,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 11,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 12,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 5,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 12,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 2,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 12,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 6,\n",
       " 8,\n",
       " 12,\n",
       " 4,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 11,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_1.save_weights('weights_final/1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Using cached https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/rajivratn/anaconda3/envs/skipflow/lib/python3.6/site-packages (from pydot) (2.4.2)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ebe9e4a76306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# !pip install graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "!pip install pydot\n",
    "# !pip install graphviz\n",
    "from keras.utils import plot_model\n",
    "plot_model(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
