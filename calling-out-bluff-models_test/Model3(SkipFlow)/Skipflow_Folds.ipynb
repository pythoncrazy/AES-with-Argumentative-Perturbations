{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/Saiteja-Reddy/Automatic-Text-Scoring.git\n",
    "import  keras.layers  as  klayers \n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from quadratic_weighted_kappa import QWK\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Tensor_layer(Layer):\n",
    "\tdef __init__(self,output_dim,input_dim=None, **kwargs):\n",
    "\t\tself.output_dim=output_dim\n",
    "\t\tself.input_dim=input_dim\n",
    "\t\tif self.input_dim:\n",
    "\t\t\tkwargs['input_shape']=(self.input_dim,)\n",
    "# \t\tprint(\"YAYY\", input_dim, output_dim)\n",
    "\t\tsuper(Neural_Tensor_layer,self).__init__(**kwargs)\n",
    "\n",
    "\tdef call(self,inputs,mask=None):\n",
    "\t\te1=inputs[0]\n",
    "\t\te2=inputs[1]\n",
    "\t\tbatch_size=K.shape(e1)[0]\n",
    "\t\tk=self.output_dim\n",
    "\t\t\n",
    "\n",
    "\t\tfeed_forward=K.dot(K.concatenate([e1,e2]),self.V)\n",
    "\n",
    "\t\tbilinear_tensor_products = [ K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1) ]\n",
    "\n",
    "\t\tfor i in range(k)[1:]:\t\n",
    "\t\t\tbtp=K.sum((e2*K.dot(e1,self.W[i]))+self.b,axis=1)\n",
    "\t\t\tbilinear_tensor_products.append(btp)\n",
    "\n",
    "\t\tresult=K.tanh(K.reshape(K.concatenate(bilinear_tensor_products,axis=0),(batch_size,k))+feed_forward)\n",
    "\n",
    "\t\treturn result\n",
    "    \n",
    "\tdef build(self,input_shape):\n",
    "\t\tmean=0.0\n",
    "\t\tstd=1.0\n",
    "\t\tk=self.output_dim\n",
    "\t\td=self.input_dim\n",
    "\t\t##truncnorm generate continuous random numbers in given range\n",
    "\t\tW_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k,d,d))\n",
    "\t\tV_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2*d,k))\n",
    "\t\tself.W=K.variable(W_val)\n",
    "\t\tself.V=K.variable(V_val)\n",
    "\t\tself.b=K.zeros((self.input_dim,))\n",
    "\t\tself.trainable_weights=[self.W,self.V,self.b]    \n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\tbatch_size=input_shape[0][0]\n",
    "\t\treturn(batch_size,self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Mean_Pooling(Layer): # conversion from (samples,timesteps,features) to (samples,features)\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Temporal_Mean_Pooling,self).__init__(**kwargs)\n",
    "\t\t# masked values in x (number_of_samples,time)\n",
    "\t\tself.supports_masking=True\n",
    "\t\t# Specifies number of dimensions to each layer\n",
    "\t\tself.input_spec=InputSpec(ndim=3)\n",
    "        \n",
    "\tdef call(self,x,mask=None):\n",
    "\t\tif mask is None:\n",
    "\t\t\tmask=K.mean(K.ones_like(x),axis=-1)\n",
    "\n",
    "\t\tmask=K.cast(mask,K.floatx())\n",
    "\t\t\t\t#dimension size single vec/number of samples\n",
    "\t\treturn K.sum(x,axis=-2)/K.sum(mask,axis=-1,keepdims=True)        \n",
    "\n",
    "\tdef compute_mask(self,input,mask):\n",
    "\t\treturn None\n",
    "    \n",
    "    \n",
    "\tdef compute_output_shape(self,input_shape):\n",
    "\t\treturn (input_shape[0],input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "MAX_NB_WORDS=4000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "VALIDATION_SPLIT=0.20\n",
    "DELTA=20\n",
    "\n",
    "texts=[]\n",
    "# texts_dev = []\n",
    "labels=[]\n",
    "# labels_dev = []\n",
    "# sentences=[]\n",
    "\n",
    "originals = []\n",
    "\n",
    "fp1=open(\"glove.6B.300d.txt\",\"r\", encoding=\"utf-8\")\n",
    "glove_emb={}\n",
    "for line in fp1:\n",
    "\ttemp=line.split(\" \")\n",
    "\tglove_emb[temp[0]]=np.asarray([float(i) for i in temp[1:]])\n",
    "\n",
    "print(\"Embedding done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range min -  1.0  ; range max -  6.0\n"
     ]
    }
   ],
   "source": [
    "essay_type = '2'\n",
    "\n",
    "fp=open(\"data/training_set_rel3.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "originals = []\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        originals.append(float(temp[6]))\n",
    "# print(originals)\n",
    "fp.close()\n",
    "# print(originals)\n",
    "print(\"range min - \", min(originals) , \" ; range max - \", max(originals))\n",
    "\n",
    "range_min = min(originals)\n",
    "range_max = max(originals)\n",
    "\n",
    "fp=open(\"data/training_set_rel3.tsv\",'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "sentences=[]\n",
    "for line in fp:\n",
    "    temp=line.split(\"\\t\")\n",
    "    if(temp[1]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
    "        texts.append(temp[2])\n",
    "        labels.append((float(temp[6])-range_min)/(range_max-range_min)) ## why ??  - normalize to range [0-1]\n",
    "        line=temp[2].strip()\n",
    "        sentences.append(nltk.tokenize.word_tokenize(line))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "labels\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Write a persuasive essay to a newspaper reflecting your views on censorship in libraries. Do you believe that certain should be removed i think so be no that yes i think should no person that in chager the book, music, movies, magazines, ect., that be no agure      why do i think if you need that please  think i no thank you please if  i need why do we if know that if i failure the this test i who need to graduate please the children allow to home please yes.          Why do we need to be a prafece person please why do we need to do this why write this assgiment because you mean to be the best teaches ever and ever facebook is my password is @PERSON1  @NUM1 that why i need my myspace is the same thingh but different at same time please know that i need to know i really  i need to my e-mail address is  @EMAIL1 that is my e-mail please work m\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text labels appended 1800\n",
      "[0.6 0.  0.2 ... 0.2 0.4 0.4]\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(\"text labels appended %s\" %len(texts))\n",
    "\n",
    "labels=np.asarray(labels)\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "\ttemp1=np.zeros((1, EMBEDDING_DIM))\n",
    "\tfor w in i:\n",
    "\t\tif(w in glove_emb):\n",
    "\t\t\ttemp1+=glove_emb[w]\n",
    "\ttemp1/=len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14685 unique tokens.\n",
      "Shape of data tensor: (1800, 500)\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(num_words = MAX_NB_WORDS) #num_words=MAX_NB_WORDS) #limits vocabulory size\n",
    "tokenizer.fit_on_texts(texts) #encoding the text\n",
    "sequences=tokenizer.texts_to_sequences(texts) #returns list of sequences\n",
    "word_index=tokenizer.word_index #dictionary mapping, word and specific token for that word...\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #padding to max_length\n",
    "\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 500)\n",
      "(1800, 500)\n",
      "(1800,)\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "indices=np.arange(data.shape[0]) #with one argument, start=0, step =1\n",
    "print(data.shape)\n",
    "np.random.shuffle(indices)\n",
    "data=data[indices]\n",
    "print(data.shape)\n",
    "labels=labels[indices]\n",
    "# np.reshape(labels, ())\n",
    "print(labels.shape)\n",
    "validation_size=int(VALIDATION_SPLIT*data.shape[0])\n",
    "print(validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 500)\n",
      "(1440,)\n",
      "(360, 500)\n"
     ]
    }
   ],
   "source": [
    "x_train=data[:-validation_size] #data-validation data\n",
    "print(x_train.shape)\n",
    "# print(x_train)\n",
    "# print(labels)\n",
    "y_train=labels[:-validation_size]\n",
    "# print(y_train.transpose)\n",
    "print(y_train.shape)\n",
    "# y_train = np.reshape(y_train, (1427, 1))\n",
    "# print(y_train_new)\n",
    "# print(y_train)\n",
    "x_val=data[-validation_size:]\n",
    "print(x_val.shape)\n",
    "y_val=labels[-validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14685, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14685\n"
     ]
    }
   ],
   "source": [
    "for word,i in word_index.items():\n",
    "\tif(i>=len(word_index)):\n",
    "\t\tcontinue\n",
    "\tif word in glove_emb:\n",
    "\t\t\tembedding_matrix[i]=glove_emb[word]\n",
    "vocab_size=len(word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0219 16:21:36.109570 140247617083200 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=True,\n",
    "\t\t\t\t\t\t\ttrainable=False)\n",
    "# print(embedding_layer.shape)\n",
    "side_embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\tmask_zero=False,\n",
    "\t\t\t\t\t\t\ttrainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SKIPFLOW(lstm_dim=50, lr=1e-4, lr_decay=1e-6, k=4, eta=3, delta=50, activation=\"relu\", maxlen=MAX_SEQUENCE_LENGTH, seed=None):\n",
    "    e = Input(name='essay',shape=(maxlen,))\n",
    "    print(\"e\", e)\n",
    "#     trad_feats=Input(shape=(7,))\n",
    "#     print(\"trad_feats\", trad_feats)\n",
    "    embed = embedding_layer(e)\n",
    "    print(embed.shape)\n",
    "    lstm_layer=LSTM(lstm_dim,return_sequences=True)\n",
    "    print(lstm_layer)\n",
    "    hidden_states=lstm_layer(embed)\n",
    "    htm=Temporal_Mean_Pooling()(hidden_states)    \n",
    "    side_embed = side_embedding_layer(e)\n",
    "    side_hidden_states=lstm_layer(side_embed)    \n",
    "    tensor_layer=Neural_Tensor_layer(output_dim=k,input_dim=500)\n",
    "#     print(input_dim, output_dim)\n",
    "    pairs = [((eta + i * delta) % maxlen, (eta + i * delta + delta) % maxlen) for i in range(maxlen // delta)]\n",
    "    hidden_pairs = [ (Lambda(lambda t: t[:, p[0], :])(side_hidden_states), Lambda(lambda t: t[:, p[1], :])(side_hidden_states)) for p in pairs]\n",
    "    sigmoid = Dense(1, activation=\"sigmoid\", kernel_initializer=initializers.glorot_normal(seed=seed))\n",
    "    coherence = [sigmoid(tensor_layer([hp[0], hp[1]])) for hp in hidden_pairs]\n",
    "    co_tm=Concatenate()(coherence[:]+[htm])\n",
    "    dense = Dense(256, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(co_tm)\n",
    "    dense = Dense(128, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    dense = Dense(64, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    out = Dense(1, activation=\"sigmoid\")(dense)\n",
    "    model = Model(inputs=[e], outputs=[out])\n",
    "    print(\"input\", [e])\n",
    "    print(\"outputs\", out)\n",
    "    adam = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"MSE\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0219 16:22:04.003850 140247617083200 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0219 16:22:04.009980 140247617083200 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0219 16:22:04.025536 140247617083200 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0219 16:22:04.026233 140247617083200 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e Tensor(\"essay:0\", shape=(?, 500), dtype=float32)\n",
      "(?, 500, 300)\n",
      "<keras.layers.recurrent.LSTM object at 0x7f8dec4f8c18>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0219 16:22:05.823732 140247617083200 deprecation.py:323] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0219 16:22:07.050283 140247617083200 deprecation_wrapper.py:119] From /home/rajivratn/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [<tf.Tensor 'essay:0' shape=(?, 500) dtype=float32>]\n",
      "outputs Tensor(\"dense_5/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "essay (InputLayer)              (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 500, 300)     4405500     essay[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 500, 500)     1602000     embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 500)          0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 500, 300)     4405500     essay[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "neural__tensor_layer_1 (Neural_ (None, 4)            1004500     lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            5           neural__tensor_layer_1[0][0]     \n",
      "                                                                 neural__tensor_layer_1[1][0]     \n",
      "                                                                 neural__tensor_layer_1[2][0]     \n",
      "                                                                 neural__tensor_layer_1[3][0]     \n",
      "                                                                 neural__tensor_layer_1[4][0]     \n",
      "                                                                 neural__tensor_layer_1[5][0]     \n",
      "                                                                 neural__tensor_layer_1[6][0]     \n",
      "                                                                 neural__tensor_layer_1[7][0]     \n",
      "                                                                 neural__tensor_layer_1[8][0]     \n",
      "                                                                 neural__tensor_layer_1[9][0]     \n",
      "__________________________________________________________________________________________________\n",
      "temporal__mean__pooling_1 (Temp (None, 500)          0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 510)          0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "                                                                 temporal__mean__pooling_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          130816      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           8256        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            65          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,589,538\n",
      "Trainable params: 2,778,538\n",
      "Non-trainable params: 8,811,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.utils.vis_utils import plot_model\n",
    "earlystopping = EarlyStopping(monitor=\"val_mean_squared_error\", patience=5)\n",
    "sf_2_500 = SKIPFLOW(lstm_dim=500, lr=2e-4, lr_decay=2e-6, k=4, eta=13, delta=50, activation=\"relu\", seed=None)\n",
    "sf_2_500.summary()\n",
    "# plot_model(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Train on 1440 samples, validate on 360 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[500,32,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node lstm_1_1/transpose}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/mul/_469]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[500,32,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node lstm_1_1/transpose}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b7daefd55b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_2_500\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlystopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[500,32,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node lstm_1_1/transpose}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/mul/_469]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[500,32,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node lstm_1_1/transpose}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "# print(sf_1)\n",
    "epochs = 100\n",
    "# epochs = 1000\n",
    "print(type(x_train))\n",
    "# y_train = np.asarray(y_train)\n",
    "print(type(y_train))\n",
    "\n",
    "hist = sf_2_500.fit(x_train, y_train, batch_size=32, epochs=epochs, validation_data=([x_val], y_val), callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=sf_2_500.predict([x_val])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 8, 9, 8, 6, 9, 10, 9, 10, 10, 9, 10, 8, 8, 8, 9, 8, 9, 10, 8, 8, 8, 7, 8, 8, 8, 8, 10, 8, 8, 9, 12, 8, 8, 10, 8, 9, 9, 10, 11, 8, 8, 11, 10, 8, 9, 8, 8, 9, 10, 9, 12, 7, 10, 8, 8, 12, 11, 8, 10, 8, 9, 7, 9, 10, 7, 11, 6, 9, 7, 11, 8, 9, 8, 7, 11, 8, 8, 4, 6, 10, 8, 10, 9, 9, 8, 8, 7, 10, 8, 9, 8, 6, 9, 10, 10, 8, 12, 8, 10, 8, 10, 8, 11, 7, 8, 10, 7, 8, 10, 10, 10, 8, 8, 9, 8, 8, 9, 10, 9, 8, 8, 8, 6, 6, 6, 8, 8, 8, 9, 8, 8, 7, 8, 8, 6, 10, 8, 9, 6, 9, 12, 9, 8, 10, 9, 9, 7, 10, 7, 6, 8, 9, 10, 7, 10, 7, 6, 8, 10, 9, 8, 8, 6, 8, 9, 10, 5, 9, 10, 8, 12, 5, 6, 8, 8, 11, 10, 8, 6, 9, 7, 9, 8, 11, 8, 10, 10, 8, 10, 4, 9, 8, 8, 9, 10, 8, 8, 8, 8, 8, 8, 9, 8, 8, 7, 9, 8, 9, 8, 8, 8, 8, 11, 9, 10, 7, 2, 10, 8, 10, 8, 9, 8, 9, 8, 10, 10, 8, 8, 10, 7, 8, 8, 8, 10, 10, 8, 8, 7, 10, 9, 9, 11, 8, 10, 8, 8, 8, 10, 12, 6, 9, 8, 10, 6, 8, 12, 4, 11, 8, 8, 8, 11, 6, 10, 9, 8, 7, 8, 8, 11, 9, 8, 10, 8, 11, 7, 8, 8, 12, 8, 8, 6, 9, 10, 9, 9, 8, 8, 6, 7, 8, 6, 5, 8, 10, 10, 8, 8, 8, 6, 10, 10, 10, 8, 9, 10, 8, 8, 5, 10, 11, 8, 8, 8, 8, 7, 8, 7, 6, 8, 6, 10, 8, 8, 11, 11, 8, 10, 8, 5, 9, 8, 11, 9, 6, 7, 7, 8, 10, 9, 8, 9, 10, 8, 9, 7, 10, 10, 8, 8, 8, 8, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "y_val_fin = [int(round(a*(range_max-range_min)+range_min)) for a in y_val]\n",
    "print(y_val_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, 8, 10, 7, 9, 11, 9, 10, 8, 9, 10, 8, 9, 8, 9, 8, 9, 9, 8, 8, 8, 7, 9, 8, 8, 8, 10, 8, 8, 9, 10, 8, 8, 9, 8, 9, 9, 10, 11, 8, 7, 10, 9, 9, 8, 8, 8, 10, 8, 9, 10, 7, 9, 9, 9, 10, 10, 8, 9, 10, 7, 7, 9, 10, 7, 9, 5, 10, 7, 9, 8, 9, 10, 8, 10, 8, 9, 5, 6, 9, 7, 10, 10, 8, 6, 9, 8, 9, 8, 8, 9, 7, 8, 9, 9, 9, 11, 9, 9, 8, 10, 9, 10, 9, 9, 10, 7, 9, 10, 10, 10, 9, 8, 9, 8, 8, 9, 10, 8, 8, 8, 8, 6, 8, 7, 7, 8, 8, 9, 8, 8, 7, 9, 9, 5, 10, 8, 10, 7, 8, 10, 8, 9, 10, 8, 10, 9, 10, 7, 7, 9, 9, 8, 8, 11, 7, 7, 9, 10, 10, 9, 8, 5, 8, 10, 10, 5, 9, 10, 8, 10, 6, 7, 8, 8, 10, 10, 9, 8, 9, 7, 8, 8, 10, 8, 10, 10, 9, 10, 6, 9, 8, 7, 10, 10, 8, 8, 10, 8, 8, 9, 9, 8, 8, 8, 8, 8, 9, 9, 8, 9, 10, 10, 9, 9, 8, 5, 9, 8, 10, 9, 8, 10, 9, 9, 10, 10, 9, 8, 10, 8, 8, 8, 9, 10, 10, 9, 8, 7, 10, 9, 10, 10, 9, 10, 9, 8, 8, 8, 11, 8, 8, 8, 10, 7, 9, 11, 5, 11, 9, 8, 8, 10, 6, 9, 8, 9, 6, 10, 8, 10, 9, 9, 10, 8, 9, 8, 9, 8, 10, 9, 9, 7, 9, 9, 9, 9, 8, 8, 5, 7, 8, 6, 5, 9, 9, 9, 8, 8, 8, 7, 9, 9, 10, 8, 9, 9, 8, 9, 5, 10, 9, 8, 8, 9, 8, 8, 8, 8, 7, 8, 6, 10, 8, 9, 10, 9, 8, 10, 9, 6, 9, 9, 10, 10, 6, 7, 8, 8, 10, 8, 8, 9, 10, 9, 9, 7, 9, 10, 8, 8, 8, 9, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "y_pred_fin =[int(round(a*(range_max-range_min)+range_min)) for a in y_pred.reshape(356).tolist()]\n",
    "print(y_pred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7962121212121211\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_val_fin,y_pred_fin,weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def QWK_new(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = Cmatrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7943090425240287"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QWK_new(y_val_fin, y_pred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_1.save('model_final/1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.077402 ],\n",
       "       [ 7.9122553],\n",
       "       [ 7.7343826],\n",
       "       [ 9.691392 ],\n",
       "       [ 7.1335278],\n",
       "       [ 8.686952 ],\n",
       "       [10.531337 ],\n",
       "       [ 9.339651 ],\n",
       "       [ 9.860868 ],\n",
       "       [ 8.380489 ],\n",
       "       [ 9.469806 ],\n",
       "       [ 9.731863 ],\n",
       "       [ 8.236764 ],\n",
       "       [ 8.833696 ],\n",
       "       [ 8.161226 ],\n",
       "       [ 9.201469 ],\n",
       "       [ 8.39722  ],\n",
       "       [ 8.848019 ],\n",
       "       [ 9.321454 ],\n",
       "       [ 8.1174755],\n",
       "       [ 8.32741  ],\n",
       "       [ 7.6823816],\n",
       "       [ 7.354194 ],\n",
       "       [ 8.8032055],\n",
       "       [ 7.743875 ],\n",
       "       [ 8.066136 ],\n",
       "       [ 7.7435837],\n",
       "       [10.297057 ],\n",
       "       [ 7.5326347],\n",
       "       [ 8.176024 ],\n",
       "       [ 9.4363   ],\n",
       "       [10.069847 ],\n",
       "       [ 8.208425 ],\n",
       "       [ 8.4025345],\n",
       "       [ 9.248013 ],\n",
       "       [ 8.388559 ],\n",
       "       [ 9.170399 ],\n",
       "       [ 8.650791 ],\n",
       "       [ 9.601019 ],\n",
       "       [10.586998 ],\n",
       "       [ 8.031967 ],\n",
       "       [ 7.1991963],\n",
       "       [10.093216 ],\n",
       "       [ 8.512126 ],\n",
       "       [ 9.358947 ],\n",
       "       [ 7.9787297],\n",
       "       [ 7.933788 ],\n",
       "       [ 8.266549 ],\n",
       "       [ 9.908953 ],\n",
       "       [ 8.351931 ],\n",
       "       [ 9.24663  ],\n",
       "       [10.492104 ],\n",
       "       [ 6.770634 ],\n",
       "       [ 8.787615 ],\n",
       "       [ 9.109518 ],\n",
       "       [ 9.1182575],\n",
       "       [10.125497 ],\n",
       "       [10.10331  ],\n",
       "       [ 8.233382 ],\n",
       "       [ 9.374973 ],\n",
       "       [ 9.505795 ],\n",
       "       [ 7.496211 ],\n",
       "       [ 6.7031884],\n",
       "       [ 9.154398 ],\n",
       "       [ 9.816329 ],\n",
       "       [ 6.8201447],\n",
       "       [ 8.792406 ],\n",
       "       [ 4.9305177],\n",
       "       [10.056349 ],\n",
       "       [ 6.5471687],\n",
       "       [ 9.310152 ],\n",
       "       [ 8.384912 ],\n",
       "       [ 8.625769 ],\n",
       "       [ 9.932495 ],\n",
       "       [ 7.9181604],\n",
       "       [ 9.888633 ],\n",
       "       [ 7.8952146],\n",
       "       [ 8.503441 ],\n",
       "       [ 4.9795113],\n",
       "       [ 6.416916 ],\n",
       "       [ 9.15028  ],\n",
       "       [ 7.19028  ],\n",
       "       [10.020024 ],\n",
       "       [10.170067 ],\n",
       "       [ 8.105314 ],\n",
       "       [ 5.8290873],\n",
       "       [ 8.532607 ],\n",
       "       [ 8.124492 ],\n",
       "       [ 9.338997 ],\n",
       "       [ 8.46656  ],\n",
       "       [ 8.379212 ],\n",
       "       [ 9.290917 ],\n",
       "       [ 7.3782043],\n",
       "       [ 8.321394 ],\n",
       "       [ 8.74515  ],\n",
       "       [ 9.354688 ],\n",
       "       [ 8.658572 ],\n",
       "       [10.608765 ],\n",
       "       [ 8.853391 ],\n",
       "       [ 8.907572 ],\n",
       "       [ 7.768181 ],\n",
       "       [ 9.549215 ],\n",
       "       [ 8.529041 ],\n",
       "       [ 9.758089 ],\n",
       "       [ 8.720209 ],\n",
       "       [ 9.048102 ],\n",
       "       [10.380974 ],\n",
       "       [ 7.2526135],\n",
       "       [ 9.087971 ],\n",
       "       [10.064995 ],\n",
       "       [ 9.739492 ],\n",
       "       [10.415004 ],\n",
       "       [ 8.626419 ],\n",
       "       [ 7.837505 ],\n",
       "       [ 9.389959 ],\n",
       "       [ 8.344927 ],\n",
       "       [ 8.400104 ],\n",
       "       [ 8.944614 ],\n",
       "       [10.421464 ],\n",
       "       [ 8.419434 ],\n",
       "       [ 8.483829 ],\n",
       "       [ 8.429087 ],\n",
       "       [ 7.924487 ],\n",
       "       [ 6.365404 ],\n",
       "       [ 8.006229 ],\n",
       "       [ 7.281894 ],\n",
       "       [ 7.399295 ],\n",
       "       [ 8.491617 ],\n",
       "       [ 7.5517664],\n",
       "       [ 8.999006 ],\n",
       "       [ 7.7902474],\n",
       "       [ 8.102987 ],\n",
       "       [ 6.6481905],\n",
       "       [ 8.694112 ],\n",
       "       [ 9.073766 ],\n",
       "       [ 5.38537  ],\n",
       "       [ 9.685251 ],\n",
       "       [ 8.255926 ],\n",
       "       [10.160599 ],\n",
       "       [ 7.2929907],\n",
       "       [ 8.374908 ],\n",
       "       [10.482376 ],\n",
       "       [ 7.890647 ],\n",
       "       [ 9.085606 ],\n",
       "       [10.126986 ],\n",
       "       [ 8.188629 ],\n",
       "       [10.020304 ],\n",
       "       [ 8.73711  ],\n",
       "       [10.238835 ],\n",
       "       [ 7.04908  ],\n",
       "       [ 7.178122 ],\n",
       "       [ 8.72059  ],\n",
       "       [ 8.537872 ],\n",
       "       [ 8.304801 ],\n",
       "       [ 7.6008472],\n",
       "       [10.637737 ],\n",
       "       [ 6.9938335],\n",
       "       [ 7.080223 ],\n",
       "       [ 9.457325 ],\n",
       "       [ 9.847914 ],\n",
       "       [ 9.70747  ],\n",
       "       [ 8.757159 ],\n",
       "       [ 8.357777 ],\n",
       "       [ 5.48075  ],\n",
       "       [ 8.46255  ],\n",
       "       [ 9.935438 ],\n",
       "       [ 9.983442 ],\n",
       "       [ 4.72458  ],\n",
       "       [ 8.748135 ],\n",
       "       [10.180211 ],\n",
       "       [ 8.252657 ],\n",
       "       [10.394155 ],\n",
       "       [ 6.17047  ],\n",
       "       [ 6.729799 ],\n",
       "       [ 8.063445 ],\n",
       "       [ 8.457604 ],\n",
       "       [10.3262825],\n",
       "       [10.304259 ],\n",
       "       [ 8.600805 ],\n",
       "       [ 7.5006075],\n",
       "       [ 8.631786 ],\n",
       "       [ 7.0659366],\n",
       "       [ 8.431751 ],\n",
       "       [ 8.054565 ],\n",
       "       [ 9.83979  ],\n",
       "       [ 8.36719  ],\n",
       "       [10.031017 ],\n",
       "       [ 9.821645 ],\n",
       "       [ 8.560116 ],\n",
       "       [10.020521 ],\n",
       "       [ 6.4711   ],\n",
       "       [ 8.51588  ],\n",
       "       [ 8.048283 ],\n",
       "       [ 6.542383 ],\n",
       "       [ 9.609255 ],\n",
       "       [10.108294 ],\n",
       "       [ 8.244682 ],\n",
       "       [ 8.249423 ],\n",
       "       [ 9.911038 ],\n",
       "       [ 8.237346 ],\n",
       "       [ 8.489504 ],\n",
       "       [ 9.189388 ],\n",
       "       [ 8.700795 ],\n",
       "       [ 8.41958  ],\n",
       "       [ 8.068232 ],\n",
       "       [ 7.7713556],\n",
       "       [ 8.421869 ],\n",
       "       [ 7.82592  ],\n",
       "       [ 8.703974 ],\n",
       "       [ 8.68656  ],\n",
       "       [ 7.6573   ],\n",
       "       [ 8.72964  ],\n",
       "       [ 9.506295 ],\n",
       "       [10.042931 ],\n",
       "       [ 8.53658  ],\n",
       "       [ 9.345452 ],\n",
       "       [ 7.9401464],\n",
       "       [ 4.864729 ],\n",
       "       [ 9.091875 ],\n",
       "       [ 7.8422585],\n",
       "       [ 9.703505 ],\n",
       "       [ 9.071057 ],\n",
       "       [ 8.058699 ],\n",
       "       [ 9.580548 ],\n",
       "       [ 9.0295315],\n",
       "       [ 9.132511 ],\n",
       "       [ 9.891974 ],\n",
       "       [ 9.863478 ],\n",
       "       [ 8.613953 ],\n",
       "       [ 7.938059 ],\n",
       "       [ 9.825121 ],\n",
       "       [ 7.9701195],\n",
       "       [ 8.231951 ],\n",
       "       [ 7.947225 ],\n",
       "       [ 8.674543 ],\n",
       "       [10.008651 ],\n",
       "       [10.3995285],\n",
       "       [ 8.717654 ],\n",
       "       [ 8.421543 ],\n",
       "       [ 7.0630574],\n",
       "       [ 9.802402 ],\n",
       "       [ 8.621151 ],\n",
       "       [ 9.512411 ],\n",
       "       [10.306797 ],\n",
       "       [ 8.668015 ],\n",
       "       [ 9.704646 ],\n",
       "       [ 9.142164 ],\n",
       "       [ 7.9617953],\n",
       "       [ 8.138451 ],\n",
       "       [ 8.327225 ],\n",
       "       [10.50783  ],\n",
       "       [ 7.684355 ],\n",
       "       [ 8.164541 ],\n",
       "       [ 8.140018 ],\n",
       "       [ 9.822983 ],\n",
       "       [ 7.3933907],\n",
       "       [ 8.8370495],\n",
       "       [10.596933 ],\n",
       "       [ 4.7420487],\n",
       "       [10.574552 ],\n",
       "       [ 8.618522 ],\n",
       "       [ 8.239492 ],\n",
       "       [ 8.398913 ],\n",
       "       [10.101496 ],\n",
       "       [ 6.308704 ],\n",
       "       [ 8.909536 ],\n",
       "       [ 8.486353 ],\n",
       "       [ 8.646996 ],\n",
       "       [ 6.453662 ],\n",
       "       [10.053785 ],\n",
       "       [ 7.895219 ],\n",
       "       [10.190446 ],\n",
       "       [ 9.199455 ],\n",
       "       [ 8.602106 ],\n",
       "       [10.128358 ],\n",
       "       [ 7.9420304],\n",
       "       [ 8.859742 ],\n",
       "       [ 8.3658905],\n",
       "       [ 8.562099 ],\n",
       "       [ 7.9907794],\n",
       "       [10.321952 ],\n",
       "       [ 8.599588 ],\n",
       "       [ 8.560779 ],\n",
       "       [ 6.6213055],\n",
       "       [ 8.871664 ],\n",
       "       [ 8.555413 ],\n",
       "       [ 8.864789 ],\n",
       "       [ 8.669564 ],\n",
       "       [ 8.244534 ],\n",
       "       [ 7.897432 ],\n",
       "       [ 5.159908 ],\n",
       "       [ 7.113278 ],\n",
       "       [ 8.241156 ],\n",
       "       [ 6.486496 ],\n",
       "       [ 5.134101 ],\n",
       "       [ 9.179871 ],\n",
       "       [ 8.875603 ],\n",
       "       [ 9.188819 ],\n",
       "       [ 8.002502 ],\n",
       "       [ 8.11519  ],\n",
       "       [ 7.872719 ],\n",
       "       [ 7.1903257],\n",
       "       [ 9.460347 ],\n",
       "       [ 8.96897  ],\n",
       "       [10.221568 ],\n",
       "       [ 8.186922 ],\n",
       "       [ 8.991247 ],\n",
       "       [ 9.379969 ],\n",
       "       [ 8.312628 ],\n",
       "       [ 8.994602 ],\n",
       "       [ 4.5675745],\n",
       "       [ 9.519976 ],\n",
       "       [ 8.569572 ],\n",
       "       [ 8.167374 ],\n",
       "       [ 8.211298 ],\n",
       "       [ 8.884252 ],\n",
       "       [ 8.455032 ],\n",
       "       [ 7.8171864],\n",
       "       [ 8.354136 ],\n",
       "       [ 8.368809 ],\n",
       "       [ 6.5781097],\n",
       "       [ 8.044455 ],\n",
       "       [ 6.3305817],\n",
       "       [ 9.715328 ],\n",
       "       [ 8.23176  ],\n",
       "       [ 8.577063 ],\n",
       "       [10.255748 ],\n",
       "       [ 9.271384 ],\n",
       "       [ 7.8598995],\n",
       "       [10.087538 ],\n",
       "       [ 8.9374075],\n",
       "       [ 6.290553 ],\n",
       "       [ 8.906066 ],\n",
       "       [ 8.602688 ],\n",
       "       [ 9.888319 ],\n",
       "       [ 9.524743 ],\n",
       "       [ 6.2982035],\n",
       "       [ 7.230398 ],\n",
       "       [ 8.190045 ],\n",
       "       [ 7.931449 ],\n",
       "       [ 9.629963 ],\n",
       "       [ 8.298735 ],\n",
       "       [ 8.020423 ],\n",
       "       [ 8.770753 ],\n",
       "       [10.383127 ],\n",
       "       [ 9.172614 ],\n",
       "       [ 8.755436 ],\n",
       "       [ 7.0984626],\n",
       "       [ 8.67568  ],\n",
       "       [ 9.977454 ],\n",
       "       [ 8.347451 ],\n",
       "       [ 8.069057 ],\n",
       "       [ 8.258487 ],\n",
       "       [ 8.551064 ],\n",
       "       [ 7.916741 ],\n",
       "       [ 8.208635 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred*(range_max-range_min)+range_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 5,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 11,\n",
       " 5,\n",
       " 11,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 12,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 11,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 11,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 12,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 5,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 12,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 2,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 12,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 6,\n",
       " 8,\n",
       " 12,\n",
       " 4,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 11,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 8]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_1.save_weights('weights_final/1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Using cached https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/rajivratn/anaconda3/envs/skipflow/lib/python3.6/site-packages (from pydot) (2.4.2)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ebe9e4a76306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# !pip install graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dependency/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "!pip install pydot\n",
    "# !pip install graphviz\n",
    "from keras.utils import plot_model\n",
    "plot_model(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('callingoutbluff')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "89eb1e8fcd1790228faf8d8d58a5698f1d4b3e86fe5fa8aa1c0fc5ec965f26af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
